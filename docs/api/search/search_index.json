{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"saev","text":"<p>Sparse autoencoders (SAEs) for vision transformers (ViTs), implemented in PyTorch.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is supported with uv. saev will likely work with pure pip, conda, etc. but I will not formally support it.</p> <p>Clone this repository, then from the root directory:</p> <pre><code>uv run python -m saev --help\n</code></pre> <p>This will create a virtual environment and display the CLI help.</p>"},{"location":"#why-saev","title":"Why saev?","text":"<p>There are plenty of alternative libraries for SAEs:</p> <ul> <li>Overcomplete, primarily developed by Thomas Fel.</li> </ul>"},{"location":"api/colors/","title":"saev.colors","text":""},{"location":"api/colors/#saev.colors","title":"<code>saev.colors</code>","text":"<p>Utility color palettes used across saev visualizations.</p>"},{"location":"api/helpers/","title":"saev.helpers","text":""},{"location":"api/helpers/#saev.helpers","title":"<code>saev.helpers</code>","text":""},{"location":"api/helpers/#saev.helpers.RemovedFeatureError","title":"<code>RemovedFeatureError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Feature existed before but is no longer supported.</p>"},{"location":"api/helpers/#saev.helpers.batched_idx","title":"<code>batched_idx(total_size, batch_size)</code>","text":"<p>Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>total_size</code> <code>int</code> <p>total number of examples</p> required <code>batch_size</code> <code>int</code> <p>maximum distance between the generated indices.</p> required <p>Returns:</p> Type Description <p>A generator of (int, int) tuples that can slice up a list or a tensor.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __init__(self, total_size: int, batch_size: int):\n    self.total_size = total_size\n    self.batch_size = batch_size\n</code></pre>"},{"location":"api/helpers/#saev.helpers.batched_idx.__iter__","title":"<code>__iter__()</code>","text":"<p>Yield (start, end) index pairs for batching.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterator[tuple[int, int]]:\n    \"\"\"Yield (start, end) index pairs for batching.\"\"\"\n    for start in range(0, self.total_size, self.batch_size):\n        stop = min(start + self.batch_size, self.total_size)\n        yield start, stop\n</code></pre>"},{"location":"api/helpers/#saev.helpers.batched_idx.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of batches.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of batches.\"\"\"\n    return (self.total_size + self.batch_size - 1) // self.batch_size\n</code></pre>"},{"location":"api/helpers/#saev.helpers.progress","title":"<code>progress(it, *, every=10, desc='progress', total=0)</code>","text":"<p>Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.</p> <p>Parameters:</p> Name Type Description Default <code>it</code> <code>Iterable</code> <p>Iterable to wrap.</p> required <code>every</code> <code>int</code> <p>How many iterations between logging progress.</p> <code>10</code> <code>desc</code> <code>str</code> <p>What to name the logger.</p> <code>'progress'</code> <code>total</code> <code>int</code> <p>If non-zero, how long the iterable is.</p> <code>0</code> Source code in <code>src/saev/helpers.py</code> <pre><code>def __init__(\n    self,\n    it: collections.abc.Iterable,\n    *,\n    every: int = 10,\n    desc: str = \"progress\",\n    total: int = 0,\n):\n    self.it = it\n    self.every = max(every, 1)\n    self.logger = logging.getLogger(desc)\n    self.total = total\n</code></pre>"},{"location":"api/helpers/#saev.helpers.current_git_commit","title":"<code>current_git_commit()</code>","text":"<p>Best-effort short SHA of the repo containing this file.</p> <p>Returns <code>None</code> when * <code>git</code> executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef current_git_commit() -&gt; str | None:\n    \"\"\"\n    Best-effort short SHA of the repo containing *this* file.\n\n    Returns `None` when\n    * `git` executable is missing,\n    * we\u2019re not inside a git repo (e.g. installed wheel),\n    * or any git call errors out.\n    \"\"\"\n    try:\n        # Walk up until we either hit a .git dir or the FS root\n        here = pathlib.Path(__file__).resolve()\n        for parent in (here, *here.parents):\n            if (parent / \".git\").exists():\n                break\n        else:  # no .git found\n            return None\n\n        result = subprocess.run(\n            [\"git\", \"-C\", str(parent), \"rev-parse\", \"--short\", \"HEAD\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.DEVNULL,\n            text=True,\n            check=True,\n        )\n        return result.stdout.strip() or None\n    except (FileNotFoundError, subprocess.CalledProcessError):\n        return None\n</code></pre>"},{"location":"api/helpers/#saev.helpers.dict_to_dataclass","title":"<code>dict_to_dataclass(data, cls)</code>","text":"<p>Recursively convert a dictionary to a dataclass instance.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef dict_to_dataclass(data: dict, cls: type[T]) -&gt; T:\n    \"\"\"Recursively convert a dictionary to a dataclass instance.\"\"\"\n    if not dataclasses.is_dataclass(cls):\n        return data\n\n    field_types = {f.name: f.type for f in dataclasses.fields(cls)}\n    kwargs = {}\n\n    for field_name, field_type in field_types.items():\n        if field_name not in data:\n            continue\n\n        value = data[field_name]\n\n        # Handle Optional types\n        origin = tp.get_origin(field_type)\n        args = tp.get_args(field_type)\n\n        # Handle tuple[str, ...]\n        if origin is tuple and args:\n            kwargs[field_name] = tuple(value) if isinstance(value, list) else value\n        # Handle list[DataclassType]\n        elif origin is list and args and dataclasses.is_dataclass(args[0]):\n            kwargs[field_name] = [dict_to_dataclass(item, args[0]) for item in value]\n        # Handle regular dataclass fields\n        elif dataclasses.is_dataclass(field_type):\n            kwargs[field_name] = dict_to_dataclass(value, field_type)\n        # Handle pathlib.Path\n        elif field_type is pathlib.Path:\n            # Required Path field - always convert\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        elif origin is tp.Union and pathlib.Path in args:\n            # Optional Path field (typing.Union style)\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        elif origin is types.UnionType and pathlib.Path in args:\n            # Optional Path field (Python 3.10+ union style with |)\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        else:\n            kwargs[field_name] = value\n\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/helpers/#saev.helpers.expand","title":"<code>expand(config)</code>","text":"<p>Expand a nested dict that may contain lists into many dicts.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef expand(config: dict[str, object]) -&gt; collections.abc.Iterator[dict[str, object]]:\n    \"\"\"Expand a nested dict that may contain lists into many dicts.\"\"\"\n\n    yield from _expand_discrete(dict(config))\n</code></pre>"},{"location":"api/helpers/#saev.helpers.flattened","title":"<code>flattened(dct, *, sep='.')</code>","text":"<p>Flatten a potentially nested dict to a single-level dict with <code>.</code>-separated keys.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef flattened(\n    dct: dict[str, object], *, sep: str = \".\"\n) -&gt; dict[str, str | int | float | bool | None]:\n    \"\"\"\n    Flatten a potentially nested dict to a single-level dict with `.`-separated keys.\n    \"\"\"\n    new = {}\n    for key, value in dct.items():\n        if isinstance(value, dict):\n            for nested_key, nested_value in flattened(value).items():\n                new[key + \".\" + nested_key] = nested_value\n            continue\n\n        new[key] = value\n\n    return new\n</code></pre>"},{"location":"api/helpers/#saev.helpers.fssafe","title":"<code>fssafe(s)</code>","text":"<p>Convert a string to be filesystem-safe by replacing special characters.</p> <p>This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>String to make filesystem-safe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Filesystem-safe version of the string.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef fssafe(s: str) -&gt; str:\n    \"\"\"\n    Convert a string to be filesystem-safe by replacing special characters.\n\n    This is particularly useful for checkpoint names that contain characters like\n    'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like\n    'hf-hub_timm_ViT-L-16-SigLIP2-256'.\n\n    Args:\n        s: String to make filesystem-safe.\n\n    Returns:\n        Filesystem-safe version of the string.\n    \"\"\"\n    # Replace common problematic characters with underscores\n    replacements = {\n        \"/\": \"_\",\n        \"\\\\\": \"_\",\n        \":\": \"_\",\n        \"*\": \"_\",\n        \"?\": \"_\",\n        '\"': \"_\",\n        \"&lt;\": \"_\",\n        \"&gt;\": \"_\",\n        \"|\": \"_\",\n        \" \": \"_\",\n    }\n    for old, new in replacements.items():\n        s = s.replace(old, new)\n    # Remove any remaining non-alphanumeric characters except - _ .\n    return \"\".join(c if c.isalnum() or c in \"-_.\" else \"_\" for c in s)\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_cache_dir","title":"<code>get_cache_dir()</code>","text":"<p>Get cache directory from environment variables, defaulting to the current working directory (.)</p> <p>Returns:</p> Type Description <code>str</code> <p>A path to a cache directory (might not exist yet).</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_cache_dir() -&gt; str:\n    \"\"\"\n    Get cache directory from environment variables, defaulting to the current working directory (.)\n\n    Returns:\n        A path to a cache directory (might not exist yet).\n    \"\"\"\n    cache_dir = \"\"\n    for var in (\"SAEV_CACHE\", \"HF_HOME\", \"HF_HUB_CACHE\"):\n        cache_dir = cache_dir or os.environ.get(var, \"\")\n    return cache_dir or \".\"\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_non_default_values","title":"<code>get_non_default_values(obj, default_obj)</code>","text":"<p>Recursively find fields that differ from defaults.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_non_default_values(obj: T, default_obj: T) -&gt; dict:\n    \"\"\"Recursively find fields that differ from defaults.\"\"\"\n    # Check that obj and default_obj are instances of a dataclass.\n    assert dataclasses.is_dataclass(obj) and not isinstance(obj, type)\n    assert dataclasses.is_dataclass(default_obj) and not isinstance(default_obj, type)\n\n    obj_dict = dataclasses.asdict(obj)\n    default_dict = dataclasses.asdict(default_obj)\n\n    diff = {}\n    for key, value in obj_dict.items():\n        default_value = default_dict.get(key)\n        if value != default_value:\n            diff[key] = value\n\n    return diff\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_job_count","title":"<code>get_slurm_job_count()</code>","text":"<p>Get the current number of jobs in the queue for the current user.</p> <p>Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_job_count() -&gt; int:\n    \"\"\"\n    Get the current number of jobs in the queue for the current user.\n\n    Uses squeue's -r flag to properly count job array elements individually.\n    For example, a job array 12345_[0-99] will be counted as 100 jobs.\n    \"\"\"\n    try:\n        # Use -r to display each array element on its own line\n        result = subprocess.run(\n            [\"squeue\", \"--me\", \"-h\", \"-r\"], capture_output=True, text=True, check=True\n        )\n\n        # Count non-empty lines\n        lines = result.stdout.strip().split(\"\\n\")\n        return len([line for line in lines if line.strip()])\n\n    except (subprocess.SubprocessError, FileNotFoundError):\n        # If we can't check, assume no jobs\n        return 0\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_max_array_size","title":"<code>get_slurm_max_array_size()</code>","text":"<p>Get the MaxArraySize configuration from the current Slurm cluster.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_max_array_size() -&gt; int:\n    \"\"\"\n    Get the MaxArraySize configuration from the current Slurm cluster.\n\n    Returns:\n        int: The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine.\n    \"\"\"\n    logger = logging.getLogger(\"helpers.slurm\")\n    try:\n        # Run scontrol command to get config information\n        result = subprocess.run(\n            [\"scontrol\", \"show\", \"config\"], capture_output=True, text=True, check=True\n        )\n\n        # Search for MaxArraySize in the output\n        match = re.search(r\"MaxArraySize\\s*=\\s*(\\d+)\", result.stdout)\n        if match:\n            max_array_size = int(match.group(1))\n            logger.info(\"Detected MaxArraySize = %d\", max_array_size)\n            return max_array_size\n        else:\n            logger.warning(\n                \"Could not find MaxArraySize in scontrol output, using default of 1000\"\n            )\n            return 1000\n\n    except subprocess.SubprocessError as e:\n        logger.error(\"Error running scontrol: %s\", e)\n        return 1000  # Safe default\n    except ValueError as e:\n        logger.error(\"Error parsing MaxArraySize: %s\", e)\n        return 1000  # Safe default\n    except FileNotFoundError:\n        logger.warning(\n            \"scontrol command not found. Assuming not in Slurm environment. Returning default MaxArraySize=1000.\"\n        )\n        return 1000\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_max_submit_jobs","title":"<code>get_slurm_max_submit_jobs()</code>","text":"<p>Get the MaxSubmitJobs limit from the current user's QOS.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The maximum number of jobs that can be submitted at once. Returns 1000 as fallback.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_max_submit_jobs() -&gt; int:\n    \"\"\"\n    Get the MaxSubmitJobs limit from the current user's QOS.\n\n    Returns:\n        int: The maximum number of jobs that can be submitted at once. Returns 1000 as fallback.\n    \"\"\"\n    logger = logging.getLogger(\"helpers.slurm\")\n    try:\n        # First, try to get the QOS from a recent job\n        result = subprocess.run(\n            [\"scontrol\", \"show\", \"job\", \"-o\"],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n\n        qos_name = None\n        if result.returncode == 0 and result.stdout:\n            # Extract QOS from job info\n            match = re.search(r\"QOS=(\\S+)\", result.stdout)\n            if match:\n                qos_name = match.group(1)\n\n        if not qos_name:\n            # If no jobs, try to get default QOS from association\n            # This is less reliable but better than nothing\n            logger.warning(\"No active jobs to determine QOS, using default of 1000\")\n            return 1000\n\n        # Get the MaxSubmitJobs for this QOS\n        result = subprocess.run(\n            [\"sacctmgr\", \"show\", \"qos\", qos_name, \"format=maxsubmitjobs\", \"-n\", \"-P\"],\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n\n        max_submit = result.stdout.strip()\n        if max_submit and max_submit.isdigit():\n            limit = int(max_submit)\n            logger.info(\"Detected MaxSubmitJobs = %d for QOS %s\", limit, qos_name)\n            return limit\n        else:\n            logger.warning(\"Could not parse MaxSubmitJobs, using default of 1000\")\n            return 1000\n\n    except subprocess.SubprocessError as e:\n        logger.error(\"Error getting MaxSubmitJobs: %s\", e)\n        return 1000\n    except (ValueError, FileNotFoundError) as e:\n        logger.error(\"Error: %s\", e)\n        return 1000\n</code></pre>"},{"location":"api/helpers/#saev.helpers.grid","title":"<code>grid(cfg, sweep_dct)</code>","text":"<p>Generate configs from <code>cfg</code> according to <code>sweep_dct</code>.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef grid(cfg: T, sweep_dct: dict[str, object]) -&gt; tuple[list[T], list[str]]:\n    \"\"\"Generate configs from ``cfg`` according to ``sweep_dct``.\"\"\"\n\n    cfgs: list[T] = []\n    errs: list[str] = []\n\n    for d, dct in enumerate(expand(sweep_dct)):\n        updates = _recursive_dataclass_update(cfg, dct, cfg, d)\n\n        if hasattr(cfg, \"seed\") and \"seed\" not in updates:\n            updates[\"seed\"] = getattr(cfg, \"seed\", 0) + d\n\n        try:\n            cfgs.append(dataclasses.replace(cfg, **updates))\n        except Exception as err:\n            errs.append(str(err))\n\n    return cfgs, errs\n</code></pre>"},{"location":"api/helpers/#saev.helpers.merge_configs","title":"<code>merge_configs(base, overrides)</code>","text":"<p>Recursively merge override values into a base config.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef merge_configs(base: T, overrides: dict) -&gt; T:\n    \"\"\"Recursively merge override values into a base config.\"\"\"\n    if not overrides:\n        return base\n\n    # Check that base is an instance of a dataclass.\n    assert dataclasses.is_dataclass(base) and not isinstance(base, type)\n\n    base_dict = dataclasses.asdict(base)\n\n    for key, value in overrides.items():\n        if key in base_dict:\n            # For nested dataclasses, merge recursively\n            if isinstance(value, dict) and dataclasses.is_dataclass(getattr(base, key)):\n                base_dict[key] = dataclasses.asdict(\n                    merge_configs(getattr(base, key), value)\n                )\n            else:\n                base_dict[key] = value\n\n    return dict_to_dataclass(base_dict, type(base))\n</code></pre>"},{"location":"api/saev/","title":"saev","text":""},{"location":"api/saev/#saev","title":"<code>saev</code>","text":"<p>saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.</p>"},{"location":"api/summary/","title":"Summary","text":"<ul> <li>saev</li> <li>saev.colors</li> <li>saev.data</li> <li>saev.data.main</li> <li>saev.data.buffers</li> <li>saev.data.clip</li> <li>saev.data.datasets</li> <li>saev.data.dinov2</li> <li>saev.data.dinov3</li> <li>saev.data.fake_clip</li> <li>saev.data.indexed</li> <li>saev.data.models</li> <li>saev.data.ordered</li> <li>saev.data.shuffled</li> <li>saev.data.siglip</li> <li>saev.data.transforms</li> <li>saev.data.writers</li> <li>saev.helpers</li> <li>saev.nn</li> <li>saev.nn.modeling</li> <li>saev.nn.objectives</li> <li>saev.utils</li> <li>saev.utils.scheduling</li> <li>saev.utils.statistics</li> <li>saev.utils.wandb</li> <li>saev.viz</li> </ul>"},{"location":"api/viz/","title":"saev.viz","text":""},{"location":"api/viz/#saev.viz","title":"<code>saev.viz</code>","text":""},{"location":"api/data/__main__/","title":"saev.data.main","text":""},{"location":"api/data/__main__/#saev.data.__main__","title":"<code>saev.data.__main__</code>","text":"<p>To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one.</p> <p>This module handles that additional complexity.</p> <p>Conceptually, activations are either thought of as</p> <ol> <li>A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a dataset</li> <li>Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations.</li> </ol>"},{"location":"api/data/__main__/#saev.data.__main__.main","title":"<code>main(cfg)</code>","text":"<p>Save ViT activations for use later on.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Annotated[Config, arg(name='')]</code> <p>Configuration for activations.</p> required Source code in <code>src/saev/data/__main__.py</code> <pre><code>@beartype.beartype\ndef main(cfg: typing.Annotated[writers.Config, tyro.conf.arg(name=\"\")]):\n    \"\"\"\n    Save ViT activations for use later on.\n\n    Args:\n        cfg: Configuration for activations.\n    \"\"\"\n    logger = logging.getLogger(\"dump\")\n\n    if not cfg.ssl:\n        logger.warning(\"Ignoring SSL certs. Try not to do this!\")\n        # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761\n        # Ideally we don't have to disable SSL but we are only downloading weights.\n        import ssl\n\n        ssl._create_default_https_context = ssl._create_unverified_context\n\n    # Actually record activations.\n    if cfg.slurm_acct:\n        import submitit\n\n        executor = submitit.SlurmExecutor(folder=cfg.log_to)\n        executor.update_parameters(\n            time=int(cfg.n_hours * 60),\n            partition=cfg.slurm_partition,\n            gpus_per_node=1,\n            ntasks_per_node=1,\n            cpus_per_task=cfg.n_workers + 4,\n            stderr_to_stdout=True,\n            account=cfg.slurm_acct,\n        )\n\n        job = executor.submit(writers.worker_fn, cfg)\n        logger.info(\"Running job '%s'.\", job.job_id)\n        job.result()\n\n    else:\n        writers.worker_fn(cfg)\n</code></pre>"},{"location":"api/data/buffers/","title":"saev.data.buffers","text":""},{"location":"api/data/buffers/#saev.data.buffers","title":"<code>saev.data.buffers</code>","text":""},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer","title":"<code>ReservoirBuffer(capacity, shape, *, dtype=torch.float32, meta_shape=(2,), meta_dtype=torch.int32, seed=0, collate_fn=None)</code>","text":"<p>Pool of (tensor, meta) pairs. Multiple producers call put(batch_x, batch_meta). Multiple consumers call get(batch_size) -&gt; (x, meta). Random order, each sample delivered once, blocking semantics.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def __init__(\n    self,\n    capacity: int,\n    shape: tuple[int, ...],\n    *,\n    dtype: torch.dtype = torch.float32,\n    meta_shape: tuple[int, ...] = (2,),\n    meta_dtype: torch.dtype = torch.int32,\n    seed: int = 0,\n    collate_fn: collections.abc.Callable | None = None,\n):\n    self.capacity = capacity\n    self._empty = 123456789\n\n    self.data = torch.full((capacity, *shape), self._empty, dtype=dtype)\n    self.data.share_memory_()\n\n    self.meta = torch.full((capacity, *meta_shape), self._empty, dtype=meta_dtype)\n    self.meta.share_memory_()\n\n    self.ctx = mp.get_context()\n\n    self.size = self.ctx.Value(\"L\", 0)  # current live items\n    self.lock = self.ctx.Lock()  # guards size+swap\n    self.free = self.ctx.Semaphore(capacity)\n    self.full = self.ctx.Semaphore(0)\n    # Each process has its own RNG.\n    self.rng = np.random.default_rng(seed)\n\n    self.collate_fn = collate_fn\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.close","title":"<code>close()</code>","text":"<p>Release the shared-memory backing store (call once in the parent).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\"\n    try:\n        self.data.untyped_storage()._free_shared_mem()\n    except (AttributeError, FileNotFoundError):\n        pass  # already freed or never allocated\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.fill","title":"<code>fill()</code>","text":"<p>Approximate proportion of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def fill(self) -&gt; float:\n    \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\"\n    return self.qsize() / self.capacity\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.qsize","title":"<code>qsize()</code>","text":"<p>Approximate number of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def qsize(self) -&gt; int:\n    \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\"\n    return self.size.value\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer","title":"<code>RingBuffer(slots, shape, dtype)</code>","text":"<p>Fixed-capacity, multiple-producer / multiple-consumer queue backed by a shared-memory tensor.</p>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer--parameters","title":"Parameters","text":"<p>slots  : int           capacity in number of items (tensor rows) shape  : tuple[int]    shape of one item, e.g. (batch, dim) dtype  : torch.dtype   tensor dtype</p> <p>put(tensor)  : blocks if full get() -&gt; tensor  : blocks if empty qsize() -&gt; int        advisory size (approximate) close()               frees shared storage (call in the main process)</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def __init__(self, slots: int, shape: tuple[int, ...], dtype: torch.dtype):\n    assert slots &gt; 0, \"slots must be positive\"\n    self.slots = slots\n    # 123456789 -&gt; Should make you very worried.\n    self.buf = torch.full((slots, *shape), 123456789, dtype=dtype)\n    self.buf.share_memory_()\n\n    ctx = mp.get_context()  # obeys the global start method (\"spawn\")\n\n    # shared, lock-free counters\n    self.head = ctx.Value(\"L\", 0, lock=False)  # next free slot\n    self.tail = ctx.Value(\"L\", 0, lock=False)  # next occupied slot\n\n    # semaphores for blocking semantics\n    self.free = ctx.Semaphore(slots)  # initially all slots free\n    self.fill = ctx.Semaphore(0)  # no filled slots yet\n\n    # one mutex for pointer updates\n    self.mutex = ctx.Lock()\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.close","title":"<code>close()</code>","text":"<p>Release the shared-memory backing store (call once in the parent).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\"\n    try:\n        self.buf.untyped_storage()._free_shared_mem()\n    except (AttributeError, FileNotFoundError):\n        pass  # already freed or never allocated\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.fill","title":"<code>fill()</code>","text":"<p>Approximate proportion of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def fill(self) -&gt; float:\n    \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\"\n    return self.qsize() / self.capacity\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.get","title":"<code>get()</code>","text":"<p>Return a view of the next item; blocks if the queue is empty.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def get(self) -&gt; torch.Tensor:\n    \"\"\"Return a view of the next item; blocks if the queue is empty.\"\"\"\n    self.fill.acquire()  # wait for data\n    with self.mutex:  # exclusive update of tail\n        idx = self.tail.value % self.slots\n        out = self.buf[idx].clone()\n        self.tail.value += 1\n    self.free.release()  # signal one more free slot\n    return out\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.put","title":"<code>put(tensor)</code>","text":"<p>Copy <code>tensor</code> into the next free slot; blocks if the queue is full.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def put(self, tensor: torch.Tensor) -&gt; None:\n    \"\"\"Copy `tensor` into the next free slot; blocks if the queue is full.\"\"\"\n    if tensor.shape != self.buf.shape[1:] or tensor.dtype != self.buf.dtype:\n        raise ValueError(\"tensor shape / dtype mismatch\")\n\n    self.free.acquire()  # wait for a free slot\n    with self.mutex:  # exclusive update of head\n        idx = self.head.value % self.slots\n        self.buf[idx].copy_(tensor)\n        self.head.value += 1\n    self.fill.release()  # signal there is data\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.qsize","title":"<code>qsize()</code>","text":"<p>Approximate number of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def qsize(self) -&gt; int:\n    \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\"\n    return (self.head.value - self.tail.value) % (1 &lt;&lt; 64)\n</code></pre>"},{"location":"api/data/clip/","title":"saev.data.clip","text":""},{"location":"api/data/clip/#saev.data.clip","title":"<code>saev.data.clip</code>","text":""},{"location":"api/data/clip/#saev.data.clip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>VisionTransformer</code>, <code>Module</code></p> Source code in <code>src/saev/data/clip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    if ckpt.startswith(\"hf-hub:\"):\n        clip, _ = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n        _, ckpt = ckpt.split(\"hf-hub:\")\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        clip, _ = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    self._ckpt = ckpt\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model.eval()\n\n    assert not isinstance(self.model, open_clip.timm_model.TimmModel)\n</code></pre>"},{"location":"api/data/clip/#saev.data.clip.Vit.patch_size","title":"<code>patch_size</code>  <code>property</code>","text":"<p>Get patch size for CLIP models.</p>"},{"location":"api/data/clip/#saev.data.clip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/clip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    if ckpt.startswith(\"hf-hub:\"):\n        _, img_transform = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        _, img_transform = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/datasets/","title":"saev.data.datasets","text":""},{"location":"api/data/datasets/#saev.data.datasets","title":"<code>saev.data.datasets</code>","text":""},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg","title":"<code>FakeSeg(n_imgs=10, n_patches_per_img=16, n_classes=3, bg_label=0)</code>  <code>dataclass</code>","text":"<p>Tiny synthetic segmentation dataset for tests.</p> <p>Generates dummy RGB images and pixel-level segmentation masks, mimicking the behavior of real segmentation datasets like SegFolder.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.bg_label","title":"<code>bg_label = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which class index is considered background.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_classes","title":"<code>n_classes = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of segmentation classes.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_imgs","title":"<code>n_imgs = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of images.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_patches_per_img","title":"<code>n_patches_per_img = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of patches per image.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeSegDataset","title":"<code>FakeSegDataset(cfg, *, img_transform=None, seg_transform=None, sample_transform=None)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Synthetic segmentation dataset providing pixel-level segmentation masks.</p> Mimics SegFolderDataset by providing <ul> <li>image: a dummy RGB PIL image</li> <li>segmentation: a PIL image with pixel-level class labels</li> <li>index, target, label</li> </ul> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __init__(\n    self,\n    cfg: FakeSeg,\n    *,\n    img_transform=None,\n    seg_transform=None,\n    sample_transform=None,\n):\n    self.cfg = cfg\n    self.img_transform = img_transform\n    self.seg_transform = seg_transform\n    self.sample_transform = sample_transform\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder","title":"<code>ImageFolder(root=os.path.join('.', 'data', 'split'))</code>  <code>dataclass</code>","text":"<p>Configuration for a generic image folder dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder.n_imgs","title":"<code>n_imgs</code>  <code>property</code>","text":"<p>Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder.root","title":"<code>root = os.path.join('.', 'data', 'split')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where the class folders with images are stored. Can be a glob pattern to match multiple directories.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolderDataset","title":"<code>ImageFolderDataset(*args, sample_transform=None, **kwargs)</code>","text":"<p>               Bases: <code>ImageFolder</code></p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __init__(self, *args, sample_transform: Callable | None = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.sample_transform = sample_transform\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolderDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index</p> required <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>dict with keys 'image', 'index', 'target' and 'label'.</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict[str, object]:\n    \"\"\"\n    Args:\n        index: Index\n\n    Returns:\n        dict with keys 'image', 'index', 'target' and 'label'.\n    \"\"\"\n    path, target = self.samples[index]\n    image = self.loader(path)\n    if self.transform is not None:\n        image = self.transform(image)\n    if self.target_transform is not None:\n        target = self.target_transform(target)\n\n    sample = {\n        \"image\": image,\n        \"target\": target,\n        \"label\": self.classes[target],\n        \"index\": index,\n    }\n\n    if self.sample_transform is not None:\n        sample = self.sample_transform(sample)\n\n    return sample\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet","title":"<code>Imagenet(name='ILSVRC/imagenet-1k', split='train')</code>  <code>dataclass</code>","text":"<p>Configuration for HuggingFace Imagenet.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.n_imgs","title":"<code>n_imgs</code>  <code>property</code>","text":"<p>Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.name","title":"<code>name = 'ILSVRC/imagenet-1k'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset name on HuggingFace. Don't need to change this..</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.split","title":"<code>split = 'train'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'.</p>"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder","title":"<code>SegFolder(root=os.path.join('.', 'data', 'segdataset'), split='training', img_label_fname='sceneCategories.txt', bg_label=0)</code>  <code>dataclass</code>","text":""},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.bg_label","title":"<code>bg_label = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Background label.</p>"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.img_label_fname","title":"<code>img_label_fname = 'sceneCategories.txt'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Image labels filename.</p>"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.n_imgs","title":"<code>n_imgs</code>  <code>property</code>","text":"<p>Number of images in the dataset. Calculated on the fly by counting image files in root/images/split.</p>"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.root","title":"<code>root = os.path.join('.', 'data', 'segdataset')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where the class folders with images are stored.</p>"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.split","title":"<code>split = 'training'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Data split.</p>"},{"location":"api/data/datasets/#saev.data.datasets.get_dataset","title":"<code>get_dataset(cfg, *, img_transform, seg_transform=None, sample_transform=None)</code>","text":"<p>Gets the dataset for the current experiment; delegates construction to dataset-specific functions.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Config</code> <p>Experiment config.</p> required <code>img_transform</code> <p>Image transform to be applied to each image.</p> required <code>seg_transform</code> <p>Segmentation transform to be applied to masks (for segmentation datasets).</p> <code>None</code> <code>sample_transform</code> <p>Transform to be applied to each sample dict.</p> <code>None</code> <p>Returns:     A dataset that has dictionaries with <code>'image'</code>, <code>'index'</code>, <code>'target'</code>, and <code>'label'</code> keys containing examples.</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>@beartype.beartype\ndef get_dataset(\n    cfg: Config, *, img_transform, seg_transform=None, sample_transform=None\n):\n    \"\"\"\n    Gets the dataset for the current experiment; delegates construction to dataset-specific functions.\n\n    Args:\n        cfg: Experiment config.\n        img_transform: Image transform to be applied to each image.\n        seg_transform: Segmentation transform to be applied to masks (for segmentation datasets).\n        sample_transform: Transform to be applied to each sample dict.\n    Returns:\n        A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples.\n    \"\"\"\n    # TODO: Can we reduce duplication? Or is it nice to see that there is no magic here?\n    if isinstance(cfg, Imagenet):\n        return ImagenetDataset(\n            cfg, img_transform=img_transform, sample_transform=sample_transform\n        )\n    elif isinstance(cfg, SegFolder):\n        return SegFolderDataset(\n            cfg,\n            img_transform=img_transform,\n            seg_transform=seg_transform,\n            sample_transform=sample_transform,\n        )\n    elif isinstance(cfg, ImageFolder):\n        ds = [\n            ImageFolderDataset(\n                root, transform=img_transform, sample_transform=sample_transform\n            )\n            for root in glob.glob(cfg.root, recursive=True)\n        ]\n        if len(ds) == 1:\n            return ds[0]\n        else:\n            return torch.utils.data.ConcatDataset(ds)\n    elif isinstance(cfg, Fake):\n        return FakeDataset(\n            cfg, img_transform=img_transform, sample_transform=sample_transform\n        )\n    elif isinstance(cfg, FakeSeg):\n        return FakeSegDataset(\n            cfg,\n            img_transform=img_transform,\n            seg_transform=seg_transform,\n            sample_transform=sample_transform,\n        )\n    else:\n        typing.assert_never(cfg)\n</code></pre>"},{"location":"api/data/dinov2/","title":"saev.data.dinov2","text":""},{"location":"api/data/dinov2/#saev.data.dinov2","title":"<code>saev.data.dinov2</code>","text":""},{"location":"api/data/dinov3/","title":"saev.data.dinov3","text":""},{"location":"api/data/dinov3/#saev.data.dinov3","title":"<code>saev.data.dinov3</code>","text":""},{"location":"api/data/dinov3/#saev.data.dinov3.Config","title":"<code>Config(img_size=224, patch_size=16, in_chans=3, pos_embed_rope_base=100.0, pos_embed_rope_min_period=None, pos_embed_rope_max_period=None, pos_embed_rope_normalize_coords='separate', pos_embed_rope_dtype='bf16', embed_dim=768, depth=12, num_heads=12, ffn_ratio=4.0, qkv_bias=True, ffn_layer='mlp', ffn_bias=True, proj_bias=True, n_storage_tokens=0, mask_k_bias=False, untie_global_and_local_cls_norm=False, device=None)</code>  <code>dataclass</code>","text":""},{"location":"api/data/dinov3/#saev.data.dinov3.Config.depth","title":"<code>depth = 12</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of transformer blocks.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.device","title":"<code>device = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device for tensor operations.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.embed_dim","title":"<code>embed_dim = 768</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Embedding dimension for transformer.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_bias","title":"<code>ffn_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in feed-forward network.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_layer","title":"<code>ffn_layer = 'mlp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of feed-forward network layer.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_ratio","title":"<code>ffn_ratio = 4.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Feed-forward network expansion ratio.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.img_size","title":"<code>img_size = 224</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Image width and height in pixels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.in_chans","title":"<code>in_chans = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of input image channels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.mask_k_bias","title":"<code>mask_k_bias = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to mask K bias in attention.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.n_storage_tokens","title":"<code>n_storage_tokens = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of storage/register tokens.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.num_heads","title":"<code>num_heads = 12</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of attention heads.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.patch_size","title":"<code>patch_size = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of each patch in pixels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_base","title":"<code>pos_embed_rope_base = 100.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base frequency for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_dtype","title":"<code>pos_embed_rope_dtype = 'bf16'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Data type for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_max_period","title":"<code>pos_embed_rope_max_period = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum period for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_min_period","title":"<code>pos_embed_rope_min_period = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum period for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_normalize_coords","title":"<code>pos_embed_rope_normalize_coords = 'separate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinate normalization method for RoPE encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.proj_bias","title":"<code>proj_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in output projection.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.qkv_bias","title":"<code>qkv_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in QKV projection.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.untie_global_and_local_cls_norm","title":"<code>untie_global_and_local_cls_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use separate norms for global and local CLS tokens.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.PatchEmbed","title":"<code>PatchEmbed(img_size=224, patch_size=16, in_chans=3, embed_dim=768, flatten_embedding=True)</code>","text":"<p>               Bases: <code>Module</code></p> <p>2D image to patch embedding: (B,C,H,W) -&gt; (B,N,D)</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int | tuple[int, int]</code> <p>Image size.</p> <code>224</code> <code>patch_size</code> <code>int | tuple[int, int]</code> <p>Patch token size.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input image channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Number of linear projection output channels.</p> <code>768</code> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>def __init__(\n    self,\n    img_size: int | tuple[int, int] = 224,\n    patch_size: int | tuple[int, int] = 16,\n    in_chans: int = 3,\n    embed_dim: int = 768,\n    flatten_embedding: bool = True,\n) -&gt; None:\n    super().__init__()\n\n    image_hw = make_2tuple(img_size)\n    patch_hw = make_2tuple(patch_size)\n\n    self.image_hw = image_hw\n    self.patch_hw = patch_hw\n\n    self.in_chans = in_chans\n    self.embed_dim = embed_dim\n\n    self.proj = nn.Conv2d(\n        in_chans, embed_dim, kernel_size=patch_hw, stride=patch_hw\n    )\n    self.k = patch_hw[0]\n    assert self.proj.kernel_size == (self.k, self.k)\n    assert self.proj.stride == (self.k, self.k)\n    assert self.proj.padding == (0, 0)\n    assert self.proj.groups == 1\n    assert self.proj.dilation == (1, 1)\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>Module</code>, <code>VisionTransformer</code></p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n    name = self._parse_name(ckpt)\n    self.model = load(name, ckpt)\n\n    self._ckpt = name\n    self.logger = logging.getLogger(f\"dinov3/{name}\")\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n    import functools\n\n    return functools.partial(\n        transforms.resize_to_patch_grid,\n        p=int(16 * scale),\n        n=n_patches_per_img,\n        resample=resample,\n    )\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    img_transform = v2.Compose([\n        transforms.FlexResize(patch_size=16, n_patches=n_patches_per_img),\n        v2.ToImage(),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),\n    ])\n    sample_transform = transforms.Patchify(\n        patch_size=16, n_patches=n_patches_per_img\n    )\n    return img_transform, sample_transform\n</code></pre>"},{"location":"api/data/fake_clip/","title":"saev.data.fake_clip","text":""},{"location":"api/data/fake_clip/#saev.data.fake_clip","title":"<code>saev.data.fake_clip</code>","text":"<p>Fake CLIP model for testing with tiny-open-clip-model.</p> <p>This module provides a test-only vision transformer that works with the tiny-open-clip-model from HuggingFace, which uses 8x8 images and 2x2 patches instead of the standard 224x224 images with 16x16 patches.</p>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>VisionTransformer</code>, <code>Module</code></p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    # Only support the tiny test model\n    assert ckpt == \"hf-hub:hf-internal-testing/tiny-open-clip-model\", (\n        f\"FakeClip only supports tiny-open-clip-model, got {ckpt}\"\n    )\n\n    clip, _ = open_clip.create_model_from_pretrained(\n        ckpt, cache_dir=helpers.get_cache_dir()\n    )\n    self._ckpt = ckpt\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model.eval()\n</code></pre>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.patch_size","title":"<code>patch_size</code>  <code>property</code>","text":"<p>Tiny model uses 2x2 patches.</p>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img=-1, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for tiny model (8x8 images).</p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int = -1,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for tiny model (8x8 images).\"\"\"\n\n    def resize(img: Image.Image) -&gt; Image.Image:\n        # Tiny model uses 8x8 images\n        size_px = (int(8 * scale), int(8 * scale))\n        return img.resize(size_px, resample=resample)\n\n    return resize\n</code></pre>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    _, img_transform = open_clip.create_model_from_pretrained(\n        ckpt, cache_dir=helpers.get_cache_dir()\n    )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/indexed/","title":"saev.data.indexed","text":""},{"location":"api/data/indexed/#saev.data.indexed","title":"<code>saev.data.indexed</code>","text":""},{"location":"api/data/indexed/#saev.data.indexed.Config","title":"<code>Config(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, seed=17, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading indexed activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shard_root</code> <code>str</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>patches</code> <code>Literal['cls', 'image', 'all']</code> <p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset","title":"<code>Dataset(cfg)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset of activations from disk.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration set via CLI args.</p> <code>metadata</code> <code>Metadata</code> <p>Activations metadata; automatically loaded from disk.</p> <code>layer_index</code> <code>int</code> <p>Layer index into the shards if we are choosing a specific layer.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    # Check if labels.bin exists\n    labels_path = os.path.join(self.cfg.shard_root, \"labels.bin\")\n    self.labels_mmap = None\n    if os.path.exists(labels_path):\n        self.labels_mmap = np.memmap(\n            labels_path,\n            mode=\"r\",\n            dtype=np.uint8,\n            shape=(self.metadata.n_imgs, self.metadata.n_patches_per_img),\n        )\n\n    # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError.\n    self.layer_index = 1_000_000\n    if isinstance(self.cfg.layer, int):\n        err_msg = f\"Non-exact matches for .layer field not supported; {self.cfg.layer} not in {self.metadata.layers}.\"\n        assert self.cfg.layer in self.metadata.layers, err_msg\n        self.layer_index = self.metadata.layers.index(self.cfg.layer)\n</code></pre>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.d_vit","title":"<code>d_vit</code>  <code>property</code>","text":"<p>Dimension of the underlying vision transformer's embedding space.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.Example","title":"<code>Example</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Dataset length depends on <code>patches</code> and <code>layer</code>.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Dataset length depends on `patches` and `layer`.\n    \"\"\"\n    match (self.cfg.patches, self.cfg.layer):\n        case (\"cls\", \"all\"):\n            # Return a CLS token from a random image and random layer.\n            return self.metadata.n_imgs * len(self.metadata.layers)\n        case (\"cls\", int()):\n            # Return a CLS token from a random image and fixed layer.\n            return self.metadata.n_imgs\n        case (\"image\", int()):\n            # Return a patch from a random image, fixed layer, and random patch.\n            return self.metadata.n_imgs * (self.metadata.n_patches_per_img)\n        case (\"image\", \"all\"):\n            # Return a patch from a random image, random layer and random patch.\n            return (\n                self.metadata.n_imgs\n                * len(self.metadata.layers)\n                * self.metadata.n_patches_per_img\n            )\n        case _:\n            typing.assert_never((self.cfg.patches, self.cfg.layer))\n</code></pre>"},{"location":"api/data/models/","title":"saev.data.models","text":""},{"location":"api/data/models/#saev.data.models","title":"<code>saev.data.models</code>","text":""},{"location":"api/data/models/#saev.data.models.VisionTransformer","title":"<code>VisionTransformer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Protocol defining the interface for all Vision Transformer models.</p>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.patch_size","title":"<code>patch_size</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Patch size in pixels (e.g., 14 or 16).</p>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.forward","title":"<code>forward(batch)</code>  <code>abstractmethod</code>","text":"<p>Run forward pass on batch of images.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef forward(\n    self, batch: Float[Tensor, \"batch 3 width height\"]\n) -&gt; Float[Tensor, \"batch patches dim\"]:\n    \"\"\"Run forward pass on batch of images.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_patches","title":"<code>get_patches(n_patches_per_img)</code>  <code>abstractmethod</code>","text":"<p>Return indices for selecting relevant patches from activations.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef get_patches(self, n_patches_per_img: int) -&gt; slice | torch.Tensor:\n    \"\"\"Return indices for selecting relevant patches from activations.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_residuals","title":"<code>get_residuals()</code>  <code>abstractmethod</code>","text":"<p>Return the list of residual blocks/layers for hook registration.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef get_residuals(self) -&gt; list[torch.nn.Module]:\n    \"\"\"Return the list of residual blocks/layers for hook registration.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.list_families","title":"<code>list_families()</code>","text":"<p>List all ViT family names.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>def list_families() -&gt; list[str]:\n    \"\"\"List all ViT family names.\"\"\"\n    return list(_global_vit_registry.keys())\n</code></pre>"},{"location":"api/data/models/#saev.data.models.load_vit_cls","title":"<code>load_vit_cls(family)</code>","text":"<p>Load a ViT family class.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@beartype.beartype\ndef load_vit_cls(family: str) -&gt; type[VisionTransformer]:\n    \"\"\"Load a ViT family class.\"\"\"\n    if family not in _global_vit_registry:\n        raise ValueError(f\"Family '{family}' not found.\")\n\n    return _global_vit_registry[family]\n</code></pre>"},{"location":"api/data/models/#saev.data.models.register_family","title":"<code>register_family(cls)</code>","text":"<p>Register a new ViT family class.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@beartype.beartype\ndef register_family(cls: type[VisionTransformer]):\n    \"\"\"Register a new ViT family class.\"\"\"\n    if cls.family in _global_vit_registry:\n        logger.warning(\"Overwriting key '%s' in registry.\", cls.family)\n    _global_vit_registry[cls.family] = cls\n</code></pre>"},{"location":"api/data/ordered/","title":"saev.data.ordered","text":""},{"location":"api/data/ordered/#saev.data.ordered","title":"<code>saev.data.ordered</code>","text":"<p>Ordered (sequential) dataloader for activation data.</p> <p>This module provides a high-throughput dataloader that reads activation data from disk shards in sequential order, without shuffling. The implementation uses a single-threaded manager process to ensure data is delivered in the exact order it appears on disk.</p> <p>Patch labels are provided if there is a labels.bin file on disk.</p> <p>See the design decisions in src/saev/data/performance.md.</p> Usage <p>cfg = Config(shard_root=\"./shards\", layer=13, batch_size=4096) dataloader = DataLoader(cfg) for batch in dataloader: ...     activations = batch[\"act\"]  # [batch_size, d_vit] ...     image_indices = batch[\"image_i\"]  # [batch_size] ...     patch_indices = batch[\"patch_i\"]  # [batch_size] ...     patch_labels = batch[\"patch_labels\"]  # [batch_size]</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config","title":"<code>Config(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, batch_size=1024 * 16, batch_timeout_s=30.0, drop_last=False, buffer_size=64, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading ordered (non-shuffled) activation data from disk.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.patches","title":"<code>patches = 'image'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config.shard_root","title":"<code>shard_root = os.path.join('.', 'shards')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Directory with .bin shards and a metadata.json file.</p>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader","title":"<code>DataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that reads data from disk shards in order (no shuffling).</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self.logger = logging.getLogger(\"ordered.DataLoader\")\n    self.ctx = mp.get_context()\n    self.manager_proc = None\n    self.batch_queue = None\n    self.stop_event = None\n    self._n_samples = self._calculate_n_samples()\n    self.logger.info(\n        \"Initialized ordered.DataLoader with %d samples. (debug=%s)\",\n        self.n_samples,\n        self.cfg.debug,\n    )\n</code></pre>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches in order.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterable[ExampleBatch]:\n    \"\"\"Yields batches in order.\"\"\"\n    self._start_manager()\n    n = 0\n\n    try:\n        while n &lt; self.n_samples:\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                batch = self.batch_queue.get(timeout=self.cfg.batch_timeout_s)\n                actual_batch_size = batch[\"act\"].shape[0]\n\n                # Handle drop_last\n                if (\n                    self.cfg.drop_last\n                    and actual_batch_size &lt; self.cfg.batch_size\n                    and n + actual_batch_size &gt;= self.n_samples\n                ):\n                    break\n\n                n += actual_batch_size\n                yield self.ExampleBatch(**batch)\n                continue\n            except queue.Empty:\n                self.logger.info(\n                    \"Did not get a batch from manager process in %.1fs seconds.\",\n                    self.cfg.batch_timeout_s,\n                )\n            except FileNotFoundError:\n                self.logger.info(\"Manager process (probably) closed.\")\n                continue\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {n}/{self.n_samples} samples.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    if self.cfg.drop_last:\n        return self.n_samples // self.cfg.batch_size\n    else:\n        return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/","title":"saev.data","text":""},{"location":"api/data/saev.data/#saev.data","title":"<code>saev.data</code>","text":"<p>.. include:: ./protocol.md</p> <p>.. include:: ./performance.md</p>"},{"location":"api/data/saev.data/#saev.data.Dataset","title":"<code>Dataset(cfg)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset of activations from disk.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration set via CLI args.</p> <code>metadata</code> <code>Metadata</code> <p>Activations metadata; automatically loaded from disk.</p> <code>layer_index</code> <code>int</code> <p>Layer index into the shards if we are choosing a specific layer.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    # Check if labels.bin exists\n    labels_path = os.path.join(self.cfg.shard_root, \"labels.bin\")\n    self.labels_mmap = None\n    if os.path.exists(labels_path):\n        self.labels_mmap = np.memmap(\n            labels_path,\n            mode=\"r\",\n            dtype=np.uint8,\n            shape=(self.metadata.n_imgs, self.metadata.n_patches_per_img),\n        )\n\n    # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError.\n    self.layer_index = 1_000_000\n    if isinstance(self.cfg.layer, int):\n        err_msg = f\"Non-exact matches for .layer field not supported; {self.cfg.layer} not in {self.metadata.layers}.\"\n        assert self.cfg.layer in self.metadata.layers, err_msg\n        self.layer_index = self.metadata.layers.index(self.cfg.layer)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.Dataset.d_vit","title":"<code>d_vit</code>  <code>property</code>","text":"<p>Dimension of the underlying vision transformer's embedding space.</p>"},{"location":"api/data/saev.data/#saev.data.Dataset.Example","title":"<code>Example</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Dataset length depends on <code>patches</code> and <code>layer</code>.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Dataset length depends on `patches` and `layer`.\n    \"\"\"\n    match (self.cfg.patches, self.cfg.layer):\n        case (\"cls\", \"all\"):\n            # Return a CLS token from a random image and random layer.\n            return self.metadata.n_imgs * len(self.metadata.layers)\n        case (\"cls\", int()):\n            # Return a CLS token from a random image and fixed layer.\n            return self.metadata.n_imgs\n        case (\"image\", int()):\n            # Return a patch from a random image, fixed layer, and random patch.\n            return self.metadata.n_imgs * (self.metadata.n_patches_per_img)\n        case (\"image\", \"all\"):\n            # Return a patch from a random image, random layer and random patch.\n            return (\n                self.metadata.n_imgs\n                * len(self.metadata.layers)\n                * self.metadata.n_patches_per_img\n            )\n        case _:\n            typing.assert_never((self.cfg.patches, self.cfg.layer))\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.IndexedConfig","title":"<code>IndexedConfig(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, seed=17, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading indexed activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shard_root</code> <code>str</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>patches</code> <code>Literal['cls', 'image', 'all']</code> <p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/saev.data/#saev.data.Metadata","title":"<code>Metadata(vit_family, vit_ckpt, layers, n_patches_per_img, cls_token, d_vit, n_imgs, max_patches_per_shard, data, pixel_agg=None, dtype='float32', protocol='1.1')</code>  <code>dataclass</code>","text":""},{"location":"api/data/saev.data/#saev.data.Metadata.n_imgs_per_shard","title":"<code>n_imgs_per_shard</code>  <code>property</code>","text":"<p>Calculate the number of images per shard based on the protocol.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of images that fit in a shard.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig","title":"<code>OrderedConfig(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, batch_size=1024 * 16, batch_timeout_s=30.0, drop_last=False, buffer_size=64, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading ordered (non-shuffled) activation data from disk.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.patches","title":"<code>patches = 'image'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.shard_root","title":"<code>shard_root = os.path.join('.', 'shards')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Directory with .bin shards and a metadata.json file.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader","title":"<code>OrderedDataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that reads data from disk shards in order (no shuffling).</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self.logger = logging.getLogger(\"ordered.DataLoader\")\n    self.ctx = mp.get_context()\n    self.manager_proc = None\n    self.batch_queue = None\n    self.stop_event = None\n    self._n_samples = self._calculate_n_samples()\n    self.logger.info(\n        \"Initialized ordered.DataLoader with %d samples. (debug=%s)\",\n        self.n_samples,\n        self.cfg.debug,\n    )\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches in order.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterable[ExampleBatch]:\n    \"\"\"Yields batches in order.\"\"\"\n    self._start_manager()\n    n = 0\n\n    try:\n        while n &lt; self.n_samples:\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                batch = self.batch_queue.get(timeout=self.cfg.batch_timeout_s)\n                actual_batch_size = batch[\"act\"].shape[0]\n\n                # Handle drop_last\n                if (\n                    self.cfg.drop_last\n                    and actual_batch_size &lt; self.cfg.batch_size\n                    and n + actual_batch_size &gt;= self.n_samples\n                ):\n                    break\n\n                n += actual_batch_size\n                yield self.ExampleBatch(**batch)\n                continue\n            except queue.Empty:\n                self.logger.info(\n                    \"Did not get a batch from manager process in %.1fs seconds.\",\n                    self.cfg.batch_timeout_s,\n                )\n            except FileNotFoundError:\n                self.logger.info(\"Manager process (probably) closed.\")\n                continue\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {n}/{self.n_samples} samples.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    if self.cfg.drop_last:\n        return self.n_samples // self.cfg.batch_size\n    else:\n        return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig","title":"<code>ShuffledConfig(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, batch_size=1024 * 16, drop_last=False, scale_norm=False, ignore_labels=list(), n_threads=4, buffer_size=64, batch_timeout_s=30.0, seed=17, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading shuffled activation data from disk.</p> <p>Attributes:</p> Name Type Description <code>shard_root</code> <code>str</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>patches</code> <code>Literal['cls', 'image', 'all']</code> <p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.ignore_labels","title":"<code>ignore_labels = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.log_every_s","title":"<code>log_every_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.n_threads","title":"<code>n_threads = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of dataloading threads.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.scale_norm","title":"<code>scale_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to scale norms to sqrt(D).</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.seed","title":"<code>seed = 17</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader","title":"<code>ShuffledDataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that deterministically shuffles data from disk shards.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n\n    self.manager_proc = None\n    self.reservoir = None\n    self.stop_event = None\n\n    self.logger = logging.getLogger(\"shuffled.DataLoader\")\n    self.ctx = mp.get_context()\n\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    if self.cfg.scale_norm:\n        raise NotImplementedError(\"scale_norm not implemented.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self._n_samples = self._calculate_n_samples()\n\n    # Check if labels.bin exists for filtering\n    self.labels_mmap = None\n    if self.cfg.ignore_labels:\n        labels_path = os.path.join(self.cfg.shard_root, \"labels.bin\")\n        if not os.path.exists(labels_path):\n            raise FileNotFoundError(\n                f\"ignore_labels filtering requested but labels.bin not found at {labels_path}\"\n            )\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterator[ExampleBatch]:\n    \"\"\"Yields batches.\"\"\"\n    self._start_manager()\n    n, b = 0, 0\n\n    try:\n        while n &lt; self.n_samples:\n            need = min(self.cfg.batch_size, self.n_samples - n)\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                act, meta = self.reservoir.get(\n                    need, timeout=self.cfg.batch_timeout_s\n                )\n                n += need\n                b += 1\n                image_i, patch_i = meta.T\n                yield self.ExampleBatch(act=act, image_i=image_i, patch_i=patch_i)\n                continue\n            except TimeoutError:\n                if self.cfg.ignore_labels:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds. This can happen when filtering out many labels.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n                else:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {b}/{len(self)} batches.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.make_ordered_config","title":"<code>make_ordered_config(shuffled_cfg, **overrides)</code>","text":"<p>Create an <code>OrderedConfig</code> from a <code>ShuffledConfig</code>, with optional overrides.</p> <p>Defaults come from <code>shuffled_cfg</code> for fields present in <code>OrderedConfig</code>, and <code>overrides</code> take precedence. Unknown override fields raise <code>TypeError</code> from the <code>OrderedConfig</code> constructor, mirroring <code>dataclasses.replace</code>.</p> Source code in <code>src/saev/data/__init__.py</code> <pre><code>@beartype.beartype\ndef make_ordered_config(\n    shuffled_cfg: ShuffledConfig, **overrides: object\n) -&gt; OrderedConfig:\n    \"\"\"Create an `OrderedConfig` from a `ShuffledConfig`, with optional overrides.\n\n    Defaults come from `shuffled_cfg` for fields present in `OrderedConfig`, and `overrides` take precedence. Unknown override fields raise `TypeError` from the `OrderedConfig` constructor, mirroring `dataclasses.replace`.\n    \"\"\"\n    params: dict[str, object] = {}\n    for f in dataclasses.fields(OrderedConfig):\n        name = f.name\n        if hasattr(shuffled_cfg, name):\n            params[name] = getattr(shuffled_cfg, name)\n    params.update(overrides)\n    return OrderedConfig(**params)\n</code></pre>"},{"location":"api/data/shuffled/","title":"saev.data.shuffled","text":""},{"location":"api/data/shuffled/#saev.data.shuffled","title":"<code>saev.data.shuffled</code>","text":""},{"location":"api/data/shuffled/#saev.data.shuffled.Config","title":"<code>Config(shard_root=os.path.join('.', 'shards'), patches='image', layer=-2, batch_size=1024 * 16, drop_last=False, scale_norm=False, ignore_labels=list(), n_threads=4, buffer_size=64, batch_timeout_s=30.0, seed=17, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading shuffled activation data from disk.</p> <p>Attributes:</p> Name Type Description <code>shard_root</code> <code>str</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>patches</code> <code>Literal['cls', 'image', 'all']</code> <p>Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.ignore_labels","title":"<code>ignore_labels = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.log_every_s","title":"<code>log_every_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.n_threads","title":"<code>n_threads = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of dataloading threads.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.scale_norm","title":"<code>scale_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to scale norms to sqrt(D).</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.seed","title":"<code>seed = 17</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader","title":"<code>DataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that deterministically shuffles data from disk shards.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n\n    self.manager_proc = None\n    self.reservoir = None\n    self.stop_event = None\n\n    self.logger = logging.getLogger(\"shuffled.DataLoader\")\n    self.ctx = mp.get_context()\n\n    if not os.path.isdir(self.cfg.shard_root):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shard_root}'.\")\n\n    if self.cfg.scale_norm:\n        raise NotImplementedError(\"scale_norm not implemented.\")\n\n    self.metadata = writers.Metadata.load(self.cfg.shard_root)\n\n    # Validate shard files exist\n    shard_info = writers.ShardInfo.load(self.cfg.shard_root)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shard_root, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self._n_samples = self._calculate_n_samples()\n\n    # Check if labels.bin exists for filtering\n    self.labels_mmap = None\n    if self.cfg.ignore_labels:\n        labels_path = os.path.join(self.cfg.shard_root, \"labels.bin\")\n        if not os.path.exists(labels_path):\n            raise FileNotFoundError(\n                f\"ignore_labels filtering requested but labels.bin not found at {labels_path}\"\n            )\n</code></pre>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterator[ExampleBatch]:\n    \"\"\"Yields batches.\"\"\"\n    self._start_manager()\n    n, b = 0, 0\n\n    try:\n        while n &lt; self.n_samples:\n            need = min(self.cfg.batch_size, self.n_samples - n)\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                act, meta = self.reservoir.get(\n                    need, timeout=self.cfg.batch_timeout_s\n                )\n                n += need\n                b += 1\n                image_i, patch_i = meta.T\n                yield self.ExampleBatch(act=act, image_i=image_i, patch_i=patch_i)\n                continue\n            except TimeoutError:\n                if self.cfg.ignore_labels:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds. This can happen when filtering out many labels.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n                else:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {b}/{len(self)} batches.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/siglip/","title":"saev.data.siglip","text":""},{"location":"api/data/siglip/#saev.data.siglip","title":"<code>saev.data.siglip</code>","text":""},{"location":"api/data/siglip/#saev.data.siglip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>Module</code>, <code>VisionTransformer</code></p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    if ckpt.startswith(\"hf-hub:\"):\n        clip, _ = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        clip, _ = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    self._ckpt = ckpt\n\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model\n\n    assert isinstance(self.model, open_clip.timm_model.TimmModel)\n</code></pre>"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img=-1, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int = -1,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n    from PIL import Image\n\n    def resize(img: Image.Image) -&gt; Image.Image:\n        # SigLIP typically uses 224x224 or 384x384 images\n        # We'll assume 224x224 for simplicity\n        resize_size_px = (int(224 * scale), int(224 * scale))\n        return img.resize(resize_size_px, resample=resample)\n\n    return resize\n</code></pre>"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    if ckpt.startswith(\"hf-hub:\"):\n        _, img_transform = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        _, img_transform = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/transforms/","title":"saev.data.transforms","text":""},{"location":"api/data/transforms/#saev.data.transforms","title":"<code>saev.data.transforms</code>","text":""},{"location":"api/data/transforms/#saev.data.transforms.conv2d_to_tokens","title":"<code>conv2d_to_tokens(x_bchw, conv)</code>","text":"<p>Conv2d then flatten spatial to L, return (B, L, D).</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef conv2d_to_tokens(\n    x_bchw: Float[Tensor, \"b c h w\"], conv: nn.Conv2d\n) -&gt; Float[Tensor, \"b n d\"]:\n    \"\"\"Conv2d then flatten spatial to L, return (B, L, D).\"\"\"\n    y_bdhw = conv(x_bchw)\n    return einops.rearrange(y_bdhw, \"b d h w -&gt; b (h w) d\")\n</code></pre>"},{"location":"api/data/transforms/#saev.data.transforms.resize_to_patch_grid","title":"<code>resize_to_patch_grid(img, *, p, n, resample=Image.LANCZOS)</code>","text":"<p>Resize image to (w, h) so that:   - w % p == 0, h % p == 0   - (h/p) * (w/p) == N   - Minimizes change in aspect ratio.</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@beartype.beartype\ndef resize_to_patch_grid(\n    img: Image.Image,\n    *,\n    p: int,\n    n: int,\n    resample: Image.Resampling | int = Image.LANCZOS,\n) -&gt; Image.Image:\n    \"\"\"\n    Resize image to (w, h) so that:\n      - w % p == 0, h % p == 0\n      - (h/p) * (w/p) == N\n      - Minimizes change in aspect ratio.\n    \"\"\"\n    if p &lt;= 0 or n &lt;= 0:\n        raise ValueError(\"p and n must be positive integers\")\n\n    w0, h0 = img.size\n    a0 = w0 / h0\n\n    # Find the aspect ratio closest to a0\n    best_c = 0\n    best_dist = float(\"inf\")\n    for i in range(1, int(math.sqrt(n) + 1)):\n        if n % i != 0:\n            continue\n\n        for d in (i, n // i):\n            c, r = d, n // d\n            aspect = c / r\n            dist = abs(aspect - a0)\n\n            if dist &lt; best_dist:\n                best_c = d\n                best_dist = dist\n\n    c = best_c\n    r = n // c\n    w, h = c * p, r * p\n    return img.resize((w, h), resample=resample)\n</code></pre>"},{"location":"api/data/transforms/#saev.data.transforms.unfolded_conv2d","title":"<code>unfolded_conv2d(x_bchw, conv)</code>","text":"<p>Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1.</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef unfolded_conv2d(\n    x_bchw: Float[Tensor, \"b c h w\"], conv: nn.Conv2d\n) -&gt; Float[Tensor, \"b n d\"]:\n    \"\"\"\n    Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels.\n    Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1.\n    \"\"\"\n    k = conv.kernel_size[0]\n\n    assert conv.kernel_size == (k, k)\n    assert conv.stride == (k, k)\n    assert conv.padding == (0, 0)\n    assert conv.groups == 1\n    assert conv.dilation == (1, 1)\n\n    *b, c, h, w = x_bchw.shape\n\n    assert h % k == 0 and w % k == 0\n\n    tokens_bnd = einops.rearrange(\n        x_bchw, \"b c (hp p1) (wp p2) -&gt; b (hp wp) (c p1 p2)\", p1=k, p2=k\n    ).contiguous()\n    w_dp = conv.weight.reshape(conv.out_channels, c * k * k)\n    tokens_bnd = tokens_bnd @ w_dp.T\n    if conv.bias is not None:\n        tokens_bnd = tokens_bnd + conv.bias[None, None, :]\n    return tokens_bnd\n</code></pre>"},{"location":"api/data/writers/","title":"saev.data.writers","text":""},{"location":"api/data/writers/#saev.data.writers","title":"<code>saev.data.writers</code>","text":""},{"location":"api/data/writers/#saev.data.writers.Config","title":"<code>Config(data=datasets.Imagenet(), dump_to=os.path.join('.', 'shards'), vit_family='clip', vit_ckpt='ViT-L-14/openai', vit_batch_size=1024, n_workers=8, d_vit=1024, vit_layers=(lambda: [-2])(), n_patches_per_img=256, cls_token=True, pixel_agg='majority', max_patches_per_shard=2400000, ssl=True, device='cuda', n_hours=24.0, slurm_acct='', slurm_partition='', log_to='./logs')</code>  <code>dataclass</code>","text":"<p>Configuration for calculating and saving ViT activations.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.cls_token","title":"<code>cls_token = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the model has a [CLS] token.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.d_vit","title":"<code>d_vit = 1024</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dimension of the ViT activations (depends on model).</p>"},{"location":"api/data/writers/#saev.data.writers.Config.data","title":"<code>data = dataclasses.field(default_factory=(datasets.Imagenet))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which dataset to use.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.device","title":"<code>device = 'cuda'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which device to use.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.dump_to","title":"<code>dump_to = os.path.join('.', 'shards')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where to write shards.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.log_to","title":"<code>log_to = './logs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where to log Slurm job stdout/stderr.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.max_patches_per_shard","title":"<code>max_patches_per_shard = 2400000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.n_hours","title":"<code>n_hours = 24.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Slurm job length.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.n_patches_per_img","title":"<code>n_patches_per_img = 256</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of ViT patches per image (depends on model).</p>"},{"location":"api/data/writers/#saev.data.writers.Config.n_workers","title":"<code>n_workers = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of dataloader workers.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.slurm_acct","title":"<code>slurm_acct = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Slurm account string.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.slurm_partition","title":"<code>slurm_partition = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Slurm partition.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.ssl","title":"<code>ssl = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use SSL.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.vit_batch_size","title":"<code>vit_batch_size = 1024</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for ViT inference.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.vit_ckpt","title":"<code>vit_ckpt = 'ViT-L-14/openai'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Specific model checkpoint.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.vit_family","title":"<code>vit_family = 'clip'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which model family.</p>"},{"location":"api/data/writers/#saev.data.writers.Config.vit_layers","title":"<code>vit_layers = dataclasses.field(default_factory=(lambda: [-2]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which layers to save. By default, the second-to-last layer.</p>"},{"location":"api/data/writers/#saev.data.writers.IndexLookup","title":"<code>IndexLookup(metadata, patches, layer)</code>","text":"<p>Index &lt;-&gt; shard helper.</p> <p><code>map()</code>      \u2013 turn a global dataset index into precise physical offsets. <code>length()</code>   \u2013 dataset size for a particular (patches, layer) view.</p>"},{"location":"api/data/writers/#saev.data.writers.IndexLookup--parameters","title":"Parameters","text":"<p>metadata : Metadata     Pre-computed dataset statistics (images, patches, layers, shard size). patches: 'cls' | 'image' | 'all' layer: int | 'all'</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def __init__(\n    self,\n    metadata: Metadata,\n    patches: tp.Literal[\"cls\", \"image\", \"all\"],\n    layer: int | tp.Literal[\"all\"],\n):\n    if not metadata.cls_token and patches == \"cls\":\n        raise ValueError(\"Cannot return [CLS] token if one isn't present.\")\n\n    self.metadata = metadata\n    self.patches = patches\n\n    if isinstance(layer, int) and layer not in metadata.layers:\n        raise ValueError(f\"Layer {layer} not in {metadata.layers}.\")\n    self.layer = layer\n    self.layer_to_idx = {layer: i for i, layer in enumerate(metadata.layers)}\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_global","title":"<code>map_global(i)</code>","text":""},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_global--return","title":"Return","text":"<p>(     shard_i,     index in shard (img_i_in_shard, layer_i, token_i) )</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def map_global(self, i: int) -&gt; tuple[int, tuple[int, int, int]]:\n    \"\"\"\n    Return\n    -------\n    (\n        shard_i,\n        index in shard (img_i_in_shard, layer_i, token_i)\n    )\n    \"\"\"\n    n = self.length()\n\n    if i &lt; 0 or i &gt;= n:\n        raise IndexError(f\"{i=} out of range [0, {n})\")\n\n    match (self.patches, self.layer):\n        case (\"cls\", \"all\"):\n            # For CLS token with all layers, i represents (img_idx * n_layers + layer_idx)\n            n_layers = len(self.metadata.layers)\n            img_i = i // n_layers\n            layer_idx = i % n_layers\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            # CLS token is at position 0\n            return shard_i, (img_i_in_shard, layer_idx, 0)\n        case (\"cls\", int()):\n            # For CLS token with specific layer, i is the image index\n            img_i = i\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            # CLS token is at position 0\n            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], 0)\n        case (\"image\", int()):\n            # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx)\n            img_i = i // self.metadata.n_patches_per_img\n            token_i = i % self.metadata.n_patches_per_img\n\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)\n        case (\"image\", \"all\"):\n            # For image patches with all layers\n            # Total patches per image across all layers\n            total_patches_per_img = self.metadata.n_patches_per_img * len(\n                self.metadata.layers\n            )\n\n            # Calculate which image and which patch within that image across all layers\n            img_i = i // total_patches_per_img\n            remainder = i % total_patches_per_img\n\n            # Calculate which layer and which patch within that layer\n            layer_idx = remainder // self.metadata.n_patches_per_img\n            token_i = remainder % self.metadata.n_patches_per_img\n\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            return shard_i, (img_i_in_shard, layer_idx, token_i)\n        case (\"all\", int()):\n            n_tokens_per_img = self.metadata.n_patches_per_img + (\n                1 if self.metadata.cls_token else 0\n            )\n            img_i = i // n_tokens_per_img\n            token_i = i % n_tokens_per_img\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)\n        case (\"all\", \"all\"):\n            # For all tokens (CLS + patches) with all layers\n            # Calculate total tokens per image across all layers\n            n_tokens_per_img = self.metadata.n_patches_per_img + (\n                1 if self.metadata.cls_token else 0\n            )\n            total_tokens_per_img = n_tokens_per_img * len(self.metadata.layers)\n\n            # Calculate which image and which token within that image\n            img_i = i // total_tokens_per_img\n            remainder = i % total_tokens_per_img\n\n            # Calculate which layer and which token within that layer\n            layer_idx = remainder // n_tokens_per_img\n            token_i = remainder % n_tokens_per_img\n\n            # Map to physical location\n            shard_i, img_i_in_shard = self.map_img(img_i)\n            return shard_i, (img_i_in_shard, layer_idx, token_i)\n\n        case _:\n            tp.assert_never((self.patches, self.layer))\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_img","title":"<code>map_img(img_i)</code>","text":""},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_img--return","title":"Return","text":"<p>(shard_i, img_i_in_shard)</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def map_img(self, img_i: int) -&gt; tuple[int, int]:\n    \"\"\"\n    Return\n    -------\n    (shard_i, img_i_in_shard)\n    \"\"\"\n    if img_i &lt; 0 or img_i &gt;= self.metadata.n_imgs:\n        raise IndexError(f\"{img_i=} out of range [0, {self.metadata.n_imgs})\")\n\n    # Calculate which shard contains this image\n    shard_i = img_i // self.metadata.n_imgs_per_shard\n    img_i_in_shard = img_i % self.metadata.n_imgs_per_shard\n\n    return shard_i, img_i_in_shard\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter","title":"<code>LabelsWriter(cfg)</code>","text":"<p>LabelsWriter handles writing patch-level segmentation labels to a single binary file.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.logger = logging.getLogger(\"labels-writer\")\n    self.root = get_acts_dir(cfg)\n    self.n_patches_per_img = cfg.n_patches_per_img\n    self.n_imgs = cfg.data.n_imgs\n    self.has_written = False\n    self.current_idx = 0\n\n    # Always create memory-mapped file for labels\n    # If nothing is written, it will be deleted in flush()\n    self.labels_path = os.path.join(self.root, \"labels.bin\")\n    self.labels = np.memmap(\n        self.labels_path,\n        mode=\"w+\",\n        dtype=np.uint8,\n        shape=(self.n_imgs, self.n_patches_per_img),\n    )\n    self.logger.info(\"Opened labels file '%s'.\", self.labels_path)\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter.flush","title":"<code>flush()</code>","text":"<p>Flush the memory-mapped file to disk if anything was written.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush the memory-mapped file to disk if anything was written.\"\"\"\n    if self.labels is not None and self.has_written:\n        self.labels.flush()\n        self.logger.info(\"Flushed labels to '%s'.\", self.labels_path)\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter.write_batch","title":"<code>write_batch(batch_labels, start_idx)</code>","text":"<p>Write a batch of labels to the memory-mapped file.</p> <p>Parameters:</p> Name Type Description Default <code>batch_labels</code> <code>ndarray | Tensor</code> <p>Array of shape (batch_size, n_patches_per_img) with uint8 dtype</p> required <code>start_idx</code> <code>int</code> <p>Starting index in the global labels array</p> required Source code in <code>src/saev/data/writers.py</code> <pre><code>@beartype.beartype\ndef write_batch(self, batch_labels: np.ndarray | Tensor, start_idx: int):\n    \"\"\"\n    Write a batch of labels to the memory-mapped file.\n\n    Args:\n        batch_labels: Array of shape (batch_size, n_patches_per_img) with uint8 dtype\n        start_idx: Starting index in the global labels array\n    \"\"\"\n    # Convert to numpy if needed\n    if isinstance(batch_labels, torch.Tensor):\n        batch_labels = batch_labels.cpu().numpy()\n\n    batch_size = len(batch_labels)\n    assert start_idx + batch_size &lt;= self.n_imgs\n    assert batch_labels.shape == (batch_size, self.n_patches_per_img)\n    assert batch_labels.dtype == np.uint8\n\n    self.labels[start_idx : start_idx + batch_size] = batch_labels\n    self.current_idx = start_idx + batch_size\n    self.has_written = True\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.Metadata","title":"<code>Metadata(vit_family, vit_ckpt, layers, n_patches_per_img, cls_token, d_vit, n_imgs, max_patches_per_shard, data, pixel_agg=None, dtype='float32', protocol='1.1')</code>  <code>dataclass</code>","text":""},{"location":"api/data/writers/#saev.data.writers.Metadata.n_imgs_per_shard","title":"<code>n_imgs_per_shard</code>  <code>property</code>","text":"<p>Calculate the number of images per shard based on the protocol.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of images that fit in a shard.</p>"},{"location":"api/data/writers/#saev.data.writers.Shard","title":"<code>Shard(name, n_imgs)</code>  <code>dataclass</code>","text":"<p>A single shard entry in shards.json, recording the filename and number of images.</p>"},{"location":"api/data/writers/#saev.data.writers.ShardInfo","title":"<code>ShardInfo(shards=list())</code>  <code>dataclass</code>","text":"<p>A read-only container for shard metadata as recorded in shards.json.</p>"},{"location":"api/data/writers/#saev.data.writers.ShardWriter","title":"<code>ShardWriter(cfg)</code>","text":"<p>ShardWriter is a stateful object that handles sharded activation writing to disk.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.logger = logging.getLogger(\"shard-writer\")\n\n    self.root = get_acts_dir(cfg)\n\n    n_patches_per_img = cfg.n_patches_per_img\n    if cfg.cls_token:\n        n_patches_per_img += 1\n    self.n_imgs_per_shard = (\n        cfg.max_patches_per_shard // len(cfg.vit_layers) // n_patches_per_img\n    )\n    self.shape = (\n        self.n_imgs_per_shard,\n        len(cfg.vit_layers),\n        n_patches_per_img,\n        cfg.d_vit,\n    )\n\n    # builder for shard manifest\n    self._shards: ShardInfo = ShardInfo()\n\n    # Always initialize labels writer (it handles non-seg datasets internally)\n    self.labels_writer = LabelsWriter(cfg)\n\n    self.shard = -1\n    self.acts = None\n    self.next_shard()\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit - handle cleanup.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - handle cleanup.\"\"\"\n    self.flush()\n\n    # Delete empty labels file if nothing was written\n    if not self.labels_writer.has_written:\n        if os.path.exists(self.labels_writer.labels_path):\n            os.remove(self.labels_writer.labels_path)\n            self.logger.info(\n                \"Removed empty labels file '%s'.\", self.labels_writer.labels_path\n            )\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.write_batch","title":"<code>write_batch(activations, start_idx, patch_labels=None)</code>","text":"<p>Write a batch of activations and optionally patch labels.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Float[Tensor, 'batch n_layers all_patches d_vit']</code> <p>Batch of activations to write.</p> required <code>start_idx</code> <code>int</code> <p>Starting index for this batch.</p> required <code>patch_labels</code> <code>UInt8[Tensor, 'batch n_patches'] | None</code> <p>Optional patch labels for segmentation datasets.</p> <code>None</code> Source code in <code>src/saev/data/writers.py</code> <pre><code>def write_batch(\n    self,\n    activations: Float[Tensor, \"batch n_layers all_patches d_vit\"],\n    start_idx: int,\n    patch_labels: UInt8[Tensor, \"batch n_patches\"] | None = None,\n) -&gt; None:\n    \"\"\"Write a batch of activations and optionally patch labels.\n\n    Args:\n        activations: Batch of activations to write.\n        start_idx: Starting index for this batch.\n        patch_labels: Optional patch labels for segmentation datasets.\n    \"\"\"\n    batch_size = len(activations)\n    end_idx = start_idx + batch_size\n\n    # Write activations (handling sharding)\n    offset = self.n_imgs_per_shard * self.shard\n\n    if end_idx &gt;= offset + self.n_imgs_per_shard:\n        # We have run out of space in this mmap'ed file. Let's fill it as much as we can.\n        n_fit = offset + self.n_imgs_per_shard - start_idx\n        self.acts[start_idx - offset : start_idx - offset + n_fit] = activations[\n            :n_fit\n        ]\n        self.filled = start_idx - offset + n_fit\n\n        # Write labels for the portion that fits\n        if patch_labels is not None:\n            # Convert to numpy uint8 if needed\n            if isinstance(patch_labels, torch.Tensor):\n                labels_to_write = (\n                    patch_labels[:n_fit].cpu().numpy().astype(np.uint8)\n                )\n            elif not isinstance(patch_labels, np.ndarray):\n                labels_to_write = np.array(patch_labels[:n_fit], dtype=np.uint8)\n            else:\n                labels_to_write = patch_labels[:n_fit]\n\n            self.labels_writer.write_batch(labels_to_write, start_idx)\n\n        self.next_shard()\n\n        # Recursively call write_batch for remaining data\n        if n_fit &lt; batch_size:\n            self.write_batch(\n                activations[n_fit:],\n                start_idx + n_fit,\n                patch_labels[n_fit:] if patch_labels is not None else None,\n            )\n    else:\n        msg = f\"0 &lt;= {start_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}\"\n        assert 0 &lt;= start_idx - offset &lt;= offset + self.n_imgs_per_shard, msg\n        msg = f\"0 &lt;= {end_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}\"\n        assert 0 &lt;= end_idx - offset &lt;= offset + self.n_imgs_per_shard, msg\n        self.acts[start_idx - offset : end_idx - offset] = activations\n        self.filled = end_idx - offset\n\n        # Write labels if provided\n        if patch_labels is not None:\n            # Convert to numpy uint8 if needed\n            if isinstance(patch_labels, torch.Tensor):\n                patch_labels = patch_labels.cpu().numpy().astype(np.uint8)\n            elif not isinstance(patch_labels, np.ndarray):\n                patch_labels = np.array(patch_labels, dtype=np.uint8)\n\n            self.labels_writer.write_batch(patch_labels, start_idx)\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.get_acts_dir","title":"<code>get_acts_dir(cfg)</code>","text":"<p>Return the activations directory based on the relevant values of a config. Also saves a metadata.json file to that directory for human reference.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Config</code> <p>Config for experiment.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Directory to where activations should be dumped/loaded from.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>@beartype.beartype\ndef get_acts_dir(cfg: Config) -&gt; str:\n    \"\"\"\n    Return the activations directory based on the relevant values of a config.\n    Also saves a metadata.json file to that directory for human reference.\n\n    Args:\n        cfg: Config for experiment.\n\n    Returns:\n        Directory to where activations should be dumped/loaded from.\n    \"\"\"\n    metadata = Metadata.from_cfg(cfg)\n\n    acts_dir = os.path.join(cfg.dump_to, metadata.hash)\n    os.makedirs(acts_dir, exist_ok=True)\n\n    metadata.dump(acts_dir)\n\n    return acts_dir\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.get_dataloader","title":"<code>get_dataloader(cfg, *, img_tr=None, seg_tr=None, sample_tr=None)</code>","text":"<p>Get a dataloader for a default map-style dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Config</code> <p>Config.</p> required <code>img_tr</code> <code>Callable | None</code> <p>Image transform to be applied to each image.</p> <code>None</code> <code>seg_tr</code> <code>Callable | None</code> <p>Segmentation transform to be applied to masks.</p> <code>None</code> <code>sample_tr</code> <code>Callable | None</code> <p>Transform to be applied to sample dicts.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A PyTorch Dataloader that yields dictionaries with <code>'image'</code> keys containing image batches, <code>'index'</code> keys containing original dataset indices and <code>'label'</code> keys containing label batches.</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>@beartype.beartype\ndef get_dataloader(\n    cfg: Config,\n    *,\n    img_tr: Callable | None = None,\n    seg_tr: Callable | None = None,\n    sample_tr: Callable | None = None,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"\n    Get a dataloader for a default map-style dataset.\n\n    Args:\n        cfg: Config.\n        img_tr: Image transform to be applied to each image.\n        seg_tr: Segmentation transform to be applied to masks.\n        sample_tr: Transform to be applied to sample dicts.\n\n    Returns:\n        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches.\n    \"\"\"\n    dataset = datasets.get_dataset(\n        cfg.data, img_transform=img_tr, seg_transform=seg_tr, sample_transform=sample_tr\n    )\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=cfg.vit_batch_size,\n        drop_last=False,\n        num_workers=cfg.n_workers,\n        persistent_workers=cfg.n_workers &gt; 0,\n        shuffle=False,\n        pin_memory=False,\n    )\n    return dataloader\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.pixel_to_patch_labels","title":"<code>pixel_to_patch_labels(seg, n_patches, patch_size, pixel_agg='majority', bg_label=0, max_classes=256)</code>","text":"<p>Convert pixel-level segmentation to patch-level labels using vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>seg</code> <code>Image</code> <p>Pixel-level segmentation mask as PIL Image</p> required <code>n_patches</code> <code>int</code> <p>Total number of patches expected</p> required <code>patch_size</code> <code>int</code> <p>Size of each patch in pixels</p> required <code>pixel_agg</code> <code>Literal['majority', 'prefer-fg']</code> <p>How to aggregate pixel labels into patch labels</p> <code>'majority'</code> <code>bg_label</code> <code>int</code> <p>Background label index</p> <code>0</code> <code>max_classes</code> <code>int</code> <p>Maximum number of classes (for bincount)</p> <code>256</code> <p>Returns:</p> Type Description <code>UInt8[Tensor, ' n_patches']</code> <p>Patch labels as uint8 tensor of shape (n_patches,)</p> Source code in <code>src/saev/data/writers.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef pixel_to_patch_labels(\n    seg: Image.Image,\n    n_patches: int,\n    patch_size: int,\n    pixel_agg: tp.Literal[\"majority\", \"prefer-fg\"] = \"majority\",\n    bg_label: int = 0,\n    max_classes: int = 256,\n) -&gt; UInt8[Tensor, \" n_patches\"]:\n    \"\"\"\n    Convert pixel-level segmentation to patch-level labels using vectorized operations.\n\n    Args:\n        seg: Pixel-level segmentation mask as PIL Image\n        n_patches: Total number of patches expected\n        patch_size: Size of each patch in pixels\n        pixel_agg: How to aggregate pixel labels into patch labels\n        bg_label: Background label index\n        max_classes: Maximum number of classes (for bincount)\n\n    Returns:\n        Patch labels as uint8 tensor of shape (n_patches,)\n    \"\"\"\n    # Convert to torch tensor for vectorized operations\n    seg_tensor = torch.from_numpy(np.array(seg, dtype=np.uint8))\n    assert seg_tensor.ndim == 2\n\n    h, w = seg_tensor.shape\n\n    # Calculate patch grid dimensions\n    patch_grid_h = h // patch_size\n    patch_grid_w = w // patch_size\n    assert patch_grid_w * patch_grid_h == n_patches, (\n        f\"Image size {w}x{h} with patch_size {patch_size} gives {patch_grid_w}x{patch_grid_h} = {patch_grid_w * patch_grid_h} patches, expected {n_patches}\"\n    )\n\n    # Reshape into patches using einops: (n_patches, patch_size * patch_size)\n    patches = einops.rearrange(\n        seg_tensor,\n        \"(h p1) (w p2) -&gt; (h w) (p1 p2)\",\n        p1=patch_size,\n        p2=patch_size,\n        h=patch_grid_h,\n        w=patch_grid_w,\n    )\n\n    # Use vectorized bincount approach to get class counts for all patches at once\n    # counts[i, c] = number of times class c appears in patch i\n    offsets = torch.arange(n_patches, device=patches.device).unsqueeze(1) * max_classes\n    flat = (patches + offsets).reshape(-1)\n    counts = torch.bincount(flat, minlength=n_patches * max_classes).reshape(\n        n_patches, max_classes\n    )\n\n    if pixel_agg == \"majority\":\n        # Take the most common label in each patch\n        patch_labels = counts.argmax(dim=1)\n    elif pixel_agg == \"prefer-fg\":\n        # Take the most common non-background label, or background if all background\n        nonbg = counts.clone()\n        nonbg[:, bg_label] = 0\n        has_nonbg = nonbg.sum(dim=1) &gt; 0\n        nonbg_arg = nonbg.argmax(dim=1)\n        bg_tensor = torch.full_like(nonbg_arg, bg_label)\n        patch_labels = torch.where(has_nonbg, nonbg_arg, bg_tensor)\n    else:\n        tp.assert_never(pixel_agg)\n\n    return patch_labels.to(torch.uint8)\n</code></pre>"},{"location":"api/data/writers/#saev.data.writers.worker_fn","title":"<code>worker_fn(cfg)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Config</code> <p>Config for activations.</p> required Source code in <code>src/saev/data/writers.py</code> <pre><code>@beartype.beartype\ndef worker_fn(cfg: Config):\n    \"\"\"\n    Args:\n        cfg: Config for activations.\n    \"\"\"\n    from . import models\n\n    if torch.cuda.is_available():\n        # This enables tf32 on Ampere GPUs which is only 8% slower than\n        # float16 and almost as accurate as float32\n        # This was a default in pytorch until 1.12\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n\n    log_format = \"[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=log_format)\n    logger = logging.getLogger(\"worker_fn\")\n\n    if cfg.device == \"cuda\" and not torch.cuda.is_available():\n        logger.warning(\"No CUDA device available, using CPU.\")\n        cfg = dataclasses.replace(cfg, device=\"cpu\")\n\n    vit_cls = models.load_vit_cls(cfg.vit_family)\n    vit_instance = vit_cls(cfg.vit_ckpt).to(cfg.device)\n    vit = RecordedVisionTransformer(\n        vit_instance, cfg.n_patches_per_img, cfg.cls_token, cfg.vit_layers\n    )\n\n    img_tr, sample_tr = vit_cls.make_transforms(cfg.vit_ckpt, cfg.n_patches_per_img)\n\n    seg_tr = None\n    if _is_segmentation_dataset(cfg.data):\n        # For segmentation datasets, create a transform that converts pixels to patches\n        # Use make_resize with NEAREST interpolation for segmentation masks\n        seg_resize_tr = vit_cls.make_resize(\n            cfg.vit_ckpt, cfg.n_patches_per_img, scale=1.0, resample=Image.NEAREST\n        )\n\n        def seg_to_patches(seg):\n            \"\"\"Transform that resizes segmentation and converts to patch labels.\"\"\"\n\n            # Convert to patch labels\n            return pixel_to_patch_labels(\n                seg_resize_tr(seg),\n                cfg.n_patches_per_img,\n                patch_size=vit_instance.patch_size,\n                pixel_agg=cfg.pixel_agg,\n                bg_label=cfg.data.bg_label,\n            )\n\n        seg_tr = seg_to_patches\n\n    dataloader = get_dataloader(cfg, img_tr=img_tr, seg_tr=seg_tr, sample_tr=sample_tr)\n\n    n_batches = math.ceil(cfg.data.n_imgs / cfg.vit_batch_size)\n    logger.info(\"Dumping %d batches of %d examples.\", n_batches, cfg.vit_batch_size)\n\n    vit = vit.to(cfg.device)\n    # vit = torch.compile(vit)\n\n    # Use context manager for proper cleanup\n    with ShardWriter(cfg) as writer:\n        i = 0\n        # Calculate and write ViT activations.\n        with torch.inference_mode():\n            for batch in helpers.progress(dataloader, total=n_batches):\n                imgs = batch.get(\"image\").to(cfg.device)\n                grid = batch.get(\"grid\")\n                if grid is not None:\n                    grid = grid.to(cfg.device)\n                    out, cache = vit(imgs, grid=grid)\n                else:\n                    out, cache = vit(imgs)\n                # cache has shape [batch size, n layers, n patches + 1, d vit]\n                del out\n\n                # Write activations and labels (if present) in one call\n                patch_labels = batch.get(\"patch_labels\")\n                if patch_labels is not None:\n                    logger.debug(\n                        f\"Found patch_labels in batch: shape={patch_labels.shape if hasattr(patch_labels, 'shape') else 'unknown'}\"\n                    )\n                    # Ensure correct shape\n                    assert patch_labels.shape == (len(cache), cfg.n_patches_per_img)\n                else:\n                    logger.debug(f\"No patch_labels in batch. Keys: {batch.keys()}\")\n\n                writer.write_batch(cache, i, patch_labels=patch_labels)\n\n                i += len(cache)\n</code></pre>"},{"location":"api/nn/modeling/","title":"saev.nn.modeling","text":""},{"location":"api/nn/modeling/#saev.nn.modeling","title":"<code>saev.nn.modeling</code>","text":"<p>Neural network architectures for sparse autoencoders.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK","title":"<code>BatchTopK(top_k=32)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK.top_k","title":"<code>top_k = 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many values are allowed to be non-zero per sample in the batch.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation","title":"<code>BatchTopKActivation(cfg=BatchTopK())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Batch Top-K activation function. For use as activation function of sparse encoder. Applies top-k selection per sample in the batch.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: BatchTopK = BatchTopK()):\n    super().__init__()\n    self.cfg = cfg\n    self.k = cfg.top_k\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation.forward","title":"<code>forward(x)</code>","text":"<p>Apply top-k activation to each sample in the batch.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(self, x: Float[Tensor, \"batch d_sae\"]) -&gt; Float[Tensor, \"batch d_sae\"]:\n    \"\"\"\n    Apply top-k activation to each sample in the batch.\n    \"\"\"\n    if self.k &lt;= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # Handle case where k exceeds number of elements per sample\n    k = min(self.k, x.shape[-1])\n\n    # Apply top-k per sample (along the last dimension)\n    k_vals, k_inds = torch.topk(x, k, dim=-1, sorted=False)\n    mask = torch.zeros_like(x).scatter_(\n        dim=-1, index=k_inds, src=torch.ones_like(x)\n    )\n\n    return torch.mul(mask, x)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.Relu","title":"<code>Relu()</code>  <code>dataclass</code>","text":"<p>Vanilla ReLU</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder","title":"<code>SparseAutoencoder(cfg)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse auto-encoder (SAE) using L1 sparsity penalty.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: SparseAutoencoderConfig):\n    super().__init__()\n\n    self.cfg = cfg\n    self.logger = logging.getLogger(f\"sae(seed={cfg.seed})\")\n\n    self.W_enc = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_vit, cfg.d_sae))\n    )\n    self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))\n\n    self.W_dec = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_vit))\n    )\n    self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_vit))\n\n    self.normalize_w_dec()\n\n    self.activation = get_activation(cfg.activation)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.decode","title":"<code>decode(f_x, *, prefixes=None)</code>","text":"<p>Decode latent features to reconstructions.</p> <p>Parameters:</p> Name Type Description Default <code>f_x</code> <code>Float[Tensor, 'batch d_sae']</code> <p>Latent features of shape (batch, d_sae)</p> required <code>prefixes</code> <code>Int64[Tensor, ' n_prefixes'] | None</code> <p>Optional tensor of prefix lengths for Matryoshka decoding.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_prefixes d_model']</code> <p>Matryoshka reconstructions (batch, n_prefixes, d_model).</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def decode(\n    self,\n    f_x: Float[Tensor, \"batch d_sae\"],\n    *,\n    prefixes: Int64[Tensor, \" n_prefixes\"] | None = None,\n) -&gt; Float[Tensor, \"batch n_prefixes d_model\"]:\n    \"\"\"\n    Decode latent features to reconstructions.\n\n    Args:\n        f_x: Latent features of shape (batch, d_sae)\n        prefixes: Optional tensor of prefix lengths for Matryoshka decoding.\n\n    Returns:\n        Matryoshka reconstructions (batch, n_prefixes, d_model).\n    \"\"\"\n    b, d_sae = f_x.shape\n\n    # Matryoshka cumulative decode\n    device = f_x.device\n    if prefixes is None:\n        prefixes = torch.tensor([d_sae], dtype=torch.int64)\n    assert torch.all(prefixes[1:] &gt; prefixes[:-1])\n    assert 1 &lt;= int(prefixes[0]) and int(prefixes[-1]) == d_sae\n    prefixes = prefixes.to(device)\n\n    # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ...\n    block_indices = torch.cat([\n        torch.tensor([0], dtype=prefixes.dtype, device=device),\n        prefixes,\n    ])\n    blocks = list(zip(block_indices[:-1], block_indices[1:]))\n\n    # Compute block outputs\n    block_outputs = []\n    for i, (start, end) in enumerate(blocks):\n        # Each block uses its portion of f_x and W_dec\n        block_f_x = f_x[:, start:end]\n        block_W_dec = self.W_dec[start:end, :]\n\n        # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -&gt; (batch, d_vit)\n        # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit)\n        block_output = einops.einsum(\n            block_f_x,\n            block_W_dec,\n            \"... d_sae_block, d_sae_block d_vit -&gt; ... d_vit\",\n        )\n\n        # Add bias only to the first block\n        if i == 0:\n            block_output = block_output + self.b_dec\n\n        block_outputs.append(block_output)\n\n    # Cumulative sum to get prefix reconstructions\n    x_hats = torch.cumsum(torch.stack(block_outputs, dim=-2), dim=-2)\n\n    return x_hats\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Given x, calculates the reconstructed x_hat and the intermediate activations f_x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model']</code> <p>a batch of ViT activations.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, \"batch d_model\"]\n) -&gt; tuple[Float[Tensor, \"batch d_model\"], Float[Tensor, \"batch d_sae\"]]:\n    \"\"\"\n    Given x, calculates the reconstructed x_hat and the intermediate activations f_x.\n\n    Arguments:\n        x: a batch of ViT activations.\n    \"\"\"\n    f_x = self.encode(x)\n    x_hat = self.decode(f_x)\n\n    return x_hat, f_x\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.normalize_w_dec","title":"<code>normalize_w_dec()</code>","text":"<p>Set W_dec to unit-norm columns.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef normalize_w_dec(self):\n    \"\"\"\n    Set W_dec to unit-norm columns.\n    \"\"\"\n    if self.cfg.normalize_w_dec:\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.remove_parallel_grads","title":"<code>remove_parallel_grads()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_vit) shape</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef remove_parallel_grads(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_vit) shape\n    \"\"\"\n    if not self.cfg.remove_parallel_grads:\n        return\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_vit, d_sae d_vit -&gt; d_sae\",\n    )\n\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_vit -&gt; d_sae d_vit\",\n    )\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig(d_vit=1024, exp_factor=16, n_reinit_samples=1024 * 16 * 32, remove_parallel_grads=True, normalize_w_dec=True, seed=0, activation=Relu())</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.exp_factor","title":"<code>exp_factor = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion factor for SAE.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples","title":"<code>n_reinit_samples = 1024 * 16 * 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec","title":"<code>normalize_w_dec = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads","title":"<code>remove_parallel_grads = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.seed","title":"<code>seed = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopK","title":"<code>TopK(top_k=32)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.TopK.top_k","title":"<code>top_k = 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many values are allowed to be non-zero.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation","title":"<code>TopKActivation(cfg=TopK())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Top-K activation function. For use as activation function of sparse encoder.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: TopK = TopK()):\n    super().__init__()\n    self.cfg = cfg\n    self.k = cfg.top_k\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation.forward","title":"<code>forward(x)</code>","text":"<p>Apply top-k activation to the input tensor.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(self, x: Float[Tensor, \"batch d_sae\"]) -&gt; Float[Tensor, \"batch d_sae\"]:\n    \"\"\"\n    Apply top-k activation to the input tensor.\n    \"\"\"\n    if self.k &lt;= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    k_vals, k_inds = torch.topk(x, self.k, dim=-1, sorted=False)\n    mask = torch.zeros_like(x).scatter_(\n        dim=-1, index=k_inds, src=torch.ones_like(x)\n    )\n\n    return torch.mul(mask, x)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.dump","title":"<code>dump(fpath, sae)</code>","text":"<p>Save an SAE checkpoint to disk along with configuration, using the trick from equinox.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str</code> <p>filepath to save checkpoint to.</p> required <code>sae</code> <code>SparseAutoencoder</code> <p>sparse autoencoder checkpoint to save.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef dump(fpath: str, sae: SparseAutoencoder):\n    \"\"\"\n    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).\n\n    Arguments:\n        fpath: filepath to save checkpoint to.\n        sae: sparse autoencoder checkpoint to save.\n    \"\"\"\n    # Custom serialization to handle activation object\n    cfg_dict = dataclasses.asdict(sae.cfg)\n    # Replace activation dict with custom format\n    activation = sae.cfg.activation\n    cfg_dict[\"activation\"] = {\n        \"cls\": activation.__class__.__name__,\n        \"params\": dataclasses.asdict(activation),\n    }\n\n    header = {\n        \"schema\": 2,\n        \"cfg\": cfg_dict,\n        \"commit\": helpers.current_git_commit() or \"unknown\",\n        \"lib\": __version__,\n    }\n\n    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n    with open(fpath, \"wb\") as fd:\n        header_str = json.dumps(header)\n        fd.write((header_str + \"\\n\").encode(\"utf-8\"))\n        torch.save(sae.state_dict(), fd)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.load","title":"<code>load(fpath, *, device='cpu')</code>","text":"<p>Loads a sparse autoencoder from disk.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef load(fpath: str, *, device=\"cpu\") -&gt; SparseAutoencoder:\n    \"\"\"\n    Loads a sparse autoencoder from disk.\n    \"\"\"\n    with open(fpath, \"rb\") as fd:\n        header = json.loads(fd.readline())\n        buffer = io.BytesIO(fd.read())\n\n    if \"schema\" not in header:\n        # Original, pre-schema format: just raw config parameters\n        # Remove old parameters that no longer exist\n        for keyword in (\"sparsity_coeff\", \"ghost_grads\", \"l1_coeff\", \"use_ghost_grads\"):\n            header.pop(keyword, None)\n        # Legacy format - create SparseAutoencoderConfig with Relu activation\n        cfg = SparseAutoencoderConfig(**header, activation=Relu())\n    elif header[\"schema\"] == 1:\n        # Schema version 1: A cautionary tale of poor version management\n        #\n        # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent!\n        #\n        # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.)\n        # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict\n        #\n        # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format.\n        #\n        # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number!\n\n        cls_name = header.get(\"cls\", \"SparseAutoencoderConfig\")\n        cfg_dict = header[\"cfg\"]\n\n        if cls_name in [\"Relu\", \"TopK\", \"BatchTopK\"]:\n            # Format 1A: Old format where cls indicates the activation type\n            activation_cls = globals()[cls_name]\n            if cls_name in [\"TopK\", \"BatchTopK\"]:\n                activation = activation_cls(top_k=cfg_dict.get(\"top_k\", 32))\n            else:\n                activation = activation_cls()\n            cfg = SparseAutoencoderConfig(**cfg_dict, activation=activation)\n        else:\n            # Format 1B: Newer format with activation as dict\n            if \"activation\" in cfg_dict:\n                activation_info = cfg_dict[\"activation\"]\n                activation_cls = globals()[activation_info[\"cls\"]]\n                activation = activation_cls(**activation_info[\"params\"])\n                cfg_dict[\"activation\"] = activation\n            cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 2:\n        # Schema version 2: cleaner format with activation serialization\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    else:\n        raise ValueError(f\"Unknown schema version: {header['schema']}\")\n\n    model = SparseAutoencoder(cfg)\n    model.load_state_dict(torch.load(buffer, weights_only=True, map_location=device))\n    return model\n</code></pre>"},{"location":"api/nn/objectives/","title":"saev.nn.objectives","text":""},{"location":"api/nn/objectives/#saev.nn.objectives","title":"<code>saev.nn.objectives</code>","text":""},{"location":"api/nn/objectives/#saev.nn.objectives.Loss","title":"<code>Loss()</code>  <code>dataclass</code>","text":"<p>The loss term for an autoencoder training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Loss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka","title":"<code>Matryoshka(sparsity_coeff=0.0004, n_prefixes=10)</code>  <code>dataclass</code>","text":"<p>Config for the Matryoshka loss for another arbitrary SAE class.</p> <p>Reference code is here: https://github.com/noanabeshima/matryoshka-saes and the original reading is https://sparselatents.com/matryoshka.html and https://arxiv.org/pdf/2503.17547</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.n_prefixes","title":"<code>n_prefixes = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of random length prefixes to use for loss calculation.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.sparsity_coeff","title":"<code>sparsity_coeff = 0.0004</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How much to weight sparsity loss term (if not using TopK/BatchTopK).</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss","title":"<code>MatryoshkaLoss(mse, sparsity, l0, l1)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Loss</code></p> <p>The composite loss terms for an training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l0","title":"<code>l0</code>  <code>instance-attribute</code>","text":"<p>Sum of L0 magnitudes of hidden activations for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l1","title":"<code>l1</code>  <code>instance-attribute</code>","text":"<p>Sum of L1 magnitudes of hidden activations for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.mse","title":"<code>mse</code>  <code>instance-attribute</code>","text":"<p>Average of reconstruction loss (mean squared error) for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.sparsity","title":"<code>sparsity</code>  <code>instance-attribute</code>","text":"<p>Sparsity loss, typically lambda * L1.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaObjective","title":"<code>MatryoshkaObjective(cfg)</code>","text":"<p>               Bases: <code>Objective</code></p> <p>Torch module for calculating the matryoshka loss for an SAE.</p> Source code in <code>src/saev/nn/objectives.py</code> <pre><code>def __init__(self, cfg: Matryoshka):\n    super().__init__()\n    self.cfg = cfg\n    # Keep sparsity_coeff as mutable attribute for scheduler compatibility\n    self.sparsity_coeff = cfg.sparsity_coeff\n</code></pre>"},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla","title":"<code>Vanilla(sparsity_coeff=0.0004)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla.sparsity_coeff","title":"<code>sparsity_coeff = 0.0004</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How much to weight sparsity loss term.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss","title":"<code>VanillaLoss(mse, sparsity, l0, l1)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Loss</code></p> <p>The vanilla loss terms for an training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l0","title":"<code>l0</code>  <code>instance-attribute</code>","text":"<p>L0 magnitude of hidden activations.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l1","title":"<code>l1</code>  <code>instance-attribute</code>","text":"<p>L1 magnitude of hidden activations.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.mse","title":"<code>mse</code>  <code>instance-attribute</code>","text":"<p>Reconstruction loss (mean squared error).</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.sparsity","title":"<code>sparsity</code>  <code>instance-attribute</code>","text":"<p>Sparsity loss, typically lambda * L1.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.sample_prefixes","title":"<code>sample_prefixes(d_sae, n_prefixes, min_prefix_length=1, pareto_power=0.5)</code>","text":"<p>Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547)</p> <p>Parameters:</p> Name Type Description Default <code>d_sae</code> <code>int</code> <p>Total number of latent dimensions</p> required <code>n_prefixes</code> <code>int</code> <p>Number of prefixes to sample</p> required <code>min_prefix_length</code> <code>int</code> <p>Minimum length of any prefix</p> <code>1</code> <code>pareto_power</code> <code>float</code> <p>Power parameter for Pareto distribution (lower = more uniform)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Int64[Tensor, ' n_prefixes']</code> <p>torch.Tensor: Sorted prefix lengths</p> Source code in <code>src/saev/nn/objectives.py</code> <pre><code>@torch.no_grad()\n@jaxtyped(typechecker=beartype.beartype)\ndef sample_prefixes(\n    d_sae: int, n_prefixes: int, min_prefix_length: int = 1, pareto_power: float = 0.5\n) -&gt; Int64[Tensor, \" n_prefixes\"]:\n    \"\"\"\n    Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with\n    Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547)\n\n    Args:\n        d_sae: Total number of latent dimensions\n        n_prefixes: Number of prefixes to sample\n        min_prefix_length: Minimum length of any prefix\n        pareto_power: Power parameter for Pareto distribution (lower = more uniform)\n\n    Returns:\n        torch.Tensor: Sorted prefix lengths\n    \"\"\"\n    if n_prefixes &lt;= 1:\n        return torch.tensor([d_sae], dtype=torch.int64)\n\n    assert n_prefixes &lt;= d_sae\n\n    # Calculate probability distribution favoring shorter prefixes\n    lengths = torch.arange(1, d_sae)\n    pareto_cdf = 1 - ((min_prefix_length / lengths.float()) ** pareto_power)\n    pareto_pdf = torch.cat([pareto_cdf[:1], pareto_cdf[1:] - pareto_cdf[:-1]])\n    probability_dist = pareto_pdf / pareto_pdf.sum()\n\n    # Sample and sort prefix lengths\n    sampled_indices = torch.multinomial(\n        probability_dist, num_samples=n_prefixes - 1, replacement=False\n    )\n\n    # Convert indices to actual prefix lengths\n    prefixes = lengths[sampled_indices]\n\n    # Add n_latents as the final prefix\n    prefixes = torch.cat((prefixes.detach().clone(), torch.tensor([d_sae])))\n\n    prefixes, _ = torch.sort(prefixes, descending=False)\n\n    return prefixes.to(torch.int64)\n</code></pre>"},{"location":"api/nn/saev.nn/","title":"saev.nn","text":""},{"location":"api/nn/saev.nn/#saev.nn","title":"<code>saev.nn</code>","text":""},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder","title":"<code>SparseAutoencoder(cfg)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse auto-encoder (SAE) using L1 sparsity penalty.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: SparseAutoencoderConfig):\n    super().__init__()\n\n    self.cfg = cfg\n    self.logger = logging.getLogger(f\"sae(seed={cfg.seed})\")\n\n    self.W_enc = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_vit, cfg.d_sae))\n    )\n    self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))\n\n    self.W_dec = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_vit))\n    )\n    self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_vit))\n\n    self.normalize_w_dec()\n\n    self.activation = get_activation(cfg.activation)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.decode","title":"<code>decode(f_x, *, prefixes=None)</code>","text":"<p>Decode latent features to reconstructions.</p> <p>Parameters:</p> Name Type Description Default <code>f_x</code> <code>Float[Tensor, 'batch d_sae']</code> <p>Latent features of shape (batch, d_sae)</p> required <code>prefixes</code> <code>Int64[Tensor, ' n_prefixes'] | None</code> <p>Optional tensor of prefix lengths for Matryoshka decoding.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_prefixes d_model']</code> <p>Matryoshka reconstructions (batch, n_prefixes, d_model).</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def decode(\n    self,\n    f_x: Float[Tensor, \"batch d_sae\"],\n    *,\n    prefixes: Int64[Tensor, \" n_prefixes\"] | None = None,\n) -&gt; Float[Tensor, \"batch n_prefixes d_model\"]:\n    \"\"\"\n    Decode latent features to reconstructions.\n\n    Args:\n        f_x: Latent features of shape (batch, d_sae)\n        prefixes: Optional tensor of prefix lengths for Matryoshka decoding.\n\n    Returns:\n        Matryoshka reconstructions (batch, n_prefixes, d_model).\n    \"\"\"\n    b, d_sae = f_x.shape\n\n    # Matryoshka cumulative decode\n    device = f_x.device\n    if prefixes is None:\n        prefixes = torch.tensor([d_sae], dtype=torch.int64)\n    assert torch.all(prefixes[1:] &gt; prefixes[:-1])\n    assert 1 &lt;= int(prefixes[0]) and int(prefixes[-1]) == d_sae\n    prefixes = prefixes.to(device)\n\n    # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ...\n    block_indices = torch.cat([\n        torch.tensor([0], dtype=prefixes.dtype, device=device),\n        prefixes,\n    ])\n    blocks = list(zip(block_indices[:-1], block_indices[1:]))\n\n    # Compute block outputs\n    block_outputs = []\n    for i, (start, end) in enumerate(blocks):\n        # Each block uses its portion of f_x and W_dec\n        block_f_x = f_x[:, start:end]\n        block_W_dec = self.W_dec[start:end, :]\n\n        # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -&gt; (batch, d_vit)\n        # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit)\n        block_output = einops.einsum(\n            block_f_x,\n            block_W_dec,\n            \"... d_sae_block, d_sae_block d_vit -&gt; ... d_vit\",\n        )\n\n        # Add bias only to the first block\n        if i == 0:\n            block_output = block_output + self.b_dec\n\n        block_outputs.append(block_output)\n\n    # Cumulative sum to get prefix reconstructions\n    x_hats = torch.cumsum(torch.stack(block_outputs, dim=-2), dim=-2)\n\n    return x_hats\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Given x, calculates the reconstructed x_hat and the intermediate activations f_x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model']</code> <p>a batch of ViT activations.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, \"batch d_model\"]\n) -&gt; tuple[Float[Tensor, \"batch d_model\"], Float[Tensor, \"batch d_sae\"]]:\n    \"\"\"\n    Given x, calculates the reconstructed x_hat and the intermediate activations f_x.\n\n    Arguments:\n        x: a batch of ViT activations.\n    \"\"\"\n    f_x = self.encode(x)\n    x_hat = self.decode(f_x)\n\n    return x_hat, f_x\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.normalize_w_dec","title":"<code>normalize_w_dec()</code>","text":"<p>Set W_dec to unit-norm columns.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef normalize_w_dec(self):\n    \"\"\"\n    Set W_dec to unit-norm columns.\n    \"\"\"\n    if self.cfg.normalize_w_dec:\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.remove_parallel_grads","title":"<code>remove_parallel_grads()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_vit) shape</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef remove_parallel_grads(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_vit) shape\n    \"\"\"\n    if not self.cfg.remove_parallel_grads:\n        return\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_vit, d_sae d_vit -&gt; d_sae\",\n    )\n\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_vit -&gt; d_sae d_vit\",\n    )\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig(d_vit=1024, exp_factor=16, n_reinit_samples=1024 * 16 * 32, remove_parallel_grads=True, normalize_w_dec=True, seed=0, activation=Relu())</code>  <code>dataclass</code>","text":""},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.exp_factor","title":"<code>exp_factor = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion factor for SAE.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.n_reinit_samples","title":"<code>n_reinit_samples = 1024 * 16 * 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.normalize_w_dec","title":"<code>normalize_w_dec = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.remove_parallel_grads","title":"<code>remove_parallel_grads = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.seed","title":"<code>seed = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/nn/saev.nn/#saev.nn.dump","title":"<code>dump(fpath, sae)</code>","text":"<p>Save an SAE checkpoint to disk along with configuration, using the trick from equinox.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str</code> <p>filepath to save checkpoint to.</p> required <code>sae</code> <code>SparseAutoencoder</code> <p>sparse autoencoder checkpoint to save.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef dump(fpath: str, sae: SparseAutoencoder):\n    \"\"\"\n    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).\n\n    Arguments:\n        fpath: filepath to save checkpoint to.\n        sae: sparse autoencoder checkpoint to save.\n    \"\"\"\n    # Custom serialization to handle activation object\n    cfg_dict = dataclasses.asdict(sae.cfg)\n    # Replace activation dict with custom format\n    activation = sae.cfg.activation\n    cfg_dict[\"activation\"] = {\n        \"cls\": activation.__class__.__name__,\n        \"params\": dataclasses.asdict(activation),\n    }\n\n    header = {\n        \"schema\": 2,\n        \"cfg\": cfg_dict,\n        \"commit\": helpers.current_git_commit() or \"unknown\",\n        \"lib\": __version__,\n    }\n\n    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n    with open(fpath, \"wb\") as fd:\n        header_str = json.dumps(header)\n        fd.write((header_str + \"\\n\").encode(\"utf-8\"))\n        torch.save(sae.state_dict(), fd)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.load","title":"<code>load(fpath, *, device='cpu')</code>","text":"<p>Loads a sparse autoencoder from disk.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef load(fpath: str, *, device=\"cpu\") -&gt; SparseAutoencoder:\n    \"\"\"\n    Loads a sparse autoencoder from disk.\n    \"\"\"\n    with open(fpath, \"rb\") as fd:\n        header = json.loads(fd.readline())\n        buffer = io.BytesIO(fd.read())\n\n    if \"schema\" not in header:\n        # Original, pre-schema format: just raw config parameters\n        # Remove old parameters that no longer exist\n        for keyword in (\"sparsity_coeff\", \"ghost_grads\", \"l1_coeff\", \"use_ghost_grads\"):\n            header.pop(keyword, None)\n        # Legacy format - create SparseAutoencoderConfig with Relu activation\n        cfg = SparseAutoencoderConfig(**header, activation=Relu())\n    elif header[\"schema\"] == 1:\n        # Schema version 1: A cautionary tale of poor version management\n        #\n        # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent!\n        #\n        # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.)\n        # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict\n        #\n        # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format.\n        #\n        # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number!\n\n        cls_name = header.get(\"cls\", \"SparseAutoencoderConfig\")\n        cfg_dict = header[\"cfg\"]\n\n        if cls_name in [\"Relu\", \"TopK\", \"BatchTopK\"]:\n            # Format 1A: Old format where cls indicates the activation type\n            activation_cls = globals()[cls_name]\n            if cls_name in [\"TopK\", \"BatchTopK\"]:\n                activation = activation_cls(top_k=cfg_dict.get(\"top_k\", 32))\n            else:\n                activation = activation_cls()\n            cfg = SparseAutoencoderConfig(**cfg_dict, activation=activation)\n        else:\n            # Format 1B: Newer format with activation as dict\n            if \"activation\" in cfg_dict:\n                activation_info = cfg_dict[\"activation\"]\n                activation_cls = globals()[activation_info[\"cls\"]]\n                activation = activation_cls(**activation_info[\"params\"])\n                cfg_dict[\"activation\"] = activation\n            cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 2:\n        # Schema version 2: cleaner format with activation serialization\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    else:\n        raise ValueError(f\"Unknown schema version: {header['schema']}\")\n\n    model = SparseAutoencoder(cfg)\n    model.load_state_dict(torch.load(buffer, weights_only=True, map_location=device))\n    return model\n</code></pre>"},{"location":"api/utils/saev.utils/","title":"saev.utils","text":""},{"location":"api/utils/saev.utils/#saev.utils","title":"<code>saev.utils</code>","text":""},{"location":"api/utils/scheduling/","title":"saev.utils.scheduling","text":""},{"location":"api/utils/scheduling/#saev.utils.scheduling","title":"<code>saev.utils.scheduling</code>","text":""},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter","title":"<code>BatchLimiter(dataloader, n_samples)</code>","text":"<p>Limits the number of batches to only return <code>n_samples</code> total samples.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(self, dataloader: DataLoaderLike, n_samples: int):\n    self.dataloader = dataloader\n    self.n_samples = n_samples\n    self.batch_size = dataloader.batch_size\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Pass through attribute access to the wrapped dataloader.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Pass through attribute access to the wrapped dataloader.\"\"\"\n    # __getattr__ is only called when the attribute wasn't found on self\n    # So we delegate to the wrapped dataloader\n    try:\n        return getattr(self.dataloader, name)\n    except AttributeError:\n        # Re-raise with more context about where the attribute was not found\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object and its wrapped dataloader have no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.Warmup","title":"<code>Warmup(init, final, n_steps)</code>","text":"<p>               Bases: <code>Scheduler</code></p> <p>Linearly increases from <code>init</code> to <code>final</code> over <code>n_warmup_steps</code> steps.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(self, init: float, final: float, n_steps: int):\n    self.final = final\n    self.init = init\n    self.n_steps = n_steps\n    self._step = 0\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.WarmupCosine","title":"<code>WarmupCosine(init, n_warmup, peak, n_steps, final)</code>","text":"<p>               Bases: <code>Scheduler</code></p> <p>Linearly increases from <code>init</code> to <code>peak</code> over <code>n_warmup</code> steps, then decrease down to final using cosine decay over n_steps - n_warmup.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(\n    self, init: float, n_warmup: int, peak: float, n_steps: int, final: float\n):\n    self.init = init\n    self.peak = peak\n    self.final = final\n    self.n_warmup = n_warmup\n    self.n_steps = n_steps\n    self._step = 0\n</code></pre>"},{"location":"api/utils/statistics/","title":"saev.utils.statistics","text":""},{"location":"api/utils/statistics/#saev.utils.statistics","title":"<code>saev.utils.statistics</code>","text":""},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator","title":"<code>PercentileEstimator(percentile, total, lr=0.001, shape=())</code>","text":"Source code in <code>src/saev/utils/statistics.py</code> <pre><code>def __init__(\n    self,\n    percentile: float | int,\n    total: int,\n    lr: float = 1e-3,\n    shape: tuple[int, ...] = (),\n):\n    self.percentile = percentile\n    self.total = total\n    self.lr = lr\n\n    self._estimate = torch.zeros(shape)\n    self._step = 0\n</code></pre>"},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator.update","title":"<code>update(x)</code>","text":"<p>Update the estimator with a new value.</p> <p>This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The new value to incorporate into the estimation</p> required Source code in <code>src/saev/utils/statistics.py</code> <pre><code>def update(self, x):\n    \"\"\"\n    Update the estimator with a new value.\n\n    This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.\n\n    Arguments:\n        x: The new value to incorporate into the estimation\n    \"\"\"\n    self._step += 1\n\n    step_size = self.lr * (self.total - self._step) / self.total\n\n    # Is a no-op if it's already on the same device.\n    if isinstance(x, Tensor):\n        self._estimate = self._estimate.to(x.device)\n\n    self._estimate += step_size * (\n        torch.sign(x - self._estimate) + 2 * self.percentile / 100 - 1.0\n    )\n</code></pre>"},{"location":"api/utils/wandb/","title":"saev.utils.wandb","text":""},{"location":"api/utils/wandb/#saev.utils.wandb","title":"<code>saev.utils.wandb</code>","text":""},{"location":"api/utils/wandb/#saev.utils.wandb.ParallelWandbRun","title":"<code>ParallelWandbRun(project, cfgs, mode, tags, dir='.wandb')</code>","text":"<p>Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387</p> Source code in <code>src/saev/utils/wandb.py</code> <pre><code>def __init__(\n    self,\n    project: str,\n    cfgs: list[dict[str, object]],\n    mode: str,\n    tags: list[str],\n    dir: str = \".wandb\",\n):\n    cfg, *cfgs = cfgs\n    self.project = project\n    self.cfgs = cfgs\n    self.mode = mode\n    self.tags = tags\n    self.dir = dir\n\n    self.live_run = wandb.init(\n        project=project, config=cfg, mode=mode, tags=tags, dir=dir\n    )\n\n    self.metric_queues: list[MetricQueue] = [[] for _ in self.cfgs]\n</code></pre>"},{"location":"developers/contributing/","title":"Contributing","text":""},{"location":"developers/contributing/#project-layout","title":"Project layout","text":"<pre><code>docs/\n    mkdocs.yml    # The configuration file.\n    src/\n        index.md  # The documentation homepage.\n        ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"users/guide/","title":"Guide","text":"<ol> <li>Record ViT activations and save them to disk.</li> <li>Train SAEs on the activations.</li> <li>Visualize the learned features from the trained SAEs.</li> <li>(your job) Propose trends and patterns in the visualized features.</li> <li>(your job, supported by code) Construct datasets to test your hypothesized trends.</li> <li>Confirm/reject hypotheses using <code>probing</code> package.</li> </ol> <p><code>saev</code> helps with steps 1, 2 and 3.</p> <pre><code>`saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X.\n</code></pre>"},{"location":"users/guide/#record-vit-activations-to-disk","title":"Record ViT Activations to Disk","text":"<p>To save activations to disk, we need to specify:</p> <ol> <li>Which model we would like to use</li> <li>Which layers we would like to save.</li> <li>Where on disk and how we would like to save activations.</li> <li>Which images we want to save activations for.</li> </ol> <p>The <code>saev.activations</code> module does all of this for us.</p> <p>Run <code>uv run python -m saev activations --help</code> to see all the configuration.</p> <p>In practice, you might run:</p> <pre><code>uv run python -m saev.data \\\n  --vit-family siglip \\\n  --vit-ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \\\n  --d-vit 1024 \\\n  --n-patches-per-img 256 \\\n  --no-cls-token \\\n  --vit-layers 13 15 17 19 21 23 \\\n  --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \\\n  --max-patches-per-shard 500_000 \\\n  --slurm-acct PAS2136 \\\n  --n-hours 48 \\\n  --slurm-partition nextgen \\\n  data:image-folder \\\n  --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/\n</code></pre> <p>Let's break down these arguments.</p> <p>This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49). It will save the second-to-last layer (<code>--layer -2</code>). It will write 2.4M patches per shard, and save shards to a new directory <code>/local/scratch/$USER/cache/saev</code>.</p> <p>.. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a total of 723GB for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.</p> <p>This script will also save a <code>metadata.json</code> file that will record the relevant metadata for these activations, which will be read by future steps. The activations will be in <code>.bin</code> files, numbered starting from 000000.</p> <p>To add your own models, see the guide to extending in <code>saev.activations</code>.</p>"},{"location":"users/guide/#train-saes-on-activations","title":"Train SAEs on Activations","text":"<p>To train an SAE, we need to specify:</p> <ol> <li>Which activations to use as input.</li> <li>SAE architectural stuff.</li> <li>Optimization-related stuff.</li> </ol> <p>The <code>saev.training</code> module handles this.</p> <p>Run <code>uv run python -m saev train --help</code> to see all the configuration.</p> <p>Continuing on from our example before, you might want to run something like:</p> <pre><code>uv run python -m saev train \\\n  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\\n  --data.layer -2 \\\n  --data.patches patches \\\n  --data.no-scale-mean \\\n  --data.no-scale-norm \\\n  --sae.d-vit 768 \\\n  --lr 5e-4\n</code></pre> <pre><code>uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-vit 1024\n</code></pre> <p><code>--data.*</code> flags describe which activations to use.</p> <p><code>--data.shard-root</code> should point to a directory with <code>*.bin</code> files and the <code>metadata.json</code> file. <code>--data.layer</code> specifies the layer, and <code>--data.patches</code> says that want to train on individual patch activations, rather than the [CLS] token activation. <code>--data.no-scale-mean</code> and <code>--data.no-scale-norm</code> mean not to scale the activation mean or L2 norm. Anthropic's and OpenAI's papers suggest normalizing these factors, but <code>saev</code> still has a bug with this, so I suggest not scaling these factors.</p> <p><code>--sae.*</code> flags are about the SAE itself.</p> <p><code>--sae.d-vit</code> is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.</p> <p>Finally, choose a slightly larger learning rate than the default with <code>--lr 5e-4</code>.</p> <p>This will train one (1) sparse autoencoder on the data. See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.</p>"},{"location":"users/guide/#visualize-the-learned-features","title":"Visualize the Learned Features","text":"<p>Now that you've trained an SAE, you probably want to look at its learned features. One way to visualize an individual learned feature (f) is by picking out images that maximize the activation of feature (f). Since we train SAEs on patch-level activations, we try to find the top patches for each feature (f). Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.</p> <p>.. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in <code>saev</code> unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to <code>contrib/</code> or another location.</p> <p><code>saev.visuals</code> records these maximally activating images for us. You can see all the options with <code>uv run python -m saev visuals --help</code>.</p> <p>The most important configuration options:</p> <ol> <li>The SAE checkpoint that you want to use (<code>--ckpt</code>).</li> <li>The ViT activations that you want to use (<code>--data.*</code> options, should be roughly the same as the options you used to train your SAE, like the same layer, same <code>--data.patches</code>).</li> <li>The images that produced the ViT activations that you want to use (<code>images</code> and <code>--images.*</code> options, should be the same as what you used to generate your ViT activtions).</li> <li>Some filtering options on which SAE latents to include (<code>--log-freq-range</code>, <code>--log-value-range</code>, <code>--include-latents</code>, <code>--n-latents</code>).</li> </ol> <p>Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on.</p> <p>.. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of (f)) are equally interesting. Some latents are dead, some are dense, some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents (<code>--include-latents</code>). I recommend using <code>saev.interactive.metrics</code> to figure out good thresholds.</p> <p>So you might run:</p> <pre><code>uv run python -m saev visuals \\\n  --ckpt checkpoints/abcdefg/sae.pt \\\n  --dump-to /nfs/$USER/saev/webapp/abcdefg \\\n  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\\n  --data.layer -2 \\\n  --data.patches patches \\\n  images:imagenet-dataset\n</code></pre> <p>This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE. It will cache these best activations to disk, then start saving images to visualize later on.</p> <p><code>saev.interactive.features</code> is a small web application based on marimo to interactively look at these images.</p> <p>You can run it with <code>uv run marimo edit saev/interactive/features.py</code>.</p>"},{"location":"users/guide/#sweeps","title":"Sweeps","text":"<p>tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU.</p>"},{"location":"users/guide/#why-parallel-sweeps","title":"Why Parallel Sweeps","text":"<p>SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation. When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself.</p> <p>A single set of ImageNet activations for a vision transformer can require terabytes of storage. Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient.</p>"},{"location":"users/guide/#parallelized-training-architecture","title":"Parallelized Training Architecture","text":"<p>To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch:</p> <pre>\nflowchart TD\n    A[Pre-computed ViT Activations] --&gt;|Slow I/O| B[Memory Buffer]\n    B --&gt;|Shared Batch| C[SAE Model 1]\n    B --&gt;|Shared Batch| D[SAE Model 2]\n    B --&gt;|Shared Batch| E[SAE Model 3]\n    B --&gt;|Shared Batch| F[...]\n</pre> <p>This approach:</p> <ul> <li>Loads each batch of activations once from disk</li> <li>Uses that same batch for multiple SAE models with different hyperparameters</li> <li>Amortizes the slow I/O cost across all models in the sweep</li> </ul>"},{"location":"users/guide/#running-a-sweep","title":"Running a Sweep","text":"<p>The <code>train</code> command accepts a <code>--sweep</code> parameter that points to a TOML file defining the hyperparameter grid:</p> <pre><code>uv run python -m saev train --sweep configs/my_sweep.toml\n</code></pre> <p>Here's an example sweep configuration file:</p> <pre><code>[sae]\nsparsity_coeff = [1e-4, 2e-4, 3e-4]\nd_vit = 768\nexp_factor = [8, 16]\n\n[data]\nscale_mean = true\n</code></pre> <p>This would train 6 models (3 sparsity coefficients \u00d7 2 expansion factors), each sharing the same data loading operation.</p>"},{"location":"users/guide/#limitations","title":"Limitations","text":"<p>Not all parameters can be swept in parallel. Parameters that affect data loading (like <code>batch_size</code> or dataset configuration) will cause the sweep to split into separate parallel groups. The system automatically handles this division to maximize efficiency.</p>"},{"location":"users/guide/#training-metrics-and-visualizations","title":"Training Metrics and Visualizations","text":"<p>When you train a sweep of SAEs, you probably want to understand which checkpoint is best. <code>saev</code> provides some tools to help with that.</p> <p>First, we offer a tool to look at some basic summary statistics of all your trained checkpoints.</p> <p><code>saev.interactive.metrics</code> is a marimo notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB.</p> <p>However, there are some pieces of code that need to be changed for you to use it.</p> <p>.. todo:: Explain how to use the <code>saev.interactive.metrics</code> notebook.</p> <ul> <li>Need to change your wandb username from samuelstevens to USERNAME from wandb</li> <li>Tag filter</li> <li>Need to run the notebook on the same machine as the original ViT shards and the shards need to be there.</li> <li>Think of better ways to do model and data keys</li> <li>Look at examples</li> <li>run visuals before features</li> </ul> <p>How to run visuals faster?</p> <p>explain how these features are visualized</p>"},{"location":"users/inference/","title":"Inference","text":"<p>Briefly, you need to:</p> <ol> <li>Download a checkpoint.</li> <li>Get the code.</li> <li>Load the checkpoint.</li> <li>Get activations.</li> </ol> <p>Details are below.</p>"},{"location":"users/inference/#download-a-checkpoint","title":"Download a Checkpoint","text":"<p>First, download an SAE checkpoint from the Huggingface collection.</p> <p>For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations here.</p> <p>You can use <code>wget</code> if you want:</p> <pre><code>wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt\n</code></pre>"},{"location":"users/inference/#get-the-code","title":"Get the Code","text":"<p>The easiest way to do this is to clone the code:</p> <pre><code>git clone https://github.com/OSU-NLP-Group/saev\n</code></pre> <p>You can also install the package from git if you use uv (not sure about pip or cuda):</p> <pre><code>uv add git+https://github.com/OSU-NLP-Group/saev\n</code></pre> <p>Or clone it and install it as an editable with pip, lik <code>pip install -e .</code> in your virtual environment.</p> <p>Then you can do things like <code>from saev import ...</code>.</p> <p>.. note::   If you struggle to get <code>saev</code> installed, open an issue on GitHub and I will figure out how to make it easier.</p>"},{"location":"users/inference/#load-the-checkpoint","title":"Load the Checkpoint","text":"<pre><code>import saev.nn\n\nsae = saev.nn.load(\"PATH_TO_YOUR_SAE_CKPT.pt\")\n</code></pre> <p>Now you have a pretrained SAE.</p>"},{"location":"users/inference/#get-activations","title":"Get Activations","text":"<p>This is the hardest part. We need to:</p> <ol> <li>Pass an image into a ViT</li> <li>Record the dense ViT activations at the same layer that the SAE was trained on.</li> <li>Pass the activations into the SAE to get sparse activations.</li> <li>Do something interesting with the sparse SAE activations.</li> </ol> <p>There are examples of this in the demo code: for classification and semantic segmentation. If the permalinks change, you are looking for the <code>get_sae_latents()</code> functions in both files.</p> <p>Below is example code to do it using the <code>saev</code> package.</p> <pre><code>import saev.nn\nimport saev.activations\n\nimg_transform = saev.activations.make_img_transform(\"clip\", \"ViT-B-16/openai\")\n\nvit = saev.activations.make_vit(\"clip\", \"ViT-B-16/openai\")\nrecorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10])\n\nimg = Image.open(\"example.jpg\")\n\nx = img_transform(img)\n# Add a batch dimension\nx = x[None, ...]\n_, vit_acts = recorded_vit(x)\n# Select the only layer in the batch and ignore the CLS token.\nvit_acts = vit_acts[:, 0, 1:, :]\n\nx_hat, f_x, loss = sae(vit_acts)\n</code></pre> <p>Now you have the reconstructed x (<code>x_hat</code>) and the sparse representation of all patches in the image (<code>f_x</code>).</p> <p>You might select the dimensions with maximal values for each patch and see what other images are maximimally activating.</p> <p>.. todo::   Provide documentation for how get maximally activating images.</p>"}]}