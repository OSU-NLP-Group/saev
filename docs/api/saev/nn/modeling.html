<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>saev.nn.modeling API documentation</title>
<meta name="description" content="Neural network architectures for sparse autoencoders.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>saev.nn.modeling</code></h1>
</header>
<section id="section-intro">
<p>Neural network architectures for sparse autoencoders.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="saev.nn.modeling.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>fpath: str,<br>sae: <a title="saev.nn.modeling.SparseAutoencoder" href="#saev.nn.modeling.SparseAutoencoder">SparseAutoencoder</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Save an SAE checkpoint to disk along with configuration, using the <a href="https://docs.kidger.site/equinox/examples/serialisation">trick from equinox</a>.</p>
<h2 id="arguments">Arguments</h2>
<p>fpath: filepath to save checkpoint to.
sae: sparse autoencoder checkpoint to save.</p></div>
</dd>
<dt id="saev.nn.modeling.get_activation"><code class="name flex">
<span>def <span class="ident">get_activation</span></span>(<span>cfg: <a title="saev.nn.modeling.Relu" href="#saev.nn.modeling.Relu">Relu</a> | <a title="saev.nn.modeling.TopK" href="#saev.nn.modeling.TopK">TopK</a> | <a title="saev.nn.modeling.BatchTopK" href="#saev.nn.modeling.BatchTopK">BatchTopK</a>) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.nn.modeling.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>fpath: str, *, device='cpu') ‑> <a title="saev.nn.modeling.SparseAutoencoder" href="#saev.nn.modeling.SparseAutoencoder">SparseAutoencoder</a></span>
</code></dt>
<dd>
<div class="desc"><p>Loads a sparse autoencoder from disk.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="saev.nn.modeling.BatchTopK"><code class="flex name class">
<span>class <span class="ident">BatchTopK</span></span>
<span>(</span><span>top_k: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>BatchTopK(top_k: int = 32)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class BatchTopK:
    top_k: int = 32
    &#34;&#34;&#34;How many values are allowed to be non-zero per sample in the batch.&#34;&#34;&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="saev.nn.modeling.BatchTopK.top_k"><code class="name">var <span class="ident">top_k</span> : int</code></dt>
<dd>
<div class="desc"><p>How many values are allowed to be non-zero per sample in the batch.</p></div>
</dd>
</dl>
</dd>
<dt id="saev.nn.modeling.BatchTopKActivation"><code class="flex name class">
<span>class <span class="ident">BatchTopKActivation</span></span>
<span>(</span><span>cfg: <a title="saev.nn.modeling.BatchTopK" href="#saev.nn.modeling.BatchTopK">BatchTopK</a> = BatchTopK(top_k=32))</span>
</code></dt>
<dd>
<div class="desc"><p>Batch Top-K activation function. For use as activation function of sparse encoder.
Applies top-k selection per sample in the batch.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class BatchTopKActivation(torch.nn.Module):
    &#34;&#34;&#34;
    Batch Top-K activation function. For use as activation function of sparse encoder.
    Applies top-k selection per sample in the batch.
    &#34;&#34;&#34;

    def __init__(self, cfg: BatchTopK = BatchTopK()):
        super().__init__()
        self.cfg = cfg
        self.k = cfg.top_k

    def forward(self, x: Float[Tensor, &#34;batch d_sae&#34;]) -&gt; Float[Tensor, &#34;batch d_sae&#34;]:
        &#34;&#34;&#34;
        Apply top-k activation to each sample in the batch.
        &#34;&#34;&#34;
        if self.k &lt;= 0:
            raise ValueError(&#34;k must be a positive integer.&#34;)

        # Handle case where k exceeds number of elements per sample
        k = min(self.k, x.shape[-1])

        # Apply top-k per sample (along the last dimension)
        k_vals, k_inds = torch.topk(x, k, dim=-1, sorted=False)
        mask = torch.zeros_like(x).scatter_(
            dim=-1, index=k_inds, src=torch.ones_like(x)
        )

        return torch.mul(mask, x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="saev.nn.modeling.BatchTopKActivation.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'batch d_sae']) ‑> jaxtyping.Float[Tensor, 'batch d_sae']</span>
</code></dt>
<dd>
<div class="desc"><p>Apply top-k activation to each sample in the batch.</p></div>
</dd>
</dl>
</dd>
<dt id="saev.nn.modeling.MatryoshkaSparseAutoencoder"><code class="flex name class">
<span>class <span class="ident">MatryoshkaSparseAutoencoder</span></span>
<span>(</span><span>cfg: <a title="saev.nn.modeling.SparseAutoencoderConfig" href="#saev.nn.modeling.SparseAutoencoderConfig">SparseAutoencoderConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Subclass of SparseAutoencoder for use with the Matryoshka objective function.
Needed since the matryoshka objective requires access to the weights of the decoder in order to calculate the
reconstructions from prefixes of the sparse encoding.</p>
<p>Still uses L1 for sparsity penalty, though when using BatchTopK as activation (recommended), this is not relevant.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class MatryoshkaSparseAutoencoder(SparseAutoencoder):
    &#34;&#34;&#34;
    Subclass of SparseAutoencoder for use with the Matryoshka objective function.
    Needed since the matryoshka objective requires access to the weights of the decoder in order to calculate the
    reconstructions from prefixes of the sparse encoding.

    Still uses L1 for sparsity penalty, though when using BatchTopK as activation (recommended), this is not relevant.
    &#34;&#34;&#34;

    def __init__(self, cfg: SparseAutoencoderConfig):
        super().__init__(cfg)

    def matryoshka_forward(
        self, x: Float[Tensor, &#34;batch d_model&#34;], n_prefixes: int
    ) -&gt; tuple[Float[Tensor, &#34;batch d_model&#34;], Float[Tensor, &#34;batch d_sae&#34;]]:
        &#34;&#34;&#34;
        Given x, calculates the reconstructed x_hat from the prefixes of encoded intermediate activations f_x.

        Arguments:
            x: a batch of ViT activations.
        &#34;&#34;&#34;

        # Remove encoder bias as per Anthropic
        h_pre = (
            einops.einsum(
                x - self.b_dec, self.W_enc, &#34;... d_vit, d_vit d_sae -&gt; ... d_sae&#34;
            )
            + self.b_enc
        )
        f_x = self.activation(h_pre)

        prefixes = self.sample_prefixes(len(f_x), n_prefixes).to(self.b_dec.device)

        block_indices = torch.torch.cat((
            torch.tensor([0]).to(self.b_dec.device),
            prefixes,
        ))
        block_bounds = list(zip(block_indices[:-1], block_indices[1:]))

        block_preds = [self.block_decode(f_x, block) for block in block_bounds]

        prefix_preds = torch.cumsum(torch.stack(block_preds), dim=0)

        return prefix_preds, f_x

    def block_decode(
        self, f_x: Float[Tensor, &#34;batch d_sae&#34;], block: tuple[int]
    ) -&gt; Float[Tensor, &#34;batch d_model&#34;]:
        &#34;&#34;&#34;Decodes sparse encoding using only the given interval of indices.

        Arguments:
            f_x: Sparse encoding&#34;&#34;&#34;

        # Can&#39;t use einsum here because the block lengths can change
        x_hat = (
            torch.matmul(f_x[:, block[0] : block[1]], self.W_dec[block[0] : block[1]])
            + self.b_dec
        )

        # x_hat = (
        #    einops.einsum(f_x[block[0]:block[1]], self.W_dec[block[0]:block[1]], &#34;... block, block d_vit -&gt; ... d_vit&#34;)
        #    + self.b_dec[block[0]:block[1]]
        # )

        return x_hat

    @torch.no_grad()
    def sample_prefixes(
        self,
        sae_dim: int,
        n_prefixes: int,
        min_prefix_length: int = 1,
        pareto_power: float = 0.5,
        replacement: bool = False,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Samples prefix lengths using a Pareto distribution. Derived from &#34;Learning Multi-Level Features with
        Matryoshka Sparse Autoencoders&#34; (https://doi.org/10.48550/arXiv.2503.17547)

        Args:
            sae_dim: Total number of latent dimensions
            n_prefixes: Number of prefixes to sample
            min_prefix_length: Minimum length of any prefix
            pareto_power: Power parameter for Pareto distribution (lower = more uniform)

        Returns:
            torch.Tensor: Sorted prefix lengths
        &#34;&#34;&#34;
        if n_prefixes &lt;= 1:
            return torch.tensor([sae_dim])

        # Calculate probability distribution favoring shorter prefixes
        lengths = torch.arange(1, sae_dim)
        pareto_cdf = 1 - ((min_prefix_length / lengths.float()) ** pareto_power)
        pareto_pdf = torch.cat([pareto_cdf[:1], pareto_cdf[1:] - pareto_cdf[:-1]])
        probability_dist = pareto_pdf / pareto_pdf.sum()

        # Sample and sort prefix lengths
        prefixes = torch.multinomial(
            probability_dist, num_samples=n_prefixes - 1, replacement=replacement
        )

        # Add n_latents as the final prefix
        prefixes = torch.cat((prefixes.detach().clone(), torch.tensor([sae_dim])))

        prefixes, _ = torch.sort(prefixes, descending=False)

        return prefixes</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="saev.nn.modeling.SparseAutoencoder" href="#saev.nn.modeling.SparseAutoencoder">SparseAutoencoder</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="saev.nn.modeling.MatryoshkaSparseAutoencoder.block_decode"><code class="name flex">
<span>def <span class="ident">block_decode</span></span>(<span>self, f_x: jaxtyping.Float[Tensor, 'batch d_sae'], block: tuple[int]) ‑> jaxtyping.Float[Tensor, 'batch d_model']</span>
</code></dt>
<dd>
<div class="desc"><p>Decodes sparse encoding using only the given interval of indices.</p>
<h2 id="arguments">Arguments</h2>
<p>f_x: Sparse encoding</p></div>
</dd>
<dt id="saev.nn.modeling.MatryoshkaSparseAutoencoder.matryoshka_forward"><code class="name flex">
<span>def <span class="ident">matryoshka_forward</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'batch d_model'], n_prefixes: int) ‑> tuple[jaxtyping.Float[Tensor, 'batch d_model'], jaxtyping.Float[Tensor, 'batch d_sae']]</span>
</code></dt>
<dd>
<div class="desc"><p>Given x, calculates the reconstructed x_hat from the prefixes of encoded intermediate activations f_x.</p>
<h2 id="arguments">Arguments</h2>
<p>x: a batch of ViT activations.</p></div>
</dd>
<dt id="saev.nn.modeling.MatryoshkaSparseAutoencoder.sample_prefixes"><code class="name flex">
<span>def <span class="ident">sample_prefixes</span></span>(<span>self,<br>sae_dim: int,<br>n_prefixes: int,<br>min_prefix_length: int = 1,<br>pareto_power: float = 0.5,<br>replacement: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples prefix lengths using a Pareto distribution. Derived from "Learning Multi-Level Features with
Matryoshka Sparse Autoencoders" (<a href="https://doi.org/10.48550/arXiv.2503.17547">https://doi.org/10.48550/arXiv.2503.17547</a>)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sae_dim</code></strong></dt>
<dd>Total number of latent dimensions</dd>
<dt><strong><code>n_prefixes</code></strong></dt>
<dd>Number of prefixes to sample</dd>
<dt><strong><code>min_prefix_length</code></strong></dt>
<dd>Minimum length of any prefix</dd>
<dt><strong><code>pareto_power</code></strong></dt>
<dd>Power parameter for Pareto distribution (lower = more uniform)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Sorted prefix lengths</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="saev.nn.modeling.SparseAutoencoder" href="#saev.nn.modeling.SparseAutoencoder">SparseAutoencoder</a></b></code>:
<ul class="hlist">
<li><code><a title="saev.nn.modeling.SparseAutoencoder.forward" href="#saev.nn.modeling.SparseAutoencoder.forward">forward</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.normalize_w_dec" href="#saev.nn.modeling.SparseAutoencoder.normalize_w_dec">normalize_w_dec</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.remove_parallel_grads" href="#saev.nn.modeling.SparseAutoencoder.remove_parallel_grads">remove_parallel_grads</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="saev.nn.modeling.Relu"><code class="flex name class">
<span>class <span class="ident">Relu</span></span>
</code></dt>
<dd>
<div class="desc"><p>Vanilla ReLU</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Relu:
    &#34;&#34;&#34;Vanilla ReLU&#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoder"><code class="flex name class">
<span>class <span class="ident">SparseAutoencoder</span></span>
<span>(</span><span>cfg: <a title="saev.nn.modeling.SparseAutoencoderConfig" href="#saev.nn.modeling.SparseAutoencoderConfig">SparseAutoencoderConfig</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Sparse auto-encoder (SAE) using L1 sparsity penalty.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class SparseAutoencoder(torch.nn.Module):
    &#34;&#34;&#34;
    Sparse auto-encoder (SAE) using L1 sparsity penalty.
    &#34;&#34;&#34;

    def __init__(self, cfg: SparseAutoencoderConfig):
        super().__init__()

        self.cfg = cfg
        self.logger = logging.getLogger(f&#34;sae(seed={cfg.seed})&#34;)

        self.W_enc = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_vit, cfg.d_sae))
        )
        self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))

        self.W_dec = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_vit))
        )
        self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_vit))

        self.normalize_w_dec()

        self.activation = get_activation(cfg.activation)

    def forward(
        self, x: Float[Tensor, &#34;batch d_model&#34;]
    ) -&gt; tuple[Float[Tensor, &#34;batch d_model&#34;], Float[Tensor, &#34;batch d_sae&#34;]]:
        &#34;&#34;&#34;
        Given x, calculates the reconstructed x_hat and the intermediate activations f_x.

        Arguments:
            x: a batch of ViT activations.
        &#34;&#34;&#34;
        f_x = self.encode(x)
        x_hat = self.decode(f_x)

        return x_hat, f_x

    def encode(self, x: Float[Tensor, &#34;batch d_model&#34;]) -&gt; Float[Tensor, &#34;batch d_sae&#34;]:
        h_pre = (
            einops.einsum(x, self.W_enc, &#34;... d_vit, d_vit d_sae -&gt; ... d_sae&#34;)
            + self.b_enc
        )
        f_x = self.activation(h_pre)
        return f_x

    def decode(
        self, f_x: Float[Tensor, &#34;batch d_sae&#34;]
    ) -&gt; Float[Tensor, &#34;batch d_model&#34;]:
        x_hat = (
            einops.einsum(f_x, self.W_dec, &#34;... d_sae, d_sae d_vit -&gt; ... d_vit&#34;)
            + self.b_dec
        )
        return x_hat

    @torch.no_grad()
    def normalize_w_dec(self):
        &#34;&#34;&#34;
        Set W_dec to unit-norm columns.
        &#34;&#34;&#34;
        if self.cfg.normalize_w_dec:
            self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)

    @torch.no_grad()
    def remove_parallel_grads(self):
        &#34;&#34;&#34;
        Update grads so that they remove the parallel component
            (d_sae, d_vit) shape
        &#34;&#34;&#34;
        if not self.cfg.remove_parallel_grads:
            return

        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            &#34;d_sae d_vit, d_sae d_vit -&gt; d_sae&#34;,
        )

        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            &#34;d_sae, d_sae d_vit -&gt; d_sae d_vit&#34;,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="saev.nn.modeling.MatryoshkaSparseAutoencoder" href="#saev.nn.modeling.MatryoshkaSparseAutoencoder">MatryoshkaSparseAutoencoder</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="saev.nn.modeling.SparseAutoencoder.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, f_x: jaxtyping.Float[Tensor, 'batch d_sae']) ‑> jaxtyping.Float[Tensor, 'batch d_model']</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoder.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'batch d_model']) ‑> jaxtyping.Float[Tensor, 'batch d_sae']</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'batch d_model']) ‑> tuple[jaxtyping.Float[Tensor, 'batch d_model'], jaxtyping.Float[Tensor, 'batch d_sae']]</span>
</code></dt>
<dd>
<div class="desc"><p>Given x, calculates the reconstructed x_hat and the intermediate activations f_x.</p>
<h2 id="arguments">Arguments</h2>
<p>x: a batch of ViT activations.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoder.normalize_w_dec"><code class="name flex">
<span>def <span class="ident">normalize_w_dec</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set W_dec to unit-norm columns.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoder.remove_parallel_grads"><code class="name flex">
<span>def <span class="ident">remove_parallel_grads</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update grads so that they remove the parallel component
(d_sae, d_vit) shape</p></div>
</dd>
</dl>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig"><code class="flex name class">
<span>class <span class="ident">SparseAutoencoderConfig</span></span>
<span>(</span><span>d_vit: int = 1024,<br>exp_factor: int = 16,<br>n_reinit_samples: int = 524288,<br>remove_parallel_grads: bool = True,<br>normalize_w_dec: bool = True,<br>seed: int = 0,<br>activation: <a title="saev.nn.modeling.Relu" href="#saev.nn.modeling.Relu">Relu</a> | <a title="saev.nn.modeling.TopK" href="#saev.nn.modeling.TopK">TopK</a> | <a title="saev.nn.modeling.BatchTopK" href="#saev.nn.modeling.BatchTopK">BatchTopK</a> = Relu())</span>
</code></dt>
<dd>
<div class="desc"><p>SparseAutoencoderConfig(d_vit: int = 1024, exp_factor: int = 16, n_reinit_samples: int = 524288, remove_parallel_grads: bool = True, normalize_w_dec: bool = True, seed: int = 0, activation: saev.nn.modeling.Relu | saev.nn.modeling.TopK | saev.nn.modeling.BatchTopK = Relu())</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class SparseAutoencoderConfig:
    d_vit: int = 1024
    exp_factor: int = 16
    &#34;&#34;&#34;Expansion factor for SAE.&#34;&#34;&#34;
    n_reinit_samples: int = 1024 * 16 * 32
    &#34;&#34;&#34;Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.&#34;&#34;&#34;
    remove_parallel_grads: bool = True
    &#34;&#34;&#34;Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.&#34;&#34;&#34;
    normalize_w_dec: bool = True
    &#34;&#34;&#34;Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.&#34;&#34;&#34;
    seed: int = 0
    &#34;&#34;&#34;Random seed.&#34;&#34;&#34;
    activation: ActivationConfig = Relu()

    @property
    def d_sae(self) -&gt; int:
        return self.d_vit * self.exp_factor</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.activation"><code class="name">var <span class="ident">activation</span> : <a title="saev.nn.modeling.Relu" href="#saev.nn.modeling.Relu">Relu</a> | <a title="saev.nn.modeling.TopK" href="#saev.nn.modeling.TopK">TopK</a> | <a title="saev.nn.modeling.BatchTopK" href="#saev.nn.modeling.BatchTopK">BatchTopK</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.d_vit"><code class="name">var <span class="ident">d_vit</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.exp_factor"><code class="name">var <span class="ident">exp_factor</span> : int</code></dt>
<dd>
<div class="desc"><p>Expansion factor for SAE.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples"><code class="name">var <span class="ident">n_reinit_samples</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias.">https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias.</a> We use the regular mean.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec"><code class="name">var <span class="ident">normalize_w_dec</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether to make sure W_dec has unit norm columns. See <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder">https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder</a> for original citation.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads"><code class="name">var <span class="ident">remove_parallel_grads</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization">https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization</a> for the original discussion from Anthropic.</p></div>
</dd>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.seed"><code class="name">var <span class="ident">seed</span> : int</code></dt>
<dd>
<div class="desc"><p>Random seed.</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="saev.nn.modeling.SparseAutoencoderConfig.d_sae"><code class="name">prop <span class="ident">d_sae</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def d_sae(self) -&gt; int:
    return self.d_vit * self.exp_factor</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.nn.modeling.TopK"><code class="flex name class">
<span>class <span class="ident">TopK</span></span>
<span>(</span><span>top_k: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>TopK(top_k: int = 32)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class TopK:
    top_k: int = 32
    &#34;&#34;&#34;How many values are allowed to be non-zero.&#34;&#34;&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="saev.nn.modeling.TopK.top_k"><code class="name">var <span class="ident">top_k</span> : int</code></dt>
<dd>
<div class="desc"><p>How many values are allowed to be non-zero.</p></div>
</dd>
</dl>
</dd>
<dt id="saev.nn.modeling.TopKActivation"><code class="flex name class">
<span>class <span class="ident">TopKActivation</span></span>
<span>(</span><span>cfg: <a title="saev.nn.modeling.TopK" href="#saev.nn.modeling.TopK">TopK</a> = TopK(top_k=32))</span>
</code></dt>
<dd>
<div class="desc"><p>Top-K activation function. For use as activation function of sparse encoder.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class TopKActivation(torch.nn.Module):
    &#34;&#34;&#34;
    Top-K activation function. For use as activation function of sparse encoder.
    &#34;&#34;&#34;

    def __init__(self, cfg: TopK = TopK()):
        super().__init__()
        self.cfg = cfg
        self.k = cfg.top_k

    def forward(self, x: Float[Tensor, &#34;batch d_sae&#34;]) -&gt; Float[Tensor, &#34;batch d_sae&#34;]:
        &#34;&#34;&#34;
        Apply top-k activation to the input tensor.
        &#34;&#34;&#34;
        if self.k &lt;= 0:
            raise ValueError(&#34;k must be a positive integer.&#34;)

        k_vals, k_inds = torch.topk(x, self.k, dim=-1, sorted=False)
        mask = torch.zeros_like(x).scatter_(
            dim=-1, index=k_inds, src=torch.ones_like(x)
        )

        return torch.mul(mask, x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="saev.nn.modeling.TopKActivation.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: jaxtyping.Float[Tensor, 'batch d_sae']) ‑> jaxtyping.Float[Tensor, 'batch d_sae']</span>
</code></dt>
<dd>
<div class="desc"><p>Apply top-k activation to the input tensor.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.css" integrity="sha512-b+T2i3P45i1LZM7I00Ci5QquB9szqaxu+uuk5TUSGjZQ4w4n+qujQiIuvTv2BxE7WCGQCifNMksyKILDiHzsOg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.js" integrity="sha512-2B9/byNV1KKRm5nQ2RLViPFD6U4dUjDGwuW1GU+ImJh8YinPU9Zlq1GzdTMO+G2ROrB5o1qasJBy1ttYz0wCug==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="saev.nn" href="index.html">saev.nn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="saev.nn.modeling.dump" href="#saev.nn.modeling.dump">dump</a></code></li>
<li><code><a title="saev.nn.modeling.get_activation" href="#saev.nn.modeling.get_activation">get_activation</a></code></li>
<li><code><a title="saev.nn.modeling.load" href="#saev.nn.modeling.load">load</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="saev.nn.modeling.BatchTopK" href="#saev.nn.modeling.BatchTopK">BatchTopK</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.BatchTopK.top_k" href="#saev.nn.modeling.BatchTopK.top_k">top_k</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.BatchTopKActivation" href="#saev.nn.modeling.BatchTopKActivation">BatchTopKActivation</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.BatchTopKActivation.forward" href="#saev.nn.modeling.BatchTopKActivation.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.MatryoshkaSparseAutoencoder" href="#saev.nn.modeling.MatryoshkaSparseAutoencoder">MatryoshkaSparseAutoencoder</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.MatryoshkaSparseAutoencoder.block_decode" href="#saev.nn.modeling.MatryoshkaSparseAutoencoder.block_decode">block_decode</a></code></li>
<li><code><a title="saev.nn.modeling.MatryoshkaSparseAutoencoder.matryoshka_forward" href="#saev.nn.modeling.MatryoshkaSparseAutoencoder.matryoshka_forward">matryoshka_forward</a></code></li>
<li><code><a title="saev.nn.modeling.MatryoshkaSparseAutoencoder.sample_prefixes" href="#saev.nn.modeling.MatryoshkaSparseAutoencoder.sample_prefixes">sample_prefixes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.Relu" href="#saev.nn.modeling.Relu">Relu</a></code></h4>
</li>
<li>
<h4><code><a title="saev.nn.modeling.SparseAutoencoder" href="#saev.nn.modeling.SparseAutoencoder">SparseAutoencoder</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.SparseAutoencoder.decode" href="#saev.nn.modeling.SparseAutoencoder.decode">decode</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.encode" href="#saev.nn.modeling.SparseAutoencoder.encode">encode</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.forward" href="#saev.nn.modeling.SparseAutoencoder.forward">forward</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.normalize_w_dec" href="#saev.nn.modeling.SparseAutoencoder.normalize_w_dec">normalize_w_dec</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoder.remove_parallel_grads" href="#saev.nn.modeling.SparseAutoencoder.remove_parallel_grads">remove_parallel_grads</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.SparseAutoencoderConfig" href="#saev.nn.modeling.SparseAutoencoderConfig">SparseAutoencoderConfig</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.activation" href="#saev.nn.modeling.SparseAutoencoderConfig.activation">activation</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.d_sae" href="#saev.nn.modeling.SparseAutoencoderConfig.d_sae">d_sae</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.d_vit" href="#saev.nn.modeling.SparseAutoencoderConfig.d_vit">d_vit</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.exp_factor" href="#saev.nn.modeling.SparseAutoencoderConfig.exp_factor">exp_factor</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples" href="#saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples">n_reinit_samples</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec" href="#saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec">normalize_w_dec</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads" href="#saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads">remove_parallel_grads</a></code></li>
<li><code><a title="saev.nn.modeling.SparseAutoencoderConfig.seed" href="#saev.nn.modeling.SparseAutoencoderConfig.seed">seed</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.TopK" href="#saev.nn.modeling.TopK">TopK</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.TopK.top_k" href="#saev.nn.modeling.TopK.top_k">top_k</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.nn.modeling.TopKActivation" href="#saev.nn.modeling.TopKActivation">TopKActivation</a></code></h4>
<ul class="">
<li><code><a title="saev.nn.modeling.TopKActivation.forward" href="#saev.nn.modeling.TopKActivation.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
