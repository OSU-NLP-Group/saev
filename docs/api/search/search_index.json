{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"saev","title":"Home"},{"location":"#saev","text":"","title":"saev"},{"location":"api/colors/","text":"saev.colors Utility color palettes used across saev visualizations.","title":"saev.colors"},{"location":"api/colors/#saev.colors","text":"Utility color palettes used across saev visualizations.","title":"colors"},{"location":"api/helpers/","text":"saev.helpers RemovedFeatureError Bases: RuntimeError Feature existed before but is no longer supported. batched_idx ( total_size , batch_size ) Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size. Parameters: total_size ( int ) \u2013 total number of examples batch_size ( int ) \u2013 maximum distance between the generated indices. Returns: \u2013 A generator of (int, int) tuples that can slice up a list or a tensor. Source code in src/saev/helpers.py 183 184 185 def __init__ ( self , total_size : int , batch_size : int ): self . total_size = total_size self . batch_size = batch_size __iter__ () Yield (start, end) index pairs for batching. Source code in src/saev/helpers.py 187 188 189 190 191 def __iter__ ( self ) -> collections . abc . Iterator [ tuple [ int , int ]]: \"\"\"Yield (start, end) index pairs for batching.\"\"\" for start in range ( 0 , self . total_size , self . batch_size ): stop = min ( start + self . batch_size , self . total_size ) yield start , stop __len__ () Return the number of batches. Source code in src/saev/helpers.py 193 194 195 def __len__ ( self ) -> int : \"\"\"Return the number of batches.\"\"\" return ( self . total_size + self . batch_size - 1 ) // self . batch_size progress ( it , * , every = 10 , desc = 'progress' , total = 0 ) Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish. Parameters: it ( Iterable ) \u2013 Iterable to wrap. every ( int , default: 10 ) \u2013 How many iterations between logging progress. desc ( str , default: 'progress' ) \u2013 What to name the logger. total ( int , default: 0 ) \u2013 If non-zero, how long the iterable is. Source code in src/saev/helpers.py 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , it : collections . abc . Iterable , * , every : int = 10 , desc : str = \"progress\" , total : int = 0 , ): self . it = it self . every = max ( every , 1 ) self . logger = logging . getLogger ( desc ) self . total = total current_git_commit () Best-effort short SHA of the repo containing this file. Returns None when * git executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out. Source code in src/saev/helpers.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 @beartype . beartype def current_git_commit () -> str | None : \"\"\" Best-effort short SHA of the repo containing *this* file. Returns `None` when * `git` executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out. \"\"\" try : # Walk up until we either hit a .git dir or the FS root here = pathlib . Path ( __file__ ) . resolve () for parent in ( here , * here . parents ): if ( parent / \".git\" ) . exists (): break else : # no .git found return None result = subprocess . run ( [ \"git\" , \"-C\" , str ( parent ), \"rev-parse\" , \"--short\" , \"HEAD\" ], stdout = subprocess . PIPE , stderr = subprocess . DEVNULL , text = True , check = True , ) return result . stdout . strip () or None except ( FileNotFoundError , subprocess . CalledProcessError ): return None dict_to_dataclass ( data , cls ) Recursively convert a dictionary to a dataclass instance. Source code in src/saev/helpers.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 @beartype . beartype def dict_to_dataclass ( data : dict , cls : type [ T ]) -> T : \"\"\"Recursively convert a dictionary to a dataclass instance.\"\"\" if not dataclasses . is_dataclass ( cls ): return data field_types = { f . name : f . type for f in dataclasses . fields ( cls )} kwargs = {} for field_name , field_type in field_types . items (): if field_name not in data : continue value = data [ field_name ] # Handle Optional types origin = tp . get_origin ( field_type ) args = tp . get_args ( field_type ) # Handle tuple[str, ...] if origin is tuple and args : kwargs [ field_name ] = tuple ( value ) if isinstance ( value , list ) else value # Handle list[DataclassType] elif origin is list and args and dataclasses . is_dataclass ( args [ 0 ]): kwargs [ field_name ] = [ dict_to_dataclass ( item , args [ 0 ]) for item in value ] # Handle regular dataclass fields elif dataclasses . is_dataclass ( field_type ): kwargs [ field_name ] = dict_to_dataclass ( value , field_type ) # Handle pathlib.Path elif field_type is pathlib . Path : # Required Path field - always convert kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value elif origin is tp . Union and pathlib . Path in args : # Optional Path field (typing.Union style) kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value elif origin is types . UnionType and pathlib . Path in args : # Optional Path field (Python 3.10+ union style with |) kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value else : kwargs [ field_name ] = value return cls ( ** kwargs ) expand ( config ) Expand a nested dict that may contain lists into many dicts. Source code in src/saev/helpers.py 206 207 208 209 210 @beartype . beartype def expand ( config : dict [ str , object ]) -> collections . abc . Iterator [ dict [ str , object ]]: \"\"\"Expand a nested dict that may contain lists into many dicts.\"\"\" yield from _expand_discrete ( dict ( config )) flattened ( dct , * , sep = '.' ) Flatten a potentially nested dict to a single-level dict with . -separated keys. Source code in src/saev/helpers.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @beartype . beartype def flattened ( dct : dict [ str , object ], * , sep : str = \".\" ) -> dict [ str , str | int | float | bool | None ]: \"\"\" Flatten a potentially nested dict to a single-level dict with `.`-separated keys. \"\"\" new = {} for key , value in dct . items (): if isinstance ( value , dict ): for nested_key , nested_value in flattened ( value ) . items (): new [ key + \".\" + nested_key ] = nested_value continue new [ key ] = value return new fssafe ( s ) Convert a string to be filesystem-safe by replacing special characters. This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'. Parameters: s ( str ) \u2013 String to make filesystem-safe. Returns: str \u2013 Filesystem-safe version of the string. Source code in src/saev/helpers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @beartype . beartype def fssafe ( s : str ) -> str : \"\"\" Convert a string to be filesystem-safe by replacing special characters. This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'. Args: s: String to make filesystem-safe. Returns: Filesystem-safe version of the string. \"\"\" # Replace common problematic characters with underscores replacements = { \"/\" : \"_\" , \" \\\\ \" : \"_\" , \":\" : \"_\" , \"*\" : \"_\" , \"?\" : \"_\" , '\"' : \"_\" , \"<\" : \"_\" , \">\" : \"_\" , \"|\" : \"_\" , \" \" : \"_\" , } for old , new in replacements . items (): s = s . replace ( old , new ) # Remove any remaining non-alphanumeric characters except - _ . return \"\" . join ( c if c . isalnum () or c in \"-_.\" else \"_\" for c in s ) get_cache_dir () Get cache directory from environment variables, defaulting to the current working directory (.) Returns: str \u2013 A path to a cache directory (might not exist yet). Source code in src/saev/helpers.py 24 25 26 27 28 29 30 31 32 33 34 35 @beartype . beartype def get_cache_dir () -> str : \"\"\" Get cache directory from environment variables, defaulting to the current working directory (.) Returns: A path to a cache directory (might not exist yet). \"\"\" cache_dir = \"\" for var in ( \"SAEV_CACHE\" , \"HF_HOME\" , \"HF_HUB_CACHE\" ): cache_dir = cache_dir or os . environ . get ( var , \"\" ) return cache_dir or \".\" get_non_default_values ( obj , default_obj ) Recursively find fields that differ from defaults. Source code in src/saev/helpers.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 @beartype . beartype def get_non_default_values ( obj : T , default_obj : T ) -> dict : \"\"\"Recursively find fields that differ from defaults.\"\"\" # Check that obj and default_obj are instances of a dataclass. assert dataclasses . is_dataclass ( obj ) and not isinstance ( obj , type ) assert dataclasses . is_dataclass ( default_obj ) and not isinstance ( default_obj , type ) obj_dict = dataclasses . asdict ( obj ) default_dict = dataclasses . asdict ( default_obj ) diff = {} for key , value in obj_dict . items (): default_value = default_dict . get ( key ) if value != default_value : diff [ key ] = value return diff get_slurm_job_count () Get the current number of jobs in the queue for the current user. Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs. Source code in src/saev/helpers.py 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 @beartype . beartype def get_slurm_job_count () -> int : \"\"\" Get the current number of jobs in the queue for the current user. Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs. \"\"\" try : # Use -r to display each array element on its own line result = subprocess . run ( [ \"squeue\" , \"--me\" , \"-h\" , \"-r\" ], capture_output = True , text = True , check = True ) # Count non-empty lines lines = result . stdout . strip () . split ( \" \\n \" ) return len ([ line for line in lines if line . strip ()]) except ( subprocess . SubprocessError , FileNotFoundError ): # If we can't check, assume no jobs return 0 get_slurm_max_array_size () Get the MaxArraySize configuration from the current Slurm cluster. Returns: int ( int ) \u2013 The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine. Source code in src/saev/helpers.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 @beartype . beartype def get_slurm_max_array_size () -> int : \"\"\" Get the MaxArraySize configuration from the current Slurm cluster. Returns: int: The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine. \"\"\" logger = logging . getLogger ( \"helpers.slurm\" ) try : # Run scontrol command to get config information result = subprocess . run ( [ \"scontrol\" , \"show\" , \"config\" ], capture_output = True , text = True , check = True ) # Search for MaxArraySize in the output match = re . search ( r \"MaxArraySize\\s*=\\s*(\\d+)\" , result . stdout ) if match : max_array_size = int ( match . group ( 1 )) logger . info ( \"Detected MaxArraySize = %d \" , max_array_size ) return max_array_size else : logger . warning ( \"Could not find MaxArraySize in scontrol output, using default of 1000\" ) return 1000 except subprocess . SubprocessError as e : logger . error ( \"Error running scontrol: %s \" , e ) return 1000 # Safe default except ValueError as e : logger . error ( \"Error parsing MaxArraySize: %s \" , e ) return 1000 # Safe default except FileNotFoundError : logger . warning ( \"scontrol command not found. Assuming not in Slurm environment. Returning default MaxArraySize=1000.\" ) return 1000 get_slurm_max_submit_jobs () Get the MaxSubmitJobs limit from the current user's QOS. Returns: int ( int ) \u2013 The maximum number of jobs that can be submitted at once. Returns 1000 as fallback. Source code in src/saev/helpers.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 @beartype . beartype def get_slurm_max_submit_jobs () -> int : \"\"\" Get the MaxSubmitJobs limit from the current user's QOS. Returns: int: The maximum number of jobs that can be submitted at once. Returns 1000 as fallback. \"\"\" logger = logging . getLogger ( \"helpers.slurm\" ) try : # First, try to get the QOS from a recent job result = subprocess . run ( [ \"scontrol\" , \"show\" , \"job\" , \"-o\" ], capture_output = True , text = True , check = False , ) qos_name = None if result . returncode == 0 and result . stdout : # Extract QOS from job info match = re . search ( r \"QOS=(\\S+)\" , result . stdout ) if match : qos_name = match . group ( 1 ) if not qos_name : # If no jobs, try to get default QOS from association # This is less reliable but better than nothing logger . warning ( \"No active jobs to determine QOS, using default of 1000\" ) return 1000 # Get the MaxSubmitJobs for this QOS result = subprocess . run ( [ \"sacctmgr\" , \"show\" , \"qos\" , qos_name , \"format=maxsubmitjobs\" , \"-n\" , \"-P\" ], capture_output = True , text = True , check = True , ) max_submit = result . stdout . strip () if max_submit and max_submit . isdigit (): limit = int ( max_submit ) logger . info ( \"Detected MaxSubmitJobs = %d for QOS %s \" , limit , qos_name ) return limit else : logger . warning ( \"Could not parse MaxSubmitJobs, using default of 1000\" ) return 1000 except subprocess . SubprocessError as e : logger . error ( \"Error getting MaxSubmitJobs: %s \" , e ) return 1000 except ( ValueError , FileNotFoundError ) as e : logger . error ( \"Error: %s \" , e ) return 1000 grid ( cfg , sweep_dct ) Generate configs from cfg according to sweep_dct . Source code in src/saev/helpers.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 @beartype . beartype def grid ( cfg : T , sweep_dct : dict [ str , object ]) -> tuple [ list [ T ], list [ str ]]: \"\"\"Generate configs from ``cfg`` according to ``sweep_dct``.\"\"\" cfgs : list [ T ] = [] errs : list [ str ] = [] for d , dct in enumerate ( expand ( sweep_dct )): updates = _recursive_dataclass_update ( cfg , dct , cfg , d ) if hasattr ( cfg , \"seed\" ) and \"seed\" not in updates : updates [ \"seed\" ] = getattr ( cfg , \"seed\" , 0 ) + d try : cfgs . append ( dataclasses . replace ( cfg , ** updates )) except Exception as err : errs . append ( str ( err )) return cfgs , errs merge_configs ( base , overrides ) Recursively merge override values into a base config. Source code in src/saev/helpers.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @beartype . beartype def merge_configs ( base : T , overrides : dict ) -> T : \"\"\"Recursively merge override values into a base config.\"\"\" if not overrides : return base # Check that base is an instance of a dataclass. assert dataclasses . is_dataclass ( base ) and not isinstance ( base , type ) base_dict = dataclasses . asdict ( base ) for key , value in overrides . items (): if key in base_dict : # For nested dataclasses, merge recursively if isinstance ( value , dict ) and dataclasses . is_dataclass ( getattr ( base , key )): base_dict [ key ] = dataclasses . asdict ( merge_configs ( getattr ( base , key ), value ) ) else : base_dict [ key ] = value return dict_to_dataclass ( base_dict , type ( base ))","title":"saev.helpers"},{"location":"api/helpers/#saev.helpers","text":"","title":"helpers"},{"location":"api/helpers/#saev.helpers.RemovedFeatureError","text":"Bases: RuntimeError Feature existed before but is no longer supported.","title":"RemovedFeatureError"},{"location":"api/helpers/#saev.helpers.batched_idx","text":"Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size. Parameters: total_size ( int ) \u2013 total number of examples batch_size ( int ) \u2013 maximum distance between the generated indices. Returns: \u2013 A generator of (int, int) tuples that can slice up a list or a tensor. Source code in src/saev/helpers.py 183 184 185 def __init__ ( self , total_size : int , batch_size : int ): self . total_size = total_size self . batch_size = batch_size","title":"batched_idx"},{"location":"api/helpers/#saev.helpers.batched_idx.__iter__","text":"Yield (start, end) index pairs for batching. Source code in src/saev/helpers.py 187 188 189 190 191 def __iter__ ( self ) -> collections . abc . Iterator [ tuple [ int , int ]]: \"\"\"Yield (start, end) index pairs for batching.\"\"\" for start in range ( 0 , self . total_size , self . batch_size ): stop = min ( start + self . batch_size , self . total_size ) yield start , stop","title":"__iter__"},{"location":"api/helpers/#saev.helpers.batched_idx.__len__","text":"Return the number of batches. Source code in src/saev/helpers.py 193 194 195 def __len__ ( self ) -> int : \"\"\"Return the number of batches.\"\"\" return ( self . total_size + self . batch_size - 1 ) // self . batch_size","title":"__len__"},{"location":"api/helpers/#saev.helpers.progress","text":"Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish. Parameters: it ( Iterable ) \u2013 Iterable to wrap. every ( int , default: 10 ) \u2013 How many iterations between logging progress. desc ( str , default: 'progress' ) \u2013 What to name the logger. total ( int , default: 0 ) \u2013 If non-zero, how long the iterable is. Source code in src/saev/helpers.py 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , it : collections . abc . Iterable , * , every : int = 10 , desc : str = \"progress\" , total : int = 0 , ): self . it = it self . every = max ( every , 1 ) self . logger = logging . getLogger ( desc ) self . total = total","title":"progress"},{"location":"api/helpers/#saev.helpers.current_git_commit","text":"Best-effort short SHA of the repo containing this file. Returns None when * git executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out. Source code in src/saev/helpers.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 @beartype . beartype def current_git_commit () -> str | None : \"\"\" Best-effort short SHA of the repo containing *this* file. Returns `None` when * `git` executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out. \"\"\" try : # Walk up until we either hit a .git dir or the FS root here = pathlib . Path ( __file__ ) . resolve () for parent in ( here , * here . parents ): if ( parent / \".git\" ) . exists (): break else : # no .git found return None result = subprocess . run ( [ \"git\" , \"-C\" , str ( parent ), \"rev-parse\" , \"--short\" , \"HEAD\" ], stdout = subprocess . PIPE , stderr = subprocess . DEVNULL , text = True , check = True , ) return result . stdout . strip () or None except ( FileNotFoundError , subprocess . CalledProcessError ): return None","title":"current_git_commit"},{"location":"api/helpers/#saev.helpers.dict_to_dataclass","text":"Recursively convert a dictionary to a dataclass instance. Source code in src/saev/helpers.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 @beartype . beartype def dict_to_dataclass ( data : dict , cls : type [ T ]) -> T : \"\"\"Recursively convert a dictionary to a dataclass instance.\"\"\" if not dataclasses . is_dataclass ( cls ): return data field_types = { f . name : f . type for f in dataclasses . fields ( cls )} kwargs = {} for field_name , field_type in field_types . items (): if field_name not in data : continue value = data [ field_name ] # Handle Optional types origin = tp . get_origin ( field_type ) args = tp . get_args ( field_type ) # Handle tuple[str, ...] if origin is tuple and args : kwargs [ field_name ] = tuple ( value ) if isinstance ( value , list ) else value # Handle list[DataclassType] elif origin is list and args and dataclasses . is_dataclass ( args [ 0 ]): kwargs [ field_name ] = [ dict_to_dataclass ( item , args [ 0 ]) for item in value ] # Handle regular dataclass fields elif dataclasses . is_dataclass ( field_type ): kwargs [ field_name ] = dict_to_dataclass ( value , field_type ) # Handle pathlib.Path elif field_type is pathlib . Path : # Required Path field - always convert kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value elif origin is tp . Union and pathlib . Path in args : # Optional Path field (typing.Union style) kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value elif origin is types . UnionType and pathlib . Path in args : # Optional Path field (Python 3.10+ union style with |) kwargs [ field_name ] = pathlib . Path ( value ) if value is not None else value else : kwargs [ field_name ] = value return cls ( ** kwargs )","title":"dict_to_dataclass"},{"location":"api/helpers/#saev.helpers.expand","text":"Expand a nested dict that may contain lists into many dicts. Source code in src/saev/helpers.py 206 207 208 209 210 @beartype . beartype def expand ( config : dict [ str , object ]) -> collections . abc . Iterator [ dict [ str , object ]]: \"\"\"Expand a nested dict that may contain lists into many dicts.\"\"\" yield from _expand_discrete ( dict ( config ))","title":"expand"},{"location":"api/helpers/#saev.helpers.flattened","text":"Flatten a potentially nested dict to a single-level dict with . -separated keys. Source code in src/saev/helpers.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @beartype . beartype def flattened ( dct : dict [ str , object ], * , sep : str = \".\" ) -> dict [ str , str | int | float | bool | None ]: \"\"\" Flatten a potentially nested dict to a single-level dict with `.`-separated keys. \"\"\" new = {} for key , value in dct . items (): if isinstance ( value , dict ): for nested_key , nested_value in flattened ( value ) . items (): new [ key + \".\" + nested_key ] = nested_value continue new [ key ] = value return new","title":"flattened"},{"location":"api/helpers/#saev.helpers.fssafe","text":"Convert a string to be filesystem-safe by replacing special characters. This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'. Parameters: s ( str ) \u2013 String to make filesystem-safe. Returns: str \u2013 Filesystem-safe version of the string. Source code in src/saev/helpers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @beartype . beartype def fssafe ( s : str ) -> str : \"\"\" Convert a string to be filesystem-safe by replacing special characters. This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'. Args: s: String to make filesystem-safe. Returns: Filesystem-safe version of the string. \"\"\" # Replace common problematic characters with underscores replacements = { \"/\" : \"_\" , \" \\\\ \" : \"_\" , \":\" : \"_\" , \"*\" : \"_\" , \"?\" : \"_\" , '\"' : \"_\" , \"<\" : \"_\" , \">\" : \"_\" , \"|\" : \"_\" , \" \" : \"_\" , } for old , new in replacements . items (): s = s . replace ( old , new ) # Remove any remaining non-alphanumeric characters except - _ . return \"\" . join ( c if c . isalnum () or c in \"-_.\" else \"_\" for c in s )","title":"fssafe"},{"location":"api/helpers/#saev.helpers.get_cache_dir","text":"Get cache directory from environment variables, defaulting to the current working directory (.) Returns: str \u2013 A path to a cache directory (might not exist yet). Source code in src/saev/helpers.py 24 25 26 27 28 29 30 31 32 33 34 35 @beartype . beartype def get_cache_dir () -> str : \"\"\" Get cache directory from environment variables, defaulting to the current working directory (.) Returns: A path to a cache directory (might not exist yet). \"\"\" cache_dir = \"\" for var in ( \"SAEV_CACHE\" , \"HF_HOME\" , \"HF_HUB_CACHE\" ): cache_dir = cache_dir or os . environ . get ( var , \"\" ) return cache_dir or \".\"","title":"get_cache_dir"},{"location":"api/helpers/#saev.helpers.get_non_default_values","text":"Recursively find fields that differ from defaults. Source code in src/saev/helpers.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 @beartype . beartype def get_non_default_values ( obj : T , default_obj : T ) -> dict : \"\"\"Recursively find fields that differ from defaults.\"\"\" # Check that obj and default_obj are instances of a dataclass. assert dataclasses . is_dataclass ( obj ) and not isinstance ( obj , type ) assert dataclasses . is_dataclass ( default_obj ) and not isinstance ( default_obj , type ) obj_dict = dataclasses . asdict ( obj ) default_dict = dataclasses . asdict ( default_obj ) diff = {} for key , value in obj_dict . items (): default_value = default_dict . get ( key ) if value != default_value : diff [ key ] = value return diff","title":"get_non_default_values"},{"location":"api/helpers/#saev.helpers.get_slurm_job_count","text":"Get the current number of jobs in the queue for the current user. Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs. Source code in src/saev/helpers.py 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 @beartype . beartype def get_slurm_job_count () -> int : \"\"\" Get the current number of jobs in the queue for the current user. Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs. \"\"\" try : # Use -r to display each array element on its own line result = subprocess . run ( [ \"squeue\" , \"--me\" , \"-h\" , \"-r\" ], capture_output = True , text = True , check = True ) # Count non-empty lines lines = result . stdout . strip () . split ( \" \\n \" ) return len ([ line for line in lines if line . strip ()]) except ( subprocess . SubprocessError , FileNotFoundError ): # If we can't check, assume no jobs return 0","title":"get_slurm_job_count"},{"location":"api/helpers/#saev.helpers.get_slurm_max_array_size","text":"Get the MaxArraySize configuration from the current Slurm cluster. Returns: int ( int ) \u2013 The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine. Source code in src/saev/helpers.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 @beartype . beartype def get_slurm_max_array_size () -> int : \"\"\" Get the MaxArraySize configuration from the current Slurm cluster. Returns: int: The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine. \"\"\" logger = logging . getLogger ( \"helpers.slurm\" ) try : # Run scontrol command to get config information result = subprocess . run ( [ \"scontrol\" , \"show\" , \"config\" ], capture_output = True , text = True , check = True ) # Search for MaxArraySize in the output match = re . search ( r \"MaxArraySize\\s*=\\s*(\\d+)\" , result . stdout ) if match : max_array_size = int ( match . group ( 1 )) logger . info ( \"Detected MaxArraySize = %d \" , max_array_size ) return max_array_size else : logger . warning ( \"Could not find MaxArraySize in scontrol output, using default of 1000\" ) return 1000 except subprocess . SubprocessError as e : logger . error ( \"Error running scontrol: %s \" , e ) return 1000 # Safe default except ValueError as e : logger . error ( \"Error parsing MaxArraySize: %s \" , e ) return 1000 # Safe default except FileNotFoundError : logger . warning ( \"scontrol command not found. Assuming not in Slurm environment. Returning default MaxArraySize=1000.\" ) return 1000","title":"get_slurm_max_array_size"},{"location":"api/helpers/#saev.helpers.get_slurm_max_submit_jobs","text":"Get the MaxSubmitJobs limit from the current user's QOS. Returns: int ( int ) \u2013 The maximum number of jobs that can be submitted at once. Returns 1000 as fallback. Source code in src/saev/helpers.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 @beartype . beartype def get_slurm_max_submit_jobs () -> int : \"\"\" Get the MaxSubmitJobs limit from the current user's QOS. Returns: int: The maximum number of jobs that can be submitted at once. Returns 1000 as fallback. \"\"\" logger = logging . getLogger ( \"helpers.slurm\" ) try : # First, try to get the QOS from a recent job result = subprocess . run ( [ \"scontrol\" , \"show\" , \"job\" , \"-o\" ], capture_output = True , text = True , check = False , ) qos_name = None if result . returncode == 0 and result . stdout : # Extract QOS from job info match = re . search ( r \"QOS=(\\S+)\" , result . stdout ) if match : qos_name = match . group ( 1 ) if not qos_name : # If no jobs, try to get default QOS from association # This is less reliable but better than nothing logger . warning ( \"No active jobs to determine QOS, using default of 1000\" ) return 1000 # Get the MaxSubmitJobs for this QOS result = subprocess . run ( [ \"sacctmgr\" , \"show\" , \"qos\" , qos_name , \"format=maxsubmitjobs\" , \"-n\" , \"-P\" ], capture_output = True , text = True , check = True , ) max_submit = result . stdout . strip () if max_submit and max_submit . isdigit (): limit = int ( max_submit ) logger . info ( \"Detected MaxSubmitJobs = %d for QOS %s \" , limit , qos_name ) return limit else : logger . warning ( \"Could not parse MaxSubmitJobs, using default of 1000\" ) return 1000 except subprocess . SubprocessError as e : logger . error ( \"Error getting MaxSubmitJobs: %s \" , e ) return 1000 except ( ValueError , FileNotFoundError ) as e : logger . error ( \"Error: %s \" , e ) return 1000","title":"get_slurm_max_submit_jobs"},{"location":"api/helpers/#saev.helpers.grid","text":"Generate configs from cfg according to sweep_dct . Source code in src/saev/helpers.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 @beartype . beartype def grid ( cfg : T , sweep_dct : dict [ str , object ]) -> tuple [ list [ T ], list [ str ]]: \"\"\"Generate configs from ``cfg`` according to ``sweep_dct``.\"\"\" cfgs : list [ T ] = [] errs : list [ str ] = [] for d , dct in enumerate ( expand ( sweep_dct )): updates = _recursive_dataclass_update ( cfg , dct , cfg , d ) if hasattr ( cfg , \"seed\" ) and \"seed\" not in updates : updates [ \"seed\" ] = getattr ( cfg , \"seed\" , 0 ) + d try : cfgs . append ( dataclasses . replace ( cfg , ** updates )) except Exception as err : errs . append ( str ( err )) return cfgs , errs","title":"grid"},{"location":"api/helpers/#saev.helpers.merge_configs","text":"Recursively merge override values into a base config. Source code in src/saev/helpers.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @beartype . beartype def merge_configs ( base : T , overrides : dict ) -> T : \"\"\"Recursively merge override values into a base config.\"\"\" if not overrides : return base # Check that base is an instance of a dataclass. assert dataclasses . is_dataclass ( base ) and not isinstance ( base , type ) base_dict = dataclasses . asdict ( base ) for key , value in overrides . items (): if key in base_dict : # For nested dataclasses, merge recursively if isinstance ( value , dict ) and dataclasses . is_dataclass ( getattr ( base , key )): base_dict [ key ] = dataclasses . asdict ( merge_configs ( getattr ( base , key ), value ) ) else : base_dict [ key ] = value return dict_to_dataclass ( base_dict , type ( base ))","title":"merge_configs"},{"location":"api/saev/","text":"saev saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.","title":"saev"},{"location":"api/saev/#saev","text":"saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.","title":"saev"},{"location":"api/summary/","text":"saev saev.colors saev.data saev.data. main saev.data.buffers saev.data.clip saev.data.datasets saev.data.dinov2 saev.data.dinov3 saev.data.fake_clip saev.data.indexed saev.data.models saev.data.ordered saev.data.shuffled saev.data.siglip saev.data.transforms saev.data.writers saev.helpers saev.nn saev.nn.modeling saev.nn.objectives saev.utils saev.utils.scheduling saev.utils.statistics saev.utils.wandb saev.viz","title":"Summary"},{"location":"api/viz/","text":"saev.viz","title":"saev.viz"},{"location":"api/viz/#saev.viz","text":"","title":"viz"},{"location":"api/data/__main__/","text":"saev.data.__main__ To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one. This module handles that additional complexity. Conceptually, activations are either thought of as A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a dataset Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations. main ( cfg ) Save ViT activations for use later on. Parameters: cfg ( Annotated [ Config , arg (name='')] ) \u2013 Configuration for activations. Source code in src/saev/data/__main__.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @beartype . beartype def main ( cfg : typing . Annotated [ writers . Config , tyro . conf . arg ( name = \"\" )]): \"\"\" Save ViT activations for use later on. Args: cfg: Configuration for activations. \"\"\" logger = logging . getLogger ( \"dump\" ) if not cfg . ssl : logger . warning ( \"Ignoring SSL certs. Try not to do this!\" ) # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761 # Ideally we don't have to disable SSL but we are only downloading weights. import ssl ssl . _create_default_https_context = ssl . _create_unverified_context # Actually record activations. if cfg . slurm_acct : import submitit executor = submitit . SlurmExecutor ( folder = cfg . log_to ) executor . update_parameters ( time = int ( cfg . n_hours * 60 ), partition = cfg . slurm_partition , gpus_per_node = 1 , ntasks_per_node = 1 , cpus_per_task = cfg . n_workers + 4 , stderr_to_stdout = True , account = cfg . slurm_acct , ) job = executor . submit ( writers . worker_fn , cfg ) logger . info ( \"Running job ' %s '.\" , job . job_id ) job . result () else : writers . worker_fn ( cfg )","title":"saev.data.main"},{"location":"api/data/__main__/#saev.data.__main__","text":"To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one. This module handles that additional complexity. Conceptually, activations are either thought of as A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a dataset Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations.","title":"__main__"},{"location":"api/data/__main__/#saev.data.__main__.main","text":"Save ViT activations for use later on. Parameters: cfg ( Annotated [ Config , arg (name='')] ) \u2013 Configuration for activations. Source code in src/saev/data/__main__.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @beartype . beartype def main ( cfg : typing . Annotated [ writers . Config , tyro . conf . arg ( name = \"\" )]): \"\"\" Save ViT activations for use later on. Args: cfg: Configuration for activations. \"\"\" logger = logging . getLogger ( \"dump\" ) if not cfg . ssl : logger . warning ( \"Ignoring SSL certs. Try not to do this!\" ) # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761 # Ideally we don't have to disable SSL but we are only downloading weights. import ssl ssl . _create_default_https_context = ssl . _create_unverified_context # Actually record activations. if cfg . slurm_acct : import submitit executor = submitit . SlurmExecutor ( folder = cfg . log_to ) executor . update_parameters ( time = int ( cfg . n_hours * 60 ), partition = cfg . slurm_partition , gpus_per_node = 1 , ntasks_per_node = 1 , cpus_per_task = cfg . n_workers + 4 , stderr_to_stdout = True , account = cfg . slurm_acct , ) job = executor . submit ( writers . worker_fn , cfg ) logger . info ( \"Running job ' %s '.\" , job . job_id ) job . result () else : writers . worker_fn ( cfg )","title":"main"},{"location":"api/data/buffers/","text":"saev.data.buffers ReservoirBuffer ( capacity , shape , * , dtype = torch . float32 , meta_shape = ( 2 ,), meta_dtype = torch . int32 , seed = 0 , collate_fn = None ) Pool of (tensor, meta) pairs. Multiple producers call put(batch_x, batch_meta). Multiple consumers call get(batch_size) -> (x, meta). Random order, each sample delivered once, blocking semantics. Source code in src/saev/data/buffers.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def __init__ ( self , capacity : int , shape : tuple [ int , ... ], * , dtype : torch . dtype = torch . float32 , meta_shape : tuple [ int , ... ] = ( 2 ,), meta_dtype : torch . dtype = torch . int32 , seed : int = 0 , collate_fn : collections . abc . Callable | None = None , ): self . capacity = capacity self . _empty = 123456789 self . data = torch . full (( capacity , * shape ), self . _empty , dtype = dtype ) self . data . share_memory_ () self . meta = torch . full (( capacity , * meta_shape ), self . _empty , dtype = meta_dtype ) self . meta . share_memory_ () self . ctx = mp . get_context () self . size = self . ctx . Value ( \"L\" , 0 ) # current live items self . lock = self . ctx . Lock () # guards size+swap self . free = self . ctx . Semaphore ( capacity ) self . full = self . ctx . Semaphore ( 0 ) # Each process has its own RNG. self . rng = np . random . default_rng ( seed ) self . collate_fn = collate_fn close () Release the shared-memory backing store (call once in the parent). Source code in src/saev/data/buffers.py 209 210 211 212 213 214 def close ( self ) -> None : \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\" try : self . data . untyped_storage () . _free_shared_mem () except ( AttributeError , FileNotFoundError ): pass # already freed or never allocated fill () Approximate proportion of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 220 221 222 def fill ( self ) -> float : \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\" return self . qsize () / self . capacity qsize () Approximate number of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 216 217 218 def qsize ( self ) -> int : \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\" return self . size . value RingBuffer ( slots , shape , dtype ) Fixed-capacity, multiple-producer / multiple-consumer queue backed by a shared-memory tensor. Parameters slots : int capacity in number of items (tensor rows) shape : tuple[int] shape of one item, e.g. (batch, dim) dtype : torch.dtype tensor dtype put(tensor) : blocks if full get() -> tensor : blocks if empty qsize() -> int advisory size (approximate) close() frees shared storage (call in the main process) Source code in src/saev/data/buffers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , slots : int , shape : tuple [ int , ... ], dtype : torch . dtype ): assert slots > 0 , \"slots must be positive\" self . slots = slots # 123456789 -> Should make you very worried. self . buf = torch . full (( slots , * shape ), 123456789 , dtype = dtype ) self . buf . share_memory_ () ctx = mp . get_context () # obeys the global start method (\"spawn\") # shared, lock-free counters self . head = ctx . Value ( \"L\" , 0 , lock = False ) # next free slot self . tail = ctx . Value ( \"L\" , 0 , lock = False ) # next occupied slot # semaphores for blocking semantics self . free = ctx . Semaphore ( slots ) # initially all slots free self . fill = ctx . Semaphore ( 0 ) # no filled slots yet # one mutex for pointer updates self . mutex = ctx . Lock () close () Release the shared-memory backing store (call once in the parent). Source code in src/saev/data/buffers.py 80 81 82 83 84 85 def close ( self ) -> None : \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\" try : self . buf . untyped_storage () . _free_shared_mem () except ( AttributeError , FileNotFoundError ): pass # already freed or never allocated fill () Approximate proportion of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 76 77 78 def fill ( self ) -> float : \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\" return self . qsize () / self . capacity get () Return a view of the next item; blocks if the queue is empty. Source code in src/saev/data/buffers.py 62 63 64 65 66 67 68 69 70 def get ( self ) -> torch . Tensor : \"\"\"Return a view of the next item; blocks if the queue is empty.\"\"\" self . fill . acquire () # wait for data with self . mutex : # exclusive update of tail idx = self . tail . value % self . slots out = self . buf [ idx ] . clone () self . tail . value += 1 self . free . release () # signal one more free slot return out put ( tensor ) Copy tensor into the next free slot; blocks if the queue is full. Source code in src/saev/data/buffers.py 50 51 52 53 54 55 56 57 58 59 60 def put ( self , tensor : torch . Tensor ) -> None : \"\"\"Copy `tensor` into the next free slot; blocks if the queue is full.\"\"\" if tensor . shape != self . buf . shape [ 1 :] or tensor . dtype != self . buf . dtype : raise ValueError ( \"tensor shape / dtype mismatch\" ) self . free . acquire () # wait for a free slot with self . mutex : # exclusive update of head idx = self . head . value % self . slots self . buf [ idx ] . copy_ ( tensor ) self . head . value += 1 self . fill . release () # signal there is data qsize () Approximate number of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 72 73 74 def qsize ( self ) -> int : \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\" return ( self . head . value - self . tail . value ) % ( 1 << 64 )","title":"saev.data.buffers"},{"location":"api/data/buffers/#saev.data.buffers","text":"","title":"buffers"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer","text":"Pool of (tensor, meta) pairs. Multiple producers call put(batch_x, batch_meta). Multiple consumers call get(batch_size) -> (x, meta). Random order, each sample delivered once, blocking semantics. Source code in src/saev/data/buffers.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def __init__ ( self , capacity : int , shape : tuple [ int , ... ], * , dtype : torch . dtype = torch . float32 , meta_shape : tuple [ int , ... ] = ( 2 ,), meta_dtype : torch . dtype = torch . int32 , seed : int = 0 , collate_fn : collections . abc . Callable | None = None , ): self . capacity = capacity self . _empty = 123456789 self . data = torch . full (( capacity , * shape ), self . _empty , dtype = dtype ) self . data . share_memory_ () self . meta = torch . full (( capacity , * meta_shape ), self . _empty , dtype = meta_dtype ) self . meta . share_memory_ () self . ctx = mp . get_context () self . size = self . ctx . Value ( \"L\" , 0 ) # current live items self . lock = self . ctx . Lock () # guards size+swap self . free = self . ctx . Semaphore ( capacity ) self . full = self . ctx . Semaphore ( 0 ) # Each process has its own RNG. self . rng = np . random . default_rng ( seed ) self . collate_fn = collate_fn","title":"ReservoirBuffer"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.close","text":"Release the shared-memory backing store (call once in the parent). Source code in src/saev/data/buffers.py 209 210 211 212 213 214 def close ( self ) -> None : \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\" try : self . data . untyped_storage () . _free_shared_mem () except ( AttributeError , FileNotFoundError ): pass # already freed or never allocated","title":"close"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.fill","text":"Approximate proportion of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 220 221 222 def fill ( self ) -> float : \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\" return self . qsize () / self . capacity","title":"fill"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.qsize","text":"Approximate number of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 216 217 218 def qsize ( self ) -> int : \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\" return self . size . value","title":"qsize"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer","text":"Fixed-capacity, multiple-producer / multiple-consumer queue backed by a shared-memory tensor.","title":"RingBuffer"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer--parameters","text":"slots : int capacity in number of items (tensor rows) shape : tuple[int] shape of one item, e.g. (batch, dim) dtype : torch.dtype tensor dtype put(tensor) : blocks if full get() -> tensor : blocks if empty qsize() -> int advisory size (approximate) close() frees shared storage (call in the main process) Source code in src/saev/data/buffers.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , slots : int , shape : tuple [ int , ... ], dtype : torch . dtype ): assert slots > 0 , \"slots must be positive\" self . slots = slots # 123456789 -> Should make you very worried. self . buf = torch . full (( slots , * shape ), 123456789 , dtype = dtype ) self . buf . share_memory_ () ctx = mp . get_context () # obeys the global start method (\"spawn\") # shared, lock-free counters self . head = ctx . Value ( \"L\" , 0 , lock = False ) # next free slot self . tail = ctx . Value ( \"L\" , 0 , lock = False ) # next occupied slot # semaphores for blocking semantics self . free = ctx . Semaphore ( slots ) # initially all slots free self . fill = ctx . Semaphore ( 0 ) # no filled slots yet # one mutex for pointer updates self . mutex = ctx . Lock ()","title":"Parameters"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.close","text":"Release the shared-memory backing store (call once in the parent). Source code in src/saev/data/buffers.py 80 81 82 83 84 85 def close ( self ) -> None : \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\" try : self . buf . untyped_storage () . _free_shared_mem () except ( AttributeError , FileNotFoundError ): pass # already freed or never allocated","title":"close"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.fill","text":"Approximate proportion of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 76 77 78 def fill ( self ) -> float : \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\" return self . qsize () / self . capacity","title":"fill"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.get","text":"Return a view of the next item; blocks if the queue is empty. Source code in src/saev/data/buffers.py 62 63 64 65 66 67 68 69 70 def get ( self ) -> torch . Tensor : \"\"\"Return a view of the next item; blocks if the queue is empty.\"\"\" self . fill . acquire () # wait for data with self . mutex : # exclusive update of tail idx = self . tail . value % self . slots out = self . buf [ idx ] . clone () self . tail . value += 1 self . free . release () # signal one more free slot return out","title":"get"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.put","text":"Copy tensor into the next free slot; blocks if the queue is full. Source code in src/saev/data/buffers.py 50 51 52 53 54 55 56 57 58 59 60 def put ( self , tensor : torch . Tensor ) -> None : \"\"\"Copy `tensor` into the next free slot; blocks if the queue is full.\"\"\" if tensor . shape != self . buf . shape [ 1 :] or tensor . dtype != self . buf . dtype : raise ValueError ( \"tensor shape / dtype mismatch\" ) self . free . acquire () # wait for a free slot with self . mutex : # exclusive update of head idx = self . head . value % self . slots self . buf [ idx ] . copy_ ( tensor ) self . head . value += 1 self . fill . release () # signal there is data","title":"put"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.qsize","text":"Approximate number of filled slots (race-safe enough for tests). Source code in src/saev/data/buffers.py 72 73 74 def qsize ( self ) -> int : \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\" return ( self . head . value - self . tail . value ) % ( 1 << 64 )","title":"qsize"},{"location":"api/data/clip/","text":"saev.data.clip Vit ( ckpt ) Bases: VisionTransformer , Module Source code in src/saev/data/clip.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , ckpt : str ): super () . __init__ () if ckpt . startswith ( \"hf-hub:\" ): clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) _ , ckpt = ckpt . split ( \"hf-hub:\" ) else : arch , ckpt = ckpt . split ( \"/\" ) clip , _ = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model . eval () assert not isinstance ( self . model , open_clip . timm_model . TimmModel ) patch_size property Get patch size for CLIP models. make_transforms ( ckpt , n_patches_per_img ) staticmethod Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/clip.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" if ckpt . startswith ( \"hf-hub:\" ): _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) _ , img_transform = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"saev.data.clip"},{"location":"api/data/clip/#saev.data.clip","text":"","title":"clip"},{"location":"api/data/clip/#saev.data.clip.Vit","text":"Bases: VisionTransformer , Module Source code in src/saev/data/clip.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , ckpt : str ): super () . __init__ () if ckpt . startswith ( \"hf-hub:\" ): clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) _ , ckpt = ckpt . split ( \"hf-hub:\" ) else : arch , ckpt = ckpt . split ( \"/\" ) clip , _ = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model . eval () assert not isinstance ( self . model , open_clip . timm_model . TimmModel )","title":"Vit"},{"location":"api/data/clip/#saev.data.clip.Vit.patch_size","text":"Get patch size for CLIP models.","title":"patch_size"},{"location":"api/data/clip/#saev.data.clip.Vit.make_transforms","text":"Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/clip.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" if ckpt . startswith ( \"hf-hub:\" ): _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) _ , img_transform = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"make_transforms"},{"location":"api/data/datasets/","text":"saev.data.datasets FakeSeg ( n_imgs = 10 , n_patches_per_img = 16 , n_classes = 3 , bg_label = 0 ) dataclass Tiny synthetic segmentation dataset for tests. Generates dummy RGB images and pixel-level segmentation masks, mimicking the behavior of real segmentation datasets like SegFolder. bg_label = 0 class-attribute instance-attribute Which class index is considered background. n_classes = 3 class-attribute instance-attribute Number of segmentation classes. n_imgs = 10 class-attribute instance-attribute Number of images. n_patches_per_img = 16 class-attribute instance-attribute Number of patches per image. FakeSegDataset ( cfg , * , img_transform = None , seg_transform = None , sample_transform = None ) Bases: Dataset Synthetic segmentation dataset providing pixel-level segmentation masks. Mimics SegFolderDataset by providing image: a dummy RGB PIL image segmentation: a PIL image with pixel-level class labels index, target, label Source code in src/saev/data/datasets.py 395 396 397 398 399 400 401 402 403 404 405 406 def __init__ ( self , cfg : FakeSeg , * , img_transform = None , seg_transform = None , sample_transform = None , ): self . cfg = cfg self . img_transform = img_transform self . seg_transform = seg_transform self . sample_transform = sample_transform ImageFolder ( root = os . path . join ( '.' , 'data' , 'split' )) dataclass Configuration for a generic image folder dataset. n_imgs property Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable. root = os . path . join ( '.' , 'data' , 'split' ) class-attribute instance-attribute Where the class folders with images are stored. Can be a glob pattern to match multiple directories. ImageFolderDataset ( * args , sample_transform = None , ** kwargs ) Bases: ImageFolder Source code in src/saev/data/datasets.py 209 210 211 def __init__ ( self , * args , sample_transform : Callable | None = None , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . sample_transform = sample_transform __getitem__ ( index ) Parameters: index ( int ) \u2013 Index Returns: dict [ str , object ] \u2013 dict with keys 'image', 'index', 'target' and 'label'. Source code in src/saev/data/datasets.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def __getitem__ ( self , index : int ) -> dict [ str , object ]: \"\"\" Args: index: Index Returns: dict with keys 'image', 'index', 'target' and 'label'. \"\"\" path , target = self . samples [ index ] image = self . loader ( path ) if self . transform is not None : image = self . transform ( image ) if self . target_transform is not None : target = self . target_transform ( target ) sample = { \"image\" : image , \"target\" : target , \"label\" : self . classes [ target ], \"index\" : index , } if self . sample_transform is not None : sample = self . sample_transform ( sample ) return sample Imagenet ( name = 'ILSVRC/imagenet-1k' , split = 'train' ) dataclass Configuration for HuggingFace Imagenet. n_imgs property Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable. name = 'ILSVRC/imagenet-1k' class-attribute instance-attribute Dataset name on HuggingFace. Don't need to change this.. split = 'train' class-attribute instance-attribute Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'. SegFolder ( root = os . path . join ( '.' , 'data' , 'segdataset' ), split = 'training' , img_label_fname = 'sceneCategories.txt' , bg_label = 0 ) dataclass bg_label = 0 class-attribute instance-attribute Background label. img_label_fname = 'sceneCategories.txt' class-attribute instance-attribute Image labels filename. n_imgs property Number of images in the dataset. Calculated on the fly by counting image files in root/images/split. root = os . path . join ( '.' , 'data' , 'segdataset' ) class-attribute instance-attribute Where the class folders with images are stored. split = 'training' class-attribute instance-attribute Data split. get_dataset ( cfg , * , img_transform , seg_transform = None , sample_transform = None ) Gets the dataset for the current experiment; delegates construction to dataset-specific functions. Parameters: cfg ( Config ) \u2013 Experiment config. img_transform \u2013 Image transform to be applied to each image. seg_transform \u2013 Segmentation transform to be applied to masks (for segmentation datasets). sample_transform \u2013 Transform to be applied to each sample dict. Returns: A dataset that has dictionaries with 'image' , 'index' , 'target' , and 'label' keys containing examples. Source code in src/saev/data/datasets.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @beartype . beartype def get_dataset ( cfg : Config , * , img_transform , seg_transform = None , sample_transform = None ): \"\"\" Gets the dataset for the current experiment; delegates construction to dataset-specific functions. Args: cfg: Experiment config. img_transform: Image transform to be applied to each image. seg_transform: Segmentation transform to be applied to masks (for segmentation datasets). sample_transform: Transform to be applied to each sample dict. Returns: A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples. \"\"\" # TODO: Can we reduce duplication? Or is it nice to see that there is no magic here? if isinstance ( cfg , Imagenet ): return ImagenetDataset ( cfg , img_transform = img_transform , sample_transform = sample_transform ) elif isinstance ( cfg , SegFolder ): return SegFolderDataset ( cfg , img_transform = img_transform , seg_transform = seg_transform , sample_transform = sample_transform , ) elif isinstance ( cfg , ImageFolder ): ds = [ ImageFolderDataset ( root , transform = img_transform , sample_transform = sample_transform ) for root in glob . glob ( cfg . root , recursive = True ) ] if len ( ds ) == 1 : return ds [ 0 ] else : return torch . utils . data . ConcatDataset ( ds ) elif isinstance ( cfg , Fake ): return FakeDataset ( cfg , img_transform = img_transform , sample_transform = sample_transform ) elif isinstance ( cfg , FakeSeg ): return FakeSegDataset ( cfg , img_transform = img_transform , seg_transform = seg_transform , sample_transform = sample_transform , ) else : typing . assert_never ( cfg )","title":"saev.data.datasets"},{"location":"api/data/datasets/#saev.data.datasets","text":"","title":"datasets"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg","text":"Tiny synthetic segmentation dataset for tests. Generates dummy RGB images and pixel-level segmentation masks, mimicking the behavior of real segmentation datasets like SegFolder.","title":"FakeSeg"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.bg_label","text":"Which class index is considered background.","title":"bg_label"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_classes","text":"Number of segmentation classes.","title":"n_classes"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_imgs","text":"Number of images.","title":"n_imgs"},{"location":"api/data/datasets/#saev.data.datasets.FakeSeg.n_patches_per_img","text":"Number of patches per image.","title":"n_patches_per_img"},{"location":"api/data/datasets/#saev.data.datasets.FakeSegDataset","text":"Bases: Dataset Synthetic segmentation dataset providing pixel-level segmentation masks. Mimics SegFolderDataset by providing image: a dummy RGB PIL image segmentation: a PIL image with pixel-level class labels index, target, label Source code in src/saev/data/datasets.py 395 396 397 398 399 400 401 402 403 404 405 406 def __init__ ( self , cfg : FakeSeg , * , img_transform = None , seg_transform = None , sample_transform = None , ): self . cfg = cfg self . img_transform = img_transform self . seg_transform = seg_transform self . sample_transform = sample_transform","title":"FakeSegDataset"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder","text":"Configuration for a generic image folder dataset.","title":"ImageFolder"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder.n_imgs","text":"Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable.","title":"n_imgs"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolder.root","text":"Where the class folders with images are stored. Can be a glob pattern to match multiple directories.","title":"root"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolderDataset","text":"Bases: ImageFolder Source code in src/saev/data/datasets.py 209 210 211 def __init__ ( self , * args , sample_transform : Callable | None = None , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . sample_transform = sample_transform","title":"ImageFolderDataset"},{"location":"api/data/datasets/#saev.data.datasets.ImageFolderDataset.__getitem__","text":"Parameters: index ( int ) \u2013 Index Returns: dict [ str , object ] \u2013 dict with keys 'image', 'index', 'target' and 'label'. Source code in src/saev/data/datasets.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def __getitem__ ( self , index : int ) -> dict [ str , object ]: \"\"\" Args: index: Index Returns: dict with keys 'image', 'index', 'target' and 'label'. \"\"\" path , target = self . samples [ index ] image = self . loader ( path ) if self . transform is not None : image = self . transform ( image ) if self . target_transform is not None : target = self . target_transform ( target ) sample = { \"image\" : image , \"target\" : target , \"label\" : self . classes [ target ], \"index\" : index , } if self . sample_transform is not None : sample = self . sample_transform ( sample ) return sample","title":"__getitem__"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet","text":"Configuration for HuggingFace Imagenet.","title":"Imagenet"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.n_imgs","text":"Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable.","title":"n_imgs"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.name","text":"Dataset name on HuggingFace. Don't need to change this..","title":"name"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.split","text":"Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'.","title":"split"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder","text":"","title":"SegFolder"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.bg_label","text":"Background label.","title":"bg_label"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.img_label_fname","text":"Image labels filename.","title":"img_label_fname"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.n_imgs","text":"Number of images in the dataset. Calculated on the fly by counting image files in root/images/split.","title":"n_imgs"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.root","text":"Where the class folders with images are stored.","title":"root"},{"location":"api/data/datasets/#saev.data.datasets.SegFolder.split","text":"Data split.","title":"split"},{"location":"api/data/datasets/#saev.data.datasets.get_dataset","text":"Gets the dataset for the current experiment; delegates construction to dataset-specific functions. Parameters: cfg ( Config ) \u2013 Experiment config. img_transform \u2013 Image transform to be applied to each image. seg_transform \u2013 Segmentation transform to be applied to masks (for segmentation datasets). sample_transform \u2013 Transform to be applied to each sample dict. Returns: A dataset that has dictionaries with 'image' , 'index' , 'target' , and 'label' keys containing examples. Source code in src/saev/data/datasets.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @beartype . beartype def get_dataset ( cfg : Config , * , img_transform , seg_transform = None , sample_transform = None ): \"\"\" Gets the dataset for the current experiment; delegates construction to dataset-specific functions. Args: cfg: Experiment config. img_transform: Image transform to be applied to each image. seg_transform: Segmentation transform to be applied to masks (for segmentation datasets). sample_transform: Transform to be applied to each sample dict. Returns: A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples. \"\"\" # TODO: Can we reduce duplication? Or is it nice to see that there is no magic here? if isinstance ( cfg , Imagenet ): return ImagenetDataset ( cfg , img_transform = img_transform , sample_transform = sample_transform ) elif isinstance ( cfg , SegFolder ): return SegFolderDataset ( cfg , img_transform = img_transform , seg_transform = seg_transform , sample_transform = sample_transform , ) elif isinstance ( cfg , ImageFolder ): ds = [ ImageFolderDataset ( root , transform = img_transform , sample_transform = sample_transform ) for root in glob . glob ( cfg . root , recursive = True ) ] if len ( ds ) == 1 : return ds [ 0 ] else : return torch . utils . data . ConcatDataset ( ds ) elif isinstance ( cfg , Fake ): return FakeDataset ( cfg , img_transform = img_transform , sample_transform = sample_transform ) elif isinstance ( cfg , FakeSeg ): return FakeSegDataset ( cfg , img_transform = img_transform , seg_transform = seg_transform , sample_transform = sample_transform , ) else : typing . assert_never ( cfg )","title":"get_dataset"},{"location":"api/data/dinov2/","text":"saev.data.dinov2","title":"saev.data.dinov2"},{"location":"api/data/dinov2/#saev.data.dinov2","text":"","title":"dinov2"},{"location":"api/data/dinov3/","text":"saev.data.dinov3 Config ( img_size = 224 , patch_size = 16 , in_chans = 3 , pos_embed_rope_base = 100.0 , pos_embed_rope_min_period = None , pos_embed_rope_max_period = None , pos_embed_rope_normalize_coords = 'separate' , pos_embed_rope_dtype = 'bf16' , embed_dim = 768 , depth = 12 , num_heads = 12 , ffn_ratio = 4.0 , qkv_bias = True , ffn_layer = 'mlp' , ffn_bias = True , proj_bias = True , n_storage_tokens = 0 , mask_k_bias = False , untie_global_and_local_cls_norm = False , device = None ) dataclass depth = 12 class-attribute instance-attribute Number of transformer blocks. device = None class-attribute instance-attribute Device for tensor operations. embed_dim = 768 class-attribute instance-attribute Embedding dimension for transformer. ffn_bias = True class-attribute instance-attribute Whether to use bias in feed-forward network. ffn_layer = 'mlp' class-attribute instance-attribute Type of feed-forward network layer. ffn_ratio = 4.0 class-attribute instance-attribute Feed-forward network expansion ratio. img_size = 224 class-attribute instance-attribute Image width and height in pixels. in_chans = 3 class-attribute instance-attribute Number of input image channels. mask_k_bias = False class-attribute instance-attribute Whether to mask K bias in attention. n_storage_tokens = 0 class-attribute instance-attribute Number of storage/register tokens. num_heads = 12 class-attribute instance-attribute Number of attention heads. patch_size = 16 class-attribute instance-attribute Size of each patch in pixels. pos_embed_rope_base = 100.0 class-attribute instance-attribute Base frequency for RoPE positional encoding. pos_embed_rope_dtype = 'bf16' class-attribute instance-attribute Data type for RoPE positional encoding. pos_embed_rope_max_period = None class-attribute instance-attribute Maximum period for RoPE positional encoding. pos_embed_rope_min_period = None class-attribute instance-attribute Minimum period for RoPE positional encoding. pos_embed_rope_normalize_coords = 'separate' class-attribute instance-attribute Coordinate normalization method for RoPE encoding. proj_bias = True class-attribute instance-attribute Whether to use bias in output projection. qkv_bias = True class-attribute instance-attribute Whether to use bias in QKV projection. untie_global_and_local_cls_norm = False class-attribute instance-attribute Whether to use separate norms for global and local CLS tokens. PatchEmbed ( img_size = 224 , patch_size = 16 , in_chans = 3 , embed_dim = 768 , flatten_embedding = True ) Bases: Module 2D image to patch embedding: (B,C,H,W) -> (B,N,D) Parameters: img_size ( int | tuple [ int , int ] , default: 224 ) \u2013 Image size. patch_size ( int | tuple [ int , int ] , default: 16 ) \u2013 Patch token size. in_chans ( int , default: 3 ) \u2013 Number of input image channels. embed_dim ( int , default: 768 ) \u2013 Number of linear projection output channels. Source code in src/saev/data/dinov3.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , img_size : int | tuple [ int , int ] = 224 , patch_size : int | tuple [ int , int ] = 16 , in_chans : int = 3 , embed_dim : int = 768 , flatten_embedding : bool = True , ) -> None : super () . __init__ () image_hw = make_2tuple ( img_size ) patch_hw = make_2tuple ( patch_size ) self . image_hw = image_hw self . patch_hw = patch_hw self . in_chans = in_chans self . embed_dim = embed_dim self . proj = nn . Conv2d ( in_chans , embed_dim , kernel_size = patch_hw , stride = patch_hw ) self . k = patch_hw [ 0 ] assert self . proj . kernel_size == ( self . k , self . k ) assert self . proj . stride == ( self . k , self . k ) assert self . proj . padding == ( 0 , 0 ) assert self . proj . groups == 1 assert self . proj . dilation == ( 1 , 1 ) Vit ( ckpt ) Bases: Module , VisionTransformer Source code in src/saev/data/dinov3.py 616 617 618 619 620 621 622 def __init__ ( self , ckpt : str ): super () . __init__ () name = self . _parse_name ( ckpt ) self . model = load ( name , ckpt ) self . _ckpt = name self . logger = logging . getLogger ( f \"dinov3/ { name } \" ) make_resize ( ckpt , n_patches_per_img , * , scale = 1.0 , resample = Image . LANCZOS ) staticmethod Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/dinov3.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\" import functools return functools . partial ( transforms . resize_to_patch_grid , p = int ( 16 * scale ), n = n_patches_per_img , resample = resample , ) make_transforms ( ckpt , n_patches_per_img ) staticmethod Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/dinov3.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" img_transform = v2 . Compose ([ transforms . FlexResize ( patch_size = 16 , n_patches = n_patches_per_img ), v2 . ToImage (), v2 . ToDtype ( torch . float32 , scale = True ), v2 . Normalize ( mean = [ 0.4850 , 0.4560 , 0.4060 ], std = [ 0.2290 , 0.2240 , 0.2250 ]), ]) sample_transform = transforms . Patchify ( patch_size = 16 , n_patches = n_patches_per_img ) return img_transform , sample_transform","title":"saev.data.dinov3"},{"location":"api/data/dinov3/#saev.data.dinov3","text":"","title":"dinov3"},{"location":"api/data/dinov3/#saev.data.dinov3.Config","text":"","title":"Config"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.depth","text":"Number of transformer blocks.","title":"depth"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.device","text":"Device for tensor operations.","title":"device"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.embed_dim","text":"Embedding dimension for transformer.","title":"embed_dim"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_bias","text":"Whether to use bias in feed-forward network.","title":"ffn_bias"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_layer","text":"Type of feed-forward network layer.","title":"ffn_layer"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_ratio","text":"Feed-forward network expansion ratio.","title":"ffn_ratio"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.img_size","text":"Image width and height in pixels.","title":"img_size"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.in_chans","text":"Number of input image channels.","title":"in_chans"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.mask_k_bias","text":"Whether to mask K bias in attention.","title":"mask_k_bias"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.n_storage_tokens","text":"Number of storage/register tokens.","title":"n_storage_tokens"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.num_heads","text":"Number of attention heads.","title":"num_heads"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.patch_size","text":"Size of each patch in pixels.","title":"patch_size"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_base","text":"Base frequency for RoPE positional encoding.","title":"pos_embed_rope_base"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_dtype","text":"Data type for RoPE positional encoding.","title":"pos_embed_rope_dtype"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_max_period","text":"Maximum period for RoPE positional encoding.","title":"pos_embed_rope_max_period"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_min_period","text":"Minimum period for RoPE positional encoding.","title":"pos_embed_rope_min_period"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_normalize_coords","text":"Coordinate normalization method for RoPE encoding.","title":"pos_embed_rope_normalize_coords"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.proj_bias","text":"Whether to use bias in output projection.","title":"proj_bias"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.qkv_bias","text":"Whether to use bias in QKV projection.","title":"qkv_bias"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.untie_global_and_local_cls_norm","text":"Whether to use separate norms for global and local CLS tokens.","title":"untie_global_and_local_cls_norm"},{"location":"api/data/dinov3/#saev.data.dinov3.PatchEmbed","text":"Bases: Module 2D image to patch embedding: (B,C,H,W) -> (B,N,D) Parameters: img_size ( int | tuple [ int , int ] , default: 224 ) \u2013 Image size. patch_size ( int | tuple [ int , int ] , default: 16 ) \u2013 Patch token size. in_chans ( int , default: 3 ) \u2013 Number of input image channels. embed_dim ( int , default: 768 ) \u2013 Number of linear projection output channels. Source code in src/saev/data/dinov3.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , img_size : int | tuple [ int , int ] = 224 , patch_size : int | tuple [ int , int ] = 16 , in_chans : int = 3 , embed_dim : int = 768 , flatten_embedding : bool = True , ) -> None : super () . __init__ () image_hw = make_2tuple ( img_size ) patch_hw = make_2tuple ( patch_size ) self . image_hw = image_hw self . patch_hw = patch_hw self . in_chans = in_chans self . embed_dim = embed_dim self . proj = nn . Conv2d ( in_chans , embed_dim , kernel_size = patch_hw , stride = patch_hw ) self . k = patch_hw [ 0 ] assert self . proj . kernel_size == ( self . k , self . k ) assert self . proj . stride == ( self . k , self . k ) assert self . proj . padding == ( 0 , 0 ) assert self . proj . groups == 1 assert self . proj . dilation == ( 1 , 1 )","title":"PatchEmbed"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit","text":"Bases: Module , VisionTransformer Source code in src/saev/data/dinov3.py 616 617 618 619 620 621 622 def __init__ ( self , ckpt : str ): super () . __init__ () name = self . _parse_name ( ckpt ) self . model = load ( name , ckpt ) self . _ckpt = name self . logger = logging . getLogger ( f \"dinov3/ { name } \" )","title":"Vit"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_resize","text":"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/dinov3.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\" import functools return functools . partial ( transforms . resize_to_patch_grid , p = int ( 16 * scale ), n = n_patches_per_img , resample = resample , )","title":"make_resize"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_transforms","text":"Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/dinov3.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" img_transform = v2 . Compose ([ transforms . FlexResize ( patch_size = 16 , n_patches = n_patches_per_img ), v2 . ToImage (), v2 . ToDtype ( torch . float32 , scale = True ), v2 . Normalize ( mean = [ 0.4850 , 0.4560 , 0.4060 ], std = [ 0.2290 , 0.2240 , 0.2250 ]), ]) sample_transform = transforms . Patchify ( patch_size = 16 , n_patches = n_patches_per_img ) return img_transform , sample_transform","title":"make_transforms"},{"location":"api/data/fake_clip/","text":"saev.data.fake_clip Fake CLIP model for testing with tiny-open-clip-model. This module provides a test-only vision transformer that works with the tiny-open-clip-model from HuggingFace, which uses 8x8 images and 2x2 patches instead of the standard 224x224 images with 16x16 patches. Vit ( ckpt ) Bases: VisionTransformer , Module Source code in src/saev/data/fake_clip.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , ckpt : str ): super () . __init__ () # Only support the tiny test model assert ckpt == \"hf-hub:hf-internal-testing/tiny-open-clip-model\" , ( f \"FakeClip only supports tiny-open-clip-model, got { ckpt } \" ) clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model . eval () patch_size property Tiny model uses 2x2 patches. make_resize ( ckpt , n_patches_per_img =- 1 , * , scale = 1.0 , resample = Image . LANCZOS ) staticmethod Create resize transform for tiny model (8x8 images). Source code in src/saev/data/fake_clip.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int = - 1 , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for tiny model (8x8 images).\"\"\" def resize ( img : Image . Image ) -> Image . Image : # Tiny model uses 8x8 images size_px = ( int ( 8 * scale ), int ( 8 * scale )) return img . resize ( size_px , resample = resample ) return resize make_transforms ( ckpt , n_patches_per_img ) staticmethod Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/fake_clip.py 54 55 56 57 58 59 60 61 62 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"saev.data.fake_clip"},{"location":"api/data/fake_clip/#saev.data.fake_clip","text":"Fake CLIP model for testing with tiny-open-clip-model. This module provides a test-only vision transformer that works with the tiny-open-clip-model from HuggingFace, which uses 8x8 images and 2x2 patches instead of the standard 224x224 images with 16x16 patches.","title":"fake_clip"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit","text":"Bases: VisionTransformer , Module Source code in src/saev/data/fake_clip.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , ckpt : str ): super () . __init__ () # Only support the tiny test model assert ckpt == \"hf-hub:hf-internal-testing/tiny-open-clip-model\" , ( f \"FakeClip only supports tiny-open-clip-model, got { ckpt } \" ) clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model . eval ()","title":"Vit"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.patch_size","text":"Tiny model uses 2x2 patches.","title":"patch_size"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_resize","text":"Create resize transform for tiny model (8x8 images). Source code in src/saev/data/fake_clip.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int = - 1 , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for tiny model (8x8 images).\"\"\" def resize ( img : Image . Image ) -> Image . Image : # Tiny model uses 8x8 images size_px = ( int ( 8 * scale ), int ( 8 * scale )) return img . resize ( size_px , resample = resample ) return resize","title":"make_resize"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_transforms","text":"Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/fake_clip.py 54 55 56 57 58 59 60 61 62 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"make_transforms"},{"location":"api/data/indexed/","text":"saev.data.indexed Config ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , seed = 17 , debug = False ) dataclass Configuration for loading indexed activation data from disk Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. layer ( int | Literal ['all'] ) \u2013 Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. seed ( int ) \u2013 Random seed. debug ( bool ) \u2013 Whether the dataloader process should log debug messages. Dataset ( cfg ) Bases: Dataset Dataset of activations from disk. Attributes: cfg ( Config ) \u2013 Configuration set via CLI args. metadata ( Metadata ) \u2013 Activations metadata; automatically loaded from disk. layer_index ( int ) \u2013 Layer index into the shards if we are choosing a specific layer. Source code in src/saev/data/indexed.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) # Check if labels.bin exists labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) self . labels_mmap = None if os . path . exists ( labels_path ): self . labels_mmap = np . memmap ( labels_path , mode = \"r\" , dtype = np . uint8 , shape = ( self . metadata . n_imgs , self . metadata . n_patches_per_img ), ) # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError. self . layer_index = 1_000_000 if isinstance ( self . cfg . layer , int ): err_msg = f \"Non-exact matches for .layer field not supported; { self . cfg . layer } not in { self . metadata . layers } .\" assert self . cfg . layer in self . metadata . layers , err_msg self . layer_index = self . metadata . layers . index ( self . cfg . layer ) d_vit property Dimension of the underlying vision transformer's embedding space. Example Bases: TypedDict Individual example. __len__ () Dataset length depends on patches and layer . Source code in src/saev/data/indexed.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __len__ ( self ) -> int : \"\"\" Dataset length depends on `patches` and `layer`. \"\"\" match ( self . cfg . patches , self . cfg . layer ): case ( \"cls\" , \"all\" ): # Return a CLS token from a random image and random layer. return self . metadata . n_imgs * len ( self . metadata . layers ) case ( \"cls\" , int ()): # Return a CLS token from a random image and fixed layer. return self . metadata . n_imgs case ( \"image\" , int ()): # Return a patch from a random image, fixed layer, and random patch. return self . metadata . n_imgs * ( self . metadata . n_patches_per_img ) case ( \"image\" , \"all\" ): # Return a patch from a random image, random layer and random patch. return ( self . metadata . n_imgs * len ( self . metadata . layers ) * self . metadata . n_patches_per_img ) case _ : typing . assert_never (( self . cfg . patches , self . cfg . layer ))","title":"saev.data.indexed"},{"location":"api/data/indexed/#saev.data.indexed","text":"","title":"indexed"},{"location":"api/data/indexed/#saev.data.indexed.Config","text":"Configuration for loading indexed activation data from disk Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. layer ( int | Literal ['all'] ) \u2013 Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. seed ( int ) \u2013 Random seed. debug ( bool ) \u2013 Whether the dataloader process should log debug messages.","title":"Config"},{"location":"api/data/indexed/#saev.data.indexed.Dataset","text":"Bases: Dataset Dataset of activations from disk. Attributes: cfg ( Config ) \u2013 Configuration set via CLI args. metadata ( Metadata ) \u2013 Activations metadata; automatically loaded from disk. layer_index ( int ) \u2013 Layer index into the shards if we are choosing a specific layer. Source code in src/saev/data/indexed.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) # Check if labels.bin exists labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) self . labels_mmap = None if os . path . exists ( labels_path ): self . labels_mmap = np . memmap ( labels_path , mode = \"r\" , dtype = np . uint8 , shape = ( self . metadata . n_imgs , self . metadata . n_patches_per_img ), ) # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError. self . layer_index = 1_000_000 if isinstance ( self . cfg . layer , int ): err_msg = f \"Non-exact matches for .layer field not supported; { self . cfg . layer } not in { self . metadata . layers } .\" assert self . cfg . layer in self . metadata . layers , err_msg self . layer_index = self . metadata . layers . index ( self . cfg . layer )","title":"Dataset"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.d_vit","text":"Dimension of the underlying vision transformer's embedding space.","title":"d_vit"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.Example","text":"Bases: TypedDict Individual example.","title":"Example"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.__len__","text":"Dataset length depends on patches and layer . Source code in src/saev/data/indexed.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __len__ ( self ) -> int : \"\"\" Dataset length depends on `patches` and `layer`. \"\"\" match ( self . cfg . patches , self . cfg . layer ): case ( \"cls\" , \"all\" ): # Return a CLS token from a random image and random layer. return self . metadata . n_imgs * len ( self . metadata . layers ) case ( \"cls\" , int ()): # Return a CLS token from a random image and fixed layer. return self . metadata . n_imgs case ( \"image\" , int ()): # Return a patch from a random image, fixed layer, and random patch. return self . metadata . n_imgs * ( self . metadata . n_patches_per_img ) case ( \"image\" , \"all\" ): # Return a patch from a random image, random layer and random patch. return ( self . metadata . n_imgs * len ( self . metadata . layers ) * self . metadata . n_patches_per_img ) case _ : typing . assert_never (( self . cfg . patches , self . cfg . layer ))","title":"__len__"},{"location":"api/data/models/","text":"saev.data.models VisionTransformer Bases: ABC Protocol defining the interface for all Vision Transformer models. patch_size abstractmethod property Patch size in pixels (e.g., 14 or 16). forward ( batch ) abstractmethod Run forward pass on batch of images. Source code in src/saev/data/models.py 61 62 63 64 65 @abc . abstractmethod def forward ( self , batch : Float [ Tensor , \"batch 3 width height\" ] ) -> Float [ Tensor , \"batch patches dim\" ]: \"\"\"Run forward pass on batch of images.\"\"\" get_patches ( n_patches_per_img ) abstractmethod Return indices for selecting relevant patches from activations. Source code in src/saev/data/models.py 57 58 59 @abc . abstractmethod def get_patches ( self , n_patches_per_img : int ) -> slice | torch . Tensor : \"\"\"Return indices for selecting relevant patches from activations.\"\"\" get_residuals () abstractmethod Return the list of residual blocks/layers for hook registration. Source code in src/saev/data/models.py 53 54 55 @abc . abstractmethod def get_residuals ( self ) -> list [ torch . nn . Module ]: \"\"\"Return the list of residual blocks/layers for hook registration.\"\"\" make_resize ( ckpt , n_patches_per_img , * , scale = 1.0 , resample = Image . LANCZOS ) abstractmethod staticmethod Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/models.py 42 43 44 45 46 47 48 49 50 51 @staticmethod @abc . abstractmethod def make_resize ( ckpt : str , n_patches_per_img : int , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\" make_transforms ( ckpt , n_patches_per_img ) abstractmethod staticmethod Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/models.py 35 36 37 38 39 40 @staticmethod @abc . abstractmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" list_families () List all ViT family names. Source code in src/saev/data/models.py 88 89 90 def list_families () -> list [ str ]: \"\"\"List all ViT family names.\"\"\" return list ( _global_vit_registry . keys ()) load_vit_cls ( family ) Load a ViT family class. Source code in src/saev/data/models.py 71 72 73 74 75 76 77 @beartype . beartype def load_vit_cls ( family : str ) -> type [ VisionTransformer ]: \"\"\"Load a ViT family class.\"\"\" if family not in _global_vit_registry : raise ValueError ( f \"Family ' { family } ' not found.\" ) return _global_vit_registry [ family ] register_family ( cls ) Register a new ViT family class. Source code in src/saev/data/models.py 80 81 82 83 84 85 @beartype . beartype def register_family ( cls : type [ VisionTransformer ]): \"\"\"Register a new ViT family class.\"\"\" if cls . family in _global_vit_registry : logger . warning ( \"Overwriting key ' %s ' in registry.\" , cls . family ) _global_vit_registry [ cls . family ] = cls","title":"saev.data.models"},{"location":"api/data/models/#saev.data.models","text":"","title":"models"},{"location":"api/data/models/#saev.data.models.VisionTransformer","text":"Bases: ABC Protocol defining the interface for all Vision Transformer models.","title":"VisionTransformer"},{"location":"api/data/models/#saev.data.models.VisionTransformer.patch_size","text":"Patch size in pixels (e.g., 14 or 16).","title":"patch_size"},{"location":"api/data/models/#saev.data.models.VisionTransformer.forward","text":"Run forward pass on batch of images. Source code in src/saev/data/models.py 61 62 63 64 65 @abc . abstractmethod def forward ( self , batch : Float [ Tensor , \"batch 3 width height\" ] ) -> Float [ Tensor , \"batch patches dim\" ]: \"\"\"Run forward pass on batch of images.\"\"\"","title":"forward"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_patches","text":"Return indices for selecting relevant patches from activations. Source code in src/saev/data/models.py 57 58 59 @abc . abstractmethod def get_patches ( self , n_patches_per_img : int ) -> slice | torch . Tensor : \"\"\"Return indices for selecting relevant patches from activations.\"\"\"","title":"get_patches"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_residuals","text":"Return the list of residual blocks/layers for hook registration. Source code in src/saev/data/models.py 53 54 55 @abc . abstractmethod def get_residuals ( self ) -> list [ torch . nn . Module ]: \"\"\"Return the list of residual blocks/layers for hook registration.\"\"\"","title":"get_residuals"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_resize","text":"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/models.py 42 43 44 45 46 47 48 49 50 51 @staticmethod @abc . abstractmethod def make_resize ( ckpt : str , n_patches_per_img : int , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"","title":"make_resize"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_transforms","text":"Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/models.py 35 36 37 38 39 40 @staticmethod @abc . abstractmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"","title":"make_transforms"},{"location":"api/data/models/#saev.data.models.list_families","text":"List all ViT family names. Source code in src/saev/data/models.py 88 89 90 def list_families () -> list [ str ]: \"\"\"List all ViT family names.\"\"\" return list ( _global_vit_registry . keys ())","title":"list_families"},{"location":"api/data/models/#saev.data.models.load_vit_cls","text":"Load a ViT family class. Source code in src/saev/data/models.py 71 72 73 74 75 76 77 @beartype . beartype def load_vit_cls ( family : str ) -> type [ VisionTransformer ]: \"\"\"Load a ViT family class.\"\"\" if family not in _global_vit_registry : raise ValueError ( f \"Family ' { family } ' not found.\" ) return _global_vit_registry [ family ]","title":"load_vit_cls"},{"location":"api/data/models/#saev.data.models.register_family","text":"Register a new ViT family class. Source code in src/saev/data/models.py 80 81 82 83 84 85 @beartype . beartype def register_family ( cls : type [ VisionTransformer ]): \"\"\"Register a new ViT family class.\"\"\" if cls . family in _global_vit_registry : logger . warning ( \"Overwriting key ' %s ' in registry.\" , cls . family ) _global_vit_registry [ cls . family ] = cls","title":"register_family"},{"location":"api/data/ordered/","text":"saev.data.ordered Ordered (sequential) dataloader for activation data. This module provides a high-throughput dataloader that reads activation data from disk shards in sequential order, without shuffling. The implementation uses a single-threaded manager process to ensure data is delivered in the exact order it appears on disk. Patch labels are provided if there is a labels.bin file on disk. See the design decisions in src/saev/data/performance.md. Usage cfg = Config(shard_root=\"./shards\", layer=13, batch_size=4096) dataloader = DataLoader(cfg) for batch in dataloader: ... activations = batch[\"act\"] # [batch_size, d_vit] ... image_indices = batch[\"image_i\"] # [batch_size] ... patch_indices = batch[\"patch_i\"] # [batch_size] ... patch_labels = batch[\"patch_labels\"] # [batch_size] Config ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , batch_size = 1024 * 16 , batch_timeout_s = 30.0 , drop_last = False , buffer_size = 64 , debug = False ) dataclass Configuration for loading ordered (non-shuffled) activation data from disk. batch_size = 1024 * 16 class-attribute instance-attribute Batch size. batch_timeout_s = 30.0 class-attribute instance-attribute How long to wait for at least one batch. buffer_size = 64 class-attribute instance-attribute Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls. debug = False class-attribute instance-attribute Whether the dataloader process should log debug messages. drop_last = False class-attribute instance-attribute Whether to drop the last batch if it's smaller than the others. layer = - 2 class-attribute instance-attribute Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. patches = 'image' class-attribute instance-attribute Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. shard_root = os . path . join ( '.' , 'shards' ) class-attribute instance-attribute Directory with .bin shards and a metadata.json file. DataLoader ( cfg ) High-throughput streaming loader that reads data from disk shards in order (no shuffling). Source code in src/saev/data/ordered.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . logger = logging . getLogger ( \"ordered.DataLoader\" ) self . ctx = mp . get_context () self . manager_proc = None self . batch_queue = None self . stop_event = None self . _n_samples = self . _calculate_n_samples () self . logger . info ( \"Initialized ordered.DataLoader with %d samples. (debug= %s )\" , self . n_samples , self . cfg . debug , ) ExampleBatch Bases: TypedDict Individual example. __iter__ () Yields batches in order. Source code in src/saev/data/ordered.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def __iter__ ( self ) -> collections . abc . Iterable [ ExampleBatch ]: \"\"\"Yields batches in order.\"\"\" self . _start_manager () n = 0 try : while n < self . n_samples : if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : batch = self . batch_queue . get ( timeout = self . cfg . batch_timeout_s ) actual_batch_size = batch [ \"act\" ] . shape [ 0 ] # Handle drop_last if ( self . cfg . drop_last and actual_batch_size < self . cfg . batch_size and n + actual_batch_size >= self . n_samples ): break n += actual_batch_size yield self . ExampleBatch ( ** batch ) continue except queue . Empty : self . logger . info ( \"Did not get a batch from manager process in %.1f s seconds.\" , self . cfg . batch_timeout_s , ) except FileNotFoundError : self . logger . info ( \"Manager process (probably) closed.\" ) continue # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { n } / { self . n_samples } samples.\" ) finally : self . shutdown () __len__ () Returns the number of batches in an epoch. Source code in src/saev/data/ordered.py 409 410 411 412 413 414 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" if self . cfg . drop_last : return self . n_samples // self . cfg . batch_size else : return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"saev.data.ordered"},{"location":"api/data/ordered/#saev.data.ordered","text":"Ordered (sequential) dataloader for activation data. This module provides a high-throughput dataloader that reads activation data from disk shards in sequential order, without shuffling. The implementation uses a single-threaded manager process to ensure data is delivered in the exact order it appears on disk. Patch labels are provided if there is a labels.bin file on disk. See the design decisions in src/saev/data/performance.md. Usage cfg = Config(shard_root=\"./shards\", layer=13, batch_size=4096) dataloader = DataLoader(cfg) for batch in dataloader: ... activations = batch[\"act\"] # [batch_size, d_vit] ... image_indices = batch[\"image_i\"] # [batch_size] ... patch_indices = batch[\"patch_i\"] # [batch_size] ... patch_labels = batch[\"patch_labels\"] # [batch_size]","title":"ordered"},{"location":"api/data/ordered/#saev.data.ordered.Config","text":"Configuration for loading ordered (non-shuffled) activation data from disk.","title":"Config"},{"location":"api/data/ordered/#saev.data.ordered.Config.batch_size","text":"Batch size.","title":"batch_size"},{"location":"api/data/ordered/#saev.data.ordered.Config.batch_timeout_s","text":"How long to wait for at least one batch.","title":"batch_timeout_s"},{"location":"api/data/ordered/#saev.data.ordered.Config.buffer_size","text":"Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.","title":"buffer_size"},{"location":"api/data/ordered/#saev.data.ordered.Config.debug","text":"Whether the dataloader process should log debug messages.","title":"debug"},{"location":"api/data/ordered/#saev.data.ordered.Config.drop_last","text":"Whether to drop the last batch if it's smaller than the others.","title":"drop_last"},{"location":"api/data/ordered/#saev.data.ordered.Config.layer","text":"Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer.","title":"layer"},{"location":"api/data/ordered/#saev.data.ordered.Config.patches","text":"Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.","title":"patches"},{"location":"api/data/ordered/#saev.data.ordered.Config.shard_root","text":"Directory with .bin shards and a metadata.json file.","title":"shard_root"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader","text":"High-throughput streaming loader that reads data from disk shards in order (no shuffling). Source code in src/saev/data/ordered.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . logger = logging . getLogger ( \"ordered.DataLoader\" ) self . ctx = mp . get_context () self . manager_proc = None self . batch_queue = None self . stop_event = None self . _n_samples = self . _calculate_n_samples () self . logger . info ( \"Initialized ordered.DataLoader with %d samples. (debug= %s )\" , self . n_samples , self . cfg . debug , )","title":"DataLoader"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.ExampleBatch","text":"Bases: TypedDict Individual example.","title":"ExampleBatch"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__iter__","text":"Yields batches in order. Source code in src/saev/data/ordered.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def __iter__ ( self ) -> collections . abc . Iterable [ ExampleBatch ]: \"\"\"Yields batches in order.\"\"\" self . _start_manager () n = 0 try : while n < self . n_samples : if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : batch = self . batch_queue . get ( timeout = self . cfg . batch_timeout_s ) actual_batch_size = batch [ \"act\" ] . shape [ 0 ] # Handle drop_last if ( self . cfg . drop_last and actual_batch_size < self . cfg . batch_size and n + actual_batch_size >= self . n_samples ): break n += actual_batch_size yield self . ExampleBatch ( ** batch ) continue except queue . Empty : self . logger . info ( \"Did not get a batch from manager process in %.1f s seconds.\" , self . cfg . batch_timeout_s , ) except FileNotFoundError : self . logger . info ( \"Manager process (probably) closed.\" ) continue # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { n } / { self . n_samples } samples.\" ) finally : self . shutdown ()","title":"__iter__"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__len__","text":"Returns the number of batches in an epoch. Source code in src/saev/data/ordered.py 409 410 411 412 413 414 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" if self . cfg . drop_last : return self . n_samples // self . cfg . batch_size else : return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"__len__"},{"location":"api/data/saev.data/","text":"saev.data .. include:: ./protocol.md .. include:: ./performance.md Dataset ( cfg ) Bases: Dataset Dataset of activations from disk. Attributes: cfg ( Config ) \u2013 Configuration set via CLI args. metadata ( Metadata ) \u2013 Activations metadata; automatically loaded from disk. layer_index ( int ) \u2013 Layer index into the shards if we are choosing a specific layer. Source code in src/saev/data/indexed.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) # Check if labels.bin exists labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) self . labels_mmap = None if os . path . exists ( labels_path ): self . labels_mmap = np . memmap ( labels_path , mode = \"r\" , dtype = np . uint8 , shape = ( self . metadata . n_imgs , self . metadata . n_patches_per_img ), ) # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError. self . layer_index = 1_000_000 if isinstance ( self . cfg . layer , int ): err_msg = f \"Non-exact matches for .layer field not supported; { self . cfg . layer } not in { self . metadata . layers } .\" assert self . cfg . layer in self . metadata . layers , err_msg self . layer_index = self . metadata . layers . index ( self . cfg . layer ) d_vit property Dimension of the underlying vision transformer's embedding space. Example Bases: TypedDict Individual example. __len__ () Dataset length depends on patches and layer . Source code in src/saev/data/indexed.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __len__ ( self ) -> int : \"\"\" Dataset length depends on `patches` and `layer`. \"\"\" match ( self . cfg . patches , self . cfg . layer ): case ( \"cls\" , \"all\" ): # Return a CLS token from a random image and random layer. return self . metadata . n_imgs * len ( self . metadata . layers ) case ( \"cls\" , int ()): # Return a CLS token from a random image and fixed layer. return self . metadata . n_imgs case ( \"image\" , int ()): # Return a patch from a random image, fixed layer, and random patch. return self . metadata . n_imgs * ( self . metadata . n_patches_per_img ) case ( \"image\" , \"all\" ): # Return a patch from a random image, random layer and random patch. return ( self . metadata . n_imgs * len ( self . metadata . layers ) * self . metadata . n_patches_per_img ) case _ : typing . assert_never (( self . cfg . patches , self . cfg . layer )) IndexedConfig ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , seed = 17 , debug = False ) dataclass Configuration for loading indexed activation data from disk Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. layer ( int | Literal ['all'] ) \u2013 Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. seed ( int ) \u2013 Random seed. debug ( bool ) \u2013 Whether the dataloader process should log debug messages. Metadata ( vit_family , vit_ckpt , layers , n_patches_per_img , cls_token , d_vit , n_imgs , max_patches_per_shard , data , pixel_agg = None , dtype = 'float32' , protocol = '1.1' ) dataclass n_imgs_per_shard property Calculate the number of images per shard based on the protocol. Returns: int \u2013 Number of images that fit in a shard. OrderedConfig ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , batch_size = 1024 * 16 , batch_timeout_s = 30.0 , drop_last = False , buffer_size = 64 , debug = False ) dataclass Configuration for loading ordered (non-shuffled) activation data from disk. batch_size = 1024 * 16 class-attribute instance-attribute Batch size. batch_timeout_s = 30.0 class-attribute instance-attribute How long to wait for at least one batch. buffer_size = 64 class-attribute instance-attribute Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls. debug = False class-attribute instance-attribute Whether the dataloader process should log debug messages. drop_last = False class-attribute instance-attribute Whether to drop the last batch if it's smaller than the others. layer = - 2 class-attribute instance-attribute Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. patches = 'image' class-attribute instance-attribute Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. shard_root = os . path . join ( '.' , 'shards' ) class-attribute instance-attribute Directory with .bin shards and a metadata.json file. OrderedDataLoader ( cfg ) High-throughput streaming loader that reads data from disk shards in order (no shuffling). Source code in src/saev/data/ordered.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . logger = logging . getLogger ( \"ordered.DataLoader\" ) self . ctx = mp . get_context () self . manager_proc = None self . batch_queue = None self . stop_event = None self . _n_samples = self . _calculate_n_samples () self . logger . info ( \"Initialized ordered.DataLoader with %d samples. (debug= %s )\" , self . n_samples , self . cfg . debug , ) ExampleBatch Bases: TypedDict Individual example. __iter__ () Yields batches in order. Source code in src/saev/data/ordered.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def __iter__ ( self ) -> collections . abc . Iterable [ ExampleBatch ]: \"\"\"Yields batches in order.\"\"\" self . _start_manager () n = 0 try : while n < self . n_samples : if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : batch = self . batch_queue . get ( timeout = self . cfg . batch_timeout_s ) actual_batch_size = batch [ \"act\" ] . shape [ 0 ] # Handle drop_last if ( self . cfg . drop_last and actual_batch_size < self . cfg . batch_size and n + actual_batch_size >= self . n_samples ): break n += actual_batch_size yield self . ExampleBatch ( ** batch ) continue except queue . Empty : self . logger . info ( \"Did not get a batch from manager process in %.1f s seconds.\" , self . cfg . batch_timeout_s , ) except FileNotFoundError : self . logger . info ( \"Manager process (probably) closed.\" ) continue # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { n } / { self . n_samples } samples.\" ) finally : self . shutdown () __len__ () Returns the number of batches in an epoch. Source code in src/saev/data/ordered.py 409 410 411 412 413 414 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" if self . cfg . drop_last : return self . n_samples // self . cfg . batch_size else : return math . ceil ( self . n_samples / self . cfg . batch_size ) ShuffledConfig ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , batch_size = 1024 * 16 , drop_last = False , scale_norm = False , ignore_labels = list (), n_threads = 4 , buffer_size = 64 , batch_timeout_s = 30.0 , seed = 17 , debug = False , log_every_s = 30.0 ) dataclass Configuration for loading shuffled activation data from disk. Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. batch_size = 1024 * 16 class-attribute instance-attribute Batch size. batch_timeout_s = 30.0 class-attribute instance-attribute How long to wait for at least one batch. buffer_size = 64 class-attribute instance-attribute Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls. debug = False class-attribute instance-attribute Whether the dataloader process should log debug messages. drop_last = False class-attribute instance-attribute Whether to drop the last batch if it's smaller than the others. ignore_labels = dataclasses . field ( default_factory = list ) class-attribute instance-attribute If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background. layer = - 2 class-attribute instance-attribute Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. log_every_s = 30.0 class-attribute instance-attribute How frequently the dataloader process should log (debug) performance messages. n_threads = 4 class-attribute instance-attribute Number of dataloading threads. scale_norm = False class-attribute instance-attribute Whether to scale norms to sqrt(D). seed = 17 class-attribute instance-attribute Random seed. ShuffledDataLoader ( cfg ) High-throughput streaming loader that deterministically shuffles data from disk shards. Source code in src/saev/data/shuffled.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __init__ ( self , cfg : Config ): self . cfg = cfg self . manager_proc = None self . reservoir = None self . stop_event = None self . logger = logging . getLogger ( \"shuffled.DataLoader\" ) self . ctx = mp . get_context () if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) if self . cfg . scale_norm : raise NotImplementedError ( \"scale_norm not implemented.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . _n_samples = self . _calculate_n_samples () # Check if labels.bin exists for filtering self . labels_mmap = None if self . cfg . ignore_labels : labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) if not os . path . exists ( labels_path ): raise FileNotFoundError ( f \"ignore_labels filtering requested but labels.bin not found at { labels_path } \" ) ExampleBatch Bases: TypedDict Individual example. __iter__ () Yields batches. Source code in src/saev/data/shuffled.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __iter__ ( self ) -> collections . abc . Iterator [ ExampleBatch ]: \"\"\"Yields batches.\"\"\" self . _start_manager () n , b = 0 , 0 try : while n < self . n_samples : need = min ( self . cfg . batch_size , self . n_samples - n ) if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : act , meta = self . reservoir . get ( need , timeout = self . cfg . batch_timeout_s ) n += need b += 1 image_i , patch_i = meta . T yield self . ExampleBatch ( act = act , image_i = image_i , patch_i = patch_i ) continue except TimeoutError : if self . cfg . ignore_labels : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds. This can happen when filtering out many labels.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) else : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { b } / { len ( self ) } batches.\" ) finally : self . shutdown () __len__ () Returns the number of batches in an epoch. Source code in src/saev/data/shuffled.py 542 543 544 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" return math . ceil ( self . n_samples / self . cfg . batch_size ) make_ordered_config ( shuffled_cfg , ** overrides ) Create an OrderedConfig from a ShuffledConfig , with optional overrides. Defaults come from shuffled_cfg for fields present in OrderedConfig , and overrides take precedence. Unknown override fields raise TypeError from the OrderedConfig constructor, mirroring dataclasses.replace . Source code in src/saev/data/__init__.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @beartype . beartype def make_ordered_config ( shuffled_cfg : ShuffledConfig , ** overrides : object ) -> OrderedConfig : \"\"\"Create an `OrderedConfig` from a `ShuffledConfig`, with optional overrides. Defaults come from `shuffled_cfg` for fields present in `OrderedConfig`, and `overrides` take precedence. Unknown override fields raise `TypeError` from the `OrderedConfig` constructor, mirroring `dataclasses.replace`. \"\"\" params : dict [ str , object ] = {} for f in dataclasses . fields ( OrderedConfig ): name = f . name if hasattr ( shuffled_cfg , name ): params [ name ] = getattr ( shuffled_cfg , name ) params . update ( overrides ) return OrderedConfig ( ** params )","title":"saev.data"},{"location":"api/data/saev.data/#saev.data","text":".. include:: ./protocol.md .. include:: ./performance.md","title":"data"},{"location":"api/data/saev.data/#saev.data.Dataset","text":"Bases: Dataset Dataset of activations from disk. Attributes: cfg ( Config ) \u2013 Configuration set via CLI args. metadata ( Metadata ) \u2013 Activations metadata; automatically loaded from disk. layer_index ( int ) \u2013 Layer index into the shards if we are choosing a specific layer. Source code in src/saev/data/indexed.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) # Check if labels.bin exists labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) self . labels_mmap = None if os . path . exists ( labels_path ): self . labels_mmap = np . memmap ( labels_path , mode = \"r\" , dtype = np . uint8 , shape = ( self . metadata . n_imgs , self . metadata . n_patches_per_img ), ) # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError. self . layer_index = 1_000_000 if isinstance ( self . cfg . layer , int ): err_msg = f \"Non-exact matches for .layer field not supported; { self . cfg . layer } not in { self . metadata . layers } .\" assert self . cfg . layer in self . metadata . layers , err_msg self . layer_index = self . metadata . layers . index ( self . cfg . layer )","title":"Dataset"},{"location":"api/data/saev.data/#saev.data.Dataset.d_vit","text":"Dimension of the underlying vision transformer's embedding space.","title":"d_vit"},{"location":"api/data/saev.data/#saev.data.Dataset.Example","text":"Bases: TypedDict Individual example.","title":"Example"},{"location":"api/data/saev.data/#saev.data.Dataset.__len__","text":"Dataset length depends on patches and layer . Source code in src/saev/data/indexed.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __len__ ( self ) -> int : \"\"\" Dataset length depends on `patches` and `layer`. \"\"\" match ( self . cfg . patches , self . cfg . layer ): case ( \"cls\" , \"all\" ): # Return a CLS token from a random image and random layer. return self . metadata . n_imgs * len ( self . metadata . layers ) case ( \"cls\" , int ()): # Return a CLS token from a random image and fixed layer. return self . metadata . n_imgs case ( \"image\" , int ()): # Return a patch from a random image, fixed layer, and random patch. return self . metadata . n_imgs * ( self . metadata . n_patches_per_img ) case ( \"image\" , \"all\" ): # Return a patch from a random image, random layer and random patch. return ( self . metadata . n_imgs * len ( self . metadata . layers ) * self . metadata . n_patches_per_img ) case _ : typing . assert_never (( self . cfg . patches , self . cfg . layer ))","title":"__len__"},{"location":"api/data/saev.data/#saev.data.IndexedConfig","text":"Configuration for loading indexed activation data from disk Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. layer ( int | Literal ['all'] ) \u2013 Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. seed ( int ) \u2013 Random seed. debug ( bool ) \u2013 Whether the dataloader process should log debug messages.","title":"IndexedConfig"},{"location":"api/data/saev.data/#saev.data.Metadata","text":"","title":"Metadata"},{"location":"api/data/saev.data/#saev.data.Metadata.n_imgs_per_shard","text":"Calculate the number of images per shard based on the protocol. Returns: int \u2013 Number of images that fit in a shard.","title":"n_imgs_per_shard"},{"location":"api/data/saev.data/#saev.data.OrderedConfig","text":"Configuration for loading ordered (non-shuffled) activation data from disk.","title":"OrderedConfig"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.batch_size","text":"Batch size.","title":"batch_size"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.batch_timeout_s","text":"How long to wait for at least one batch.","title":"batch_timeout_s"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.buffer_size","text":"Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.","title":"buffer_size"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.debug","text":"Whether the dataloader process should log debug messages.","title":"debug"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.drop_last","text":"Whether to drop the last batch if it's smaller than the others.","title":"drop_last"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.layer","text":"Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer.","title":"layer"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.patches","text":"Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.","title":"patches"},{"location":"api/data/saev.data/#saev.data.OrderedConfig.shard_root","text":"Directory with .bin shards and a metadata.json file.","title":"shard_root"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader","text":"High-throughput streaming loader that reads data from disk shards in order (no shuffling). Source code in src/saev/data/ordered.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def __init__ ( self , cfg : Config ): self . cfg = cfg if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . logger = logging . getLogger ( \"ordered.DataLoader\" ) self . ctx = mp . get_context () self . manager_proc = None self . batch_queue = None self . stop_event = None self . _n_samples = self . _calculate_n_samples () self . logger . info ( \"Initialized ordered.DataLoader with %d samples. (debug= %s )\" , self . n_samples , self . cfg . debug , )","title":"OrderedDataLoader"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.ExampleBatch","text":"Bases: TypedDict Individual example.","title":"ExampleBatch"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__iter__","text":"Yields batches in order. Source code in src/saev/data/ordered.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def __iter__ ( self ) -> collections . abc . Iterable [ ExampleBatch ]: \"\"\"Yields batches in order.\"\"\" self . _start_manager () n = 0 try : while n < self . n_samples : if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : batch = self . batch_queue . get ( timeout = self . cfg . batch_timeout_s ) actual_batch_size = batch [ \"act\" ] . shape [ 0 ] # Handle drop_last if ( self . cfg . drop_last and actual_batch_size < self . cfg . batch_size and n + actual_batch_size >= self . n_samples ): break n += actual_batch_size yield self . ExampleBatch ( ** batch ) continue except queue . Empty : self . logger . info ( \"Did not get a batch from manager process in %.1f s seconds.\" , self . cfg . batch_timeout_s , ) except FileNotFoundError : self . logger . info ( \"Manager process (probably) closed.\" ) continue # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { n } / { self . n_samples } samples.\" ) finally : self . shutdown ()","title":"__iter__"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__len__","text":"Returns the number of batches in an epoch. Source code in src/saev/data/ordered.py 409 410 411 412 413 414 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" if self . cfg . drop_last : return self . n_samples // self . cfg . batch_size else : return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"__len__"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig","text":"Configuration for loading shuffled activation data from disk. Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.","title":"ShuffledConfig"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_size","text":"Batch size.","title":"batch_size"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_timeout_s","text":"How long to wait for at least one batch.","title":"batch_timeout_s"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.buffer_size","text":"Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.","title":"buffer_size"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.debug","text":"Whether the dataloader process should log debug messages.","title":"debug"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.drop_last","text":"Whether to drop the last batch if it's smaller than the others.","title":"drop_last"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.ignore_labels","text":"If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.","title":"ignore_labels"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.layer","text":"Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer.","title":"layer"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.log_every_s","text":"How frequently the dataloader process should log (debug) performance messages.","title":"log_every_s"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.n_threads","text":"Number of dataloading threads.","title":"n_threads"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.scale_norm","text":"Whether to scale norms to sqrt(D).","title":"scale_norm"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.seed","text":"Random seed.","title":"seed"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader","text":"High-throughput streaming loader that deterministically shuffles data from disk shards. Source code in src/saev/data/shuffled.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __init__ ( self , cfg : Config ): self . cfg = cfg self . manager_proc = None self . reservoir = None self . stop_event = None self . logger = logging . getLogger ( \"shuffled.DataLoader\" ) self . ctx = mp . get_context () if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) if self . cfg . scale_norm : raise NotImplementedError ( \"scale_norm not implemented.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . _n_samples = self . _calculate_n_samples () # Check if labels.bin exists for filtering self . labels_mmap = None if self . cfg . ignore_labels : labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) if not os . path . exists ( labels_path ): raise FileNotFoundError ( f \"ignore_labels filtering requested but labels.bin not found at { labels_path } \" )","title":"ShuffledDataLoader"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.ExampleBatch","text":"Bases: TypedDict Individual example.","title":"ExampleBatch"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__iter__","text":"Yields batches. Source code in src/saev/data/shuffled.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __iter__ ( self ) -> collections . abc . Iterator [ ExampleBatch ]: \"\"\"Yields batches.\"\"\" self . _start_manager () n , b = 0 , 0 try : while n < self . n_samples : need = min ( self . cfg . batch_size , self . n_samples - n ) if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : act , meta = self . reservoir . get ( need , timeout = self . cfg . batch_timeout_s ) n += need b += 1 image_i , patch_i = meta . T yield self . ExampleBatch ( act = act , image_i = image_i , patch_i = patch_i ) continue except TimeoutError : if self . cfg . ignore_labels : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds. This can happen when filtering out many labels.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) else : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { b } / { len ( self ) } batches.\" ) finally : self . shutdown ()","title":"__iter__"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__len__","text":"Returns the number of batches in an epoch. Source code in src/saev/data/shuffled.py 542 543 544 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"__len__"},{"location":"api/data/saev.data/#saev.data.make_ordered_config","text":"Create an OrderedConfig from a ShuffledConfig , with optional overrides. Defaults come from shuffled_cfg for fields present in OrderedConfig , and overrides take precedence. Unknown override fields raise TypeError from the OrderedConfig constructor, mirroring dataclasses.replace . Source code in src/saev/data/__init__.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @beartype . beartype def make_ordered_config ( shuffled_cfg : ShuffledConfig , ** overrides : object ) -> OrderedConfig : \"\"\"Create an `OrderedConfig` from a `ShuffledConfig`, with optional overrides. Defaults come from `shuffled_cfg` for fields present in `OrderedConfig`, and `overrides` take precedence. Unknown override fields raise `TypeError` from the `OrderedConfig` constructor, mirroring `dataclasses.replace`. \"\"\" params : dict [ str , object ] = {} for f in dataclasses . fields ( OrderedConfig ): name = f . name if hasattr ( shuffled_cfg , name ): params [ name ] = getattr ( shuffled_cfg , name ) params . update ( overrides ) return OrderedConfig ( ** params )","title":"make_ordered_config"},{"location":"api/data/shuffled/","text":"saev.data.shuffled Config ( shard_root = os . path . join ( '.' , 'shards' ), patches = 'image' , layer =- 2 , batch_size = 1024 * 16 , drop_last = False , scale_norm = False , ignore_labels = list (), n_threads = 4 , buffer_size = 64 , batch_timeout_s = 30.0 , seed = 17 , debug = False , log_every_s = 30.0 ) dataclass Configuration for loading shuffled activation data from disk. Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches. batch_size = 1024 * 16 class-attribute instance-attribute Batch size. batch_timeout_s = 30.0 class-attribute instance-attribute How long to wait for at least one batch. buffer_size = 64 class-attribute instance-attribute Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls. debug = False class-attribute instance-attribute Whether the dataloader process should log debug messages. drop_last = False class-attribute instance-attribute Whether to drop the last batch if it's smaller than the others. ignore_labels = dataclasses . field ( default_factory = list ) class-attribute instance-attribute If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background. layer = - 2 class-attribute instance-attribute Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer. log_every_s = 30.0 class-attribute instance-attribute How frequently the dataloader process should log (debug) performance messages. n_threads = 4 class-attribute instance-attribute Number of dataloading threads. scale_norm = False class-attribute instance-attribute Whether to scale norms to sqrt(D). seed = 17 class-attribute instance-attribute Random seed. DataLoader ( cfg ) High-throughput streaming loader that deterministically shuffles data from disk shards. Source code in src/saev/data/shuffled.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __init__ ( self , cfg : Config ): self . cfg = cfg self . manager_proc = None self . reservoir = None self . stop_event = None self . logger = logging . getLogger ( \"shuffled.DataLoader\" ) self . ctx = mp . get_context () if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) if self . cfg . scale_norm : raise NotImplementedError ( \"scale_norm not implemented.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . _n_samples = self . _calculate_n_samples () # Check if labels.bin exists for filtering self . labels_mmap = None if self . cfg . ignore_labels : labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) if not os . path . exists ( labels_path ): raise FileNotFoundError ( f \"ignore_labels filtering requested but labels.bin not found at { labels_path } \" ) ExampleBatch Bases: TypedDict Individual example. __iter__ () Yields batches. Source code in src/saev/data/shuffled.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __iter__ ( self ) -> collections . abc . Iterator [ ExampleBatch ]: \"\"\"Yields batches.\"\"\" self . _start_manager () n , b = 0 , 0 try : while n < self . n_samples : need = min ( self . cfg . batch_size , self . n_samples - n ) if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : act , meta = self . reservoir . get ( need , timeout = self . cfg . batch_timeout_s ) n += need b += 1 image_i , patch_i = meta . T yield self . ExampleBatch ( act = act , image_i = image_i , patch_i = patch_i ) continue except TimeoutError : if self . cfg . ignore_labels : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds. This can happen when filtering out many labels.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) else : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { b } / { len ( self ) } batches.\" ) finally : self . shutdown () __len__ () Returns the number of batches in an epoch. Source code in src/saev/data/shuffled.py 542 543 544 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"saev.data.shuffled"},{"location":"api/data/shuffled/#saev.data.shuffled","text":"","title":"shuffled"},{"location":"api/data/shuffled/#saev.data.shuffled.Config","text":"Configuration for loading shuffled activation data from disk. Attributes: shard_root ( str ) \u2013 Directory with .bin shards and a metadata.json file. patches ( Literal ['cls', 'image', 'all'] ) \u2013 Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches.","title":"Config"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_size","text":"Batch size.","title":"batch_size"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_timeout_s","text":"How long to wait for at least one batch.","title":"batch_timeout_s"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.buffer_size","text":"Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.","title":"buffer_size"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.debug","text":"Whether the dataloader process should log debug messages.","title":"debug"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.drop_last","text":"Whether to drop the last batch if it's smaller than the others.","title":"drop_last"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.ignore_labels","text":"If provided, exclude patches with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.","title":"ignore_labels"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.layer","text":"Which ViT layer(s) to read from disk. -2 selects the second-to-last layer. \"all\" enumerates every recorded layer.","title":"layer"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.log_every_s","text":"How frequently the dataloader process should log (debug) performance messages.","title":"log_every_s"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.n_threads","text":"Number of dataloading threads.","title":"n_threads"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.scale_norm","text":"Whether to scale norms to sqrt(D).","title":"scale_norm"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.seed","text":"Random seed.","title":"seed"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader","text":"High-throughput streaming loader that deterministically shuffles data from disk shards. Source code in src/saev/data/shuffled.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def __init__ ( self , cfg : Config ): self . cfg = cfg self . manager_proc = None self . reservoir = None self . stop_event = None self . logger = logging . getLogger ( \"shuffled.DataLoader\" ) self . ctx = mp . get_context () if not os . path . isdir ( self . cfg . shard_root ): raise RuntimeError ( f \"Activations are not saved at ' { self . cfg . shard_root } '.\" ) if self . cfg . scale_norm : raise NotImplementedError ( \"scale_norm not implemented.\" ) self . metadata = writers . Metadata . load ( self . cfg . shard_root ) # Validate shard files exist shard_info = writers . ShardInfo . load ( self . cfg . shard_root ) for shard in shard_info : shard_path = os . path . join ( self . cfg . shard_root , shard . name ) if not os . path . exists ( shard_path ): raise FileNotFoundError ( f \"Shard file not found: { shard_path } \" ) self . _n_samples = self . _calculate_n_samples () # Check if labels.bin exists for filtering self . labels_mmap = None if self . cfg . ignore_labels : labels_path = os . path . join ( self . cfg . shard_root , \"labels.bin\" ) if not os . path . exists ( labels_path ): raise FileNotFoundError ( f \"ignore_labels filtering requested but labels.bin not found at { labels_path } \" )","title":"DataLoader"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.ExampleBatch","text":"Bases: TypedDict Individual example.","title":"ExampleBatch"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__iter__","text":"Yields batches. Source code in src/saev/data/shuffled.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __iter__ ( self ) -> collections . abc . Iterator [ ExampleBatch ]: \"\"\"Yields batches.\"\"\" self . _start_manager () n , b = 0 , 0 try : while n < self . n_samples : need = min ( self . cfg . batch_size , self . n_samples - n ) if not self . err_queue . empty (): who , tb = self . err_queue . get_nowait () raise RuntimeError ( f \" { who } crashed: \\n { tb } \" ) try : act , meta = self . reservoir . get ( need , timeout = self . cfg . batch_timeout_s ) n += need b += 1 image_i , patch_i = meta . T yield self . ExampleBatch ( act = act , image_i = image_i , patch_i = patch_i ) continue except TimeoutError : if self . cfg . ignore_labels : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds. This can happen when filtering out many labels.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) else : self . logger . info ( \"Did not get a batch from %d worker threads in %.1f s seconds.\" , self . cfg . n_threads , self . cfg . batch_timeout_s , ) # If we don't continue, then we should check on the manager process. if not self . manager_proc . is_alive (): raise RuntimeError ( f \"Manager process died unexpectedly after { b } / { len ( self ) } batches.\" ) finally : self . shutdown ()","title":"__iter__"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__len__","text":"Returns the number of batches in an epoch. Source code in src/saev/data/shuffled.py 542 543 544 def __len__ ( self ) -> int : \"\"\"Returns the number of batches in an epoch.\"\"\" return math . ceil ( self . n_samples / self . cfg . batch_size )","title":"__len__"},{"location":"api/data/siglip/","text":"saev.data.siglip Vit ( ckpt ) Bases: Module , VisionTransformer Source code in src/saev/data/siglip.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , ckpt : str ): super () . __init__ () if ckpt . startswith ( \"hf-hub:\" ): clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) clip , _ = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model assert isinstance ( self . model , open_clip . timm_model . TimmModel ) make_resize ( ckpt , n_patches_per_img =- 1 , * , scale = 1.0 , resample = Image . LANCZOS ) staticmethod Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/siglip.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int = - 1 , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\" from PIL import Image def resize ( img : Image . Image ) -> Image . Image : # SigLIP typically uses 224x224 or 384x384 images # We'll assume 224x224 for simplicity resize_size_px = ( int ( 224 * scale ), int ( 224 * scale )) return img . resize ( resize_size_px , resample = resample ) return resize make_transforms ( ckpt , n_patches_per_img ) staticmethod Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/siglip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" if ckpt . startswith ( \"hf-hub:\" ): _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) _ , img_transform = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"saev.data.siglip"},{"location":"api/data/siglip/#saev.data.siglip","text":"","title":"siglip"},{"location":"api/data/siglip/#saev.data.siglip.Vit","text":"Bases: Module , VisionTransformer Source code in src/saev/data/siglip.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , ckpt : str ): super () . __init__ () if ckpt . startswith ( \"hf-hub:\" ): clip , _ = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) clip , _ = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) self . _ckpt = ckpt model = clip . visual model . proj = None model . output_tokens = True # type: ignore self . model = model assert isinstance ( self . model , open_clip . timm_model . TimmModel )","title":"Vit"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_resize","text":"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks. Source code in src/saev/data/siglip.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @staticmethod def make_resize ( ckpt : str , n_patches_per_img : int = - 1 , * , scale : float = 1.0 , resample : Image . Resampling = Image . LANCZOS , ) -> Callable [[ Image . Image ], Image . Image ]: \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\" from PIL import Image def resize ( img : Image . Image ) -> Image . Image : # SigLIP typically uses 224x224 or 384x384 images # We'll assume 224x224 for simplicity resize_size_px = ( int ( 224 * scale ), int ( 224 * scale )) return img . resize ( resize_size_px , resample = resample ) return resize","title":"make_resize"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_transforms","text":"Create transforms for preprocessing: (img_transform, sample_transform | None). Source code in src/saev/data/siglip.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @staticmethod def make_transforms ( ckpt : str , n_patches_per_img : int ) -> tuple [ Callable , Callable | None ]: \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\" if ckpt . startswith ( \"hf-hub:\" ): _ , img_transform = open_clip . create_model_from_pretrained ( ckpt , cache_dir = helpers . get_cache_dir () ) else : arch , ckpt = ckpt . split ( \"/\" ) _ , img_transform = open_clip . create_model_from_pretrained ( arch , pretrained = ckpt , cache_dir = helpers . get_cache_dir () ) return img_transform , None","title":"make_transforms"},{"location":"api/data/transforms/","text":"saev.data.transforms conv2d_to_tokens ( x_bchw , conv ) Conv2d then flatten spatial to L, return (B, L, D). Source code in src/saev/data/transforms.py 135 136 137 138 139 140 141 @jaxtyped ( typechecker = beartype . beartype ) def conv2d_to_tokens ( x_bchw : Float [ Tensor , \"b c h w\" ], conv : nn . Conv2d ) -> Float [ Tensor , \"b n d\" ]: \"\"\"Conv2d then flatten spatial to L, return (B, L, D).\"\"\" y_bdhw = conv ( x_bchw ) return einops . rearrange ( y_bdhw , \"b d h w -> b (h w) d\" ) resize_to_patch_grid ( img , * , p , n , resample = Image . LANCZOS ) Resize image to (w, h) so that: - w % p == 0, h % p == 0 - (h/p) * (w/p) == N - Minimizes change in aspect ratio. Source code in src/saev/data/transforms.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @beartype . beartype def resize_to_patch_grid ( img : Image . Image , * , p : int , n : int , resample : Image . Resampling | int = Image . LANCZOS , ) -> Image . Image : \"\"\" Resize image to (w, h) so that: - w % p == 0, h % p == 0 - (h/p) * (w/p) == N - Minimizes change in aspect ratio. \"\"\" if p <= 0 or n <= 0 : raise ValueError ( \"p and n must be positive integers\" ) w0 , h0 = img . size a0 = w0 / h0 # Find the aspect ratio closest to a0 best_c = 0 best_dist = float ( \"inf\" ) for i in range ( 1 , int ( math . sqrt ( n ) + 1 )): if n % i != 0 : continue for d in ( i , n // i ): c , r = d , n // d aspect = c / r dist = abs ( aspect - a0 ) if dist < best_dist : best_c = d best_dist = dist c = best_c r = n // c w , h = c * p , r * p return img . resize (( w , h ), resample = resample ) unfolded_conv2d ( x_bchw , conv ) Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1. Source code in src/saev/data/transforms.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @jaxtyped ( typechecker = beartype . beartype ) def unfolded_conv2d ( x_bchw : Float [ Tensor , \"b c h w\" ], conv : nn . Conv2d ) -> Float [ Tensor , \"b n d\" ]: \"\"\" Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1. \"\"\" k = conv . kernel_size [ 0 ] assert conv . kernel_size == ( k , k ) assert conv . stride == ( k , k ) assert conv . padding == ( 0 , 0 ) assert conv . groups == 1 assert conv . dilation == ( 1 , 1 ) * b , c , h , w = x_bchw . shape assert h % k == 0 and w % k == 0 tokens_bnd = einops . rearrange ( x_bchw , \"b c (hp p1) (wp p2) -> b (hp wp) (c p1 p2)\" , p1 = k , p2 = k ) . contiguous () w_dp = conv . weight . reshape ( conv . out_channels , c * k * k ) tokens_bnd = tokens_bnd @ w_dp . T if conv . bias is not None : tokens_bnd = tokens_bnd + conv . bias [ None , None , :] return tokens_bnd","title":"saev.data.transforms"},{"location":"api/data/transforms/#saev.data.transforms","text":"","title":"transforms"},{"location":"api/data/transforms/#saev.data.transforms.conv2d_to_tokens","text":"Conv2d then flatten spatial to L, return (B, L, D). Source code in src/saev/data/transforms.py 135 136 137 138 139 140 141 @jaxtyped ( typechecker = beartype . beartype ) def conv2d_to_tokens ( x_bchw : Float [ Tensor , \"b c h w\" ], conv : nn . Conv2d ) -> Float [ Tensor , \"b n d\" ]: \"\"\"Conv2d then flatten spatial to L, return (B, L, D).\"\"\" y_bdhw = conv ( x_bchw ) return einops . rearrange ( y_bdhw , \"b d h w -> b (h w) d\" )","title":"conv2d_to_tokens"},{"location":"api/data/transforms/#saev.data.transforms.resize_to_patch_grid","text":"Resize image to (w, h) so that: - w % p == 0, h % p == 0 - (h/p) * (w/p) == N - Minimizes change in aspect ratio. Source code in src/saev/data/transforms.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @beartype . beartype def resize_to_patch_grid ( img : Image . Image , * , p : int , n : int , resample : Image . Resampling | int = Image . LANCZOS , ) -> Image . Image : \"\"\" Resize image to (w, h) so that: - w % p == 0, h % p == 0 - (h/p) * (w/p) == N - Minimizes change in aspect ratio. \"\"\" if p <= 0 or n <= 0 : raise ValueError ( \"p and n must be positive integers\" ) w0 , h0 = img . size a0 = w0 / h0 # Find the aspect ratio closest to a0 best_c = 0 best_dist = float ( \"inf\" ) for i in range ( 1 , int ( math . sqrt ( n ) + 1 )): if n % i != 0 : continue for d in ( i , n // i ): c , r = d , n // d aspect = c / r dist = abs ( aspect - a0 ) if dist < best_dist : best_c = d best_dist = dist c = best_c r = n // c w , h = c * p , r * p return img . resize (( w , h ), resample = resample )","title":"resize_to_patch_grid"},{"location":"api/data/transforms/#saev.data.transforms.unfolded_conv2d","text":"Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1. Source code in src/saev/data/transforms.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @jaxtyped ( typechecker = beartype . beartype ) def unfolded_conv2d ( x_bchw : Float [ Tensor , \"b c h w\" ], conv : nn . Conv2d ) -> Float [ Tensor , \"b n d\" ]: \"\"\" Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1. \"\"\" k = conv . kernel_size [ 0 ] assert conv . kernel_size == ( k , k ) assert conv . stride == ( k , k ) assert conv . padding == ( 0 , 0 ) assert conv . groups == 1 assert conv . dilation == ( 1 , 1 ) * b , c , h , w = x_bchw . shape assert h % k == 0 and w % k == 0 tokens_bnd = einops . rearrange ( x_bchw , \"b c (hp p1) (wp p2) -> b (hp wp) (c p1 p2)\" , p1 = k , p2 = k ) . contiguous () w_dp = conv . weight . reshape ( conv . out_channels , c * k * k ) tokens_bnd = tokens_bnd @ w_dp . T if conv . bias is not None : tokens_bnd = tokens_bnd + conv . bias [ None , None , :] return tokens_bnd","title":"unfolded_conv2d"},{"location":"api/data/writers/","text":"saev.data.writers Config ( data = datasets . Imagenet (), dump_to = os . path . join ( '.' , 'shards' ), vit_family = 'clip' , vit_ckpt = 'ViT-L-14/openai' , vit_batch_size = 1024 , n_workers = 8 , d_vit = 1024 , vit_layers = ( lambda : [ - 2 ])(), n_patches_per_img = 256 , cls_token = True , pixel_agg = 'majority' , max_patches_per_shard = 2400000 , ssl = True , device = 'cuda' , n_hours = 24.0 , slurm_acct = '' , slurm_partition = '' , log_to = './logs' ) dataclass Configuration for calculating and saving ViT activations. cls_token = True class-attribute instance-attribute Whether the model has a [CLS] token. d_vit = 1024 class-attribute instance-attribute Dimension of the ViT activations (depends on model). data = dataclasses . field ( default_factory = ( datasets . Imagenet )) class-attribute instance-attribute Which dataset to use. device = 'cuda' class-attribute instance-attribute Which device to use. dump_to = os . path . join ( '.' , 'shards' ) class-attribute instance-attribute Where to write shards. log_to = './logs' class-attribute instance-attribute Where to log Slurm job stdout/stderr. max_patches_per_shard = 2400000 class-attribute instance-attribute Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations. n_hours = 24.0 class-attribute instance-attribute Slurm job length. n_patches_per_img = 256 class-attribute instance-attribute Number of ViT patches per image (depends on model). n_workers = 8 class-attribute instance-attribute Number of dataloader workers. slurm_acct = '' class-attribute instance-attribute Slurm account string. slurm_partition = '' class-attribute instance-attribute Slurm partition. ssl = True class-attribute instance-attribute Whether to use SSL. vit_batch_size = 1024 class-attribute instance-attribute Batch size for ViT inference. vit_ckpt = 'ViT-L-14/openai' class-attribute instance-attribute Specific model checkpoint. vit_family = 'clip' class-attribute instance-attribute Which model family. vit_layers = dataclasses . field ( default_factory = ( lambda : [ - 2 ])) class-attribute instance-attribute Which layers to save. By default, the second-to-last layer. IndexLookup ( metadata , patches , layer ) Index <-> shard helper. map() \u2013 turn a global dataset index into precise physical offsets. length() \u2013 dataset size for a particular (patches, layer) view. Parameters metadata : Metadata Pre-computed dataset statistics (images, patches, layers, shard size). patches: 'cls' | 'image' | 'all' layer: int | 'all' Source code in src/saev/data/writers.py 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def __init__ ( self , metadata : Metadata , patches : tp . Literal [ \"cls\" , \"image\" , \"all\" ], layer : int | tp . Literal [ \"all\" ], ): if not metadata . cls_token and patches == \"cls\" : raise ValueError ( \"Cannot return [CLS] token if one isn't present.\" ) self . metadata = metadata self . patches = patches if isinstance ( layer , int ) and layer not in metadata . layers : raise ValueError ( f \"Layer { layer } not in { metadata . layers } .\" ) self . layer = layer self . layer_to_idx = { layer : i for i , layer in enumerate ( metadata . layers )} map_global ( i ) Return ( shard_i, index in shard (img_i_in_shard, layer_i, token_i) ) Source code in src/saev/data/writers.py 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 def map_global ( self , i : int ) -> tuple [ int , tuple [ int , int , int ]]: \"\"\" Return ------- ( shard_i, index in shard (img_i_in_shard, layer_i, token_i) ) \"\"\" n = self . length () if i < 0 or i >= n : raise IndexError ( f \" { i =} out of range [0, { n } )\" ) match ( self . patches , self . layer ): case ( \"cls\" , \"all\" ): # For CLS token with all layers, i represents (img_idx * n_layers + layer_idx) n_layers = len ( self . metadata . layers ) img_i = i // n_layers layer_idx = i % n_layers shard_i , img_i_in_shard = self . map_img ( img_i ) # CLS token is at position 0 return shard_i , ( img_i_in_shard , layer_idx , 0 ) case ( \"cls\" , int ()): # For CLS token with specific layer, i is the image index img_i = i shard_i , img_i_in_shard = self . map_img ( img_i ) # CLS token is at position 0 return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], 0 ) case ( \"image\" , int ()): # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx) img_i = i // self . metadata . n_patches_per_img token_i = i % self . metadata . n_patches_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], token_i ) case ( \"image\" , \"all\" ): # For image patches with all layers # Total patches per image across all layers total_patches_per_img = self . metadata . n_patches_per_img * len ( self . metadata . layers ) # Calculate which image and which patch within that image across all layers img_i = i // total_patches_per_img remainder = i % total_patches_per_img # Calculate which layer and which patch within that layer layer_idx = remainder // self . metadata . n_patches_per_img token_i = remainder % self . metadata . n_patches_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , layer_idx , token_i ) case ( \"all\" , int ()): n_tokens_per_img = self . metadata . n_patches_per_img + ( 1 if self . metadata . cls_token else 0 ) img_i = i // n_tokens_per_img token_i = i % n_tokens_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], token_i ) case ( \"all\" , \"all\" ): # For all tokens (CLS + patches) with all layers # Calculate total tokens per image across all layers n_tokens_per_img = self . metadata . n_patches_per_img + ( 1 if self . metadata . cls_token else 0 ) total_tokens_per_img = n_tokens_per_img * len ( self . metadata . layers ) # Calculate which image and which token within that image img_i = i // total_tokens_per_img remainder = i % total_tokens_per_img # Calculate which layer and which token within that layer layer_idx = remainder // n_tokens_per_img token_i = remainder % n_tokens_per_img # Map to physical location shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , layer_idx , token_i ) case _ : tp . assert_never (( self . patches , self . layer )) map_img ( img_i ) Return (shard_i, img_i_in_shard) Source code in src/saev/data/writers.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def map_img ( self , img_i : int ) -> tuple [ int , int ]: \"\"\" Return ------- (shard_i, img_i_in_shard) \"\"\" if img_i < 0 or img_i >= self . metadata . n_imgs : raise IndexError ( f \" { img_i =} out of range [0, { self . metadata . n_imgs } )\" ) # Calculate which shard contains this image shard_i = img_i // self . metadata . n_imgs_per_shard img_i_in_shard = img_i % self . metadata . n_imgs_per_shard return shard_i , img_i_in_shard LabelsWriter ( cfg ) LabelsWriter handles writing patch-level segmentation labels to a single binary file. Source code in src/saev/data/writers.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def __init__ ( self , cfg : Config ): self . logger = logging . getLogger ( \"labels-writer\" ) self . root = get_acts_dir ( cfg ) self . n_patches_per_img = cfg . n_patches_per_img self . n_imgs = cfg . data . n_imgs self . has_written = False self . current_idx = 0 # Always create memory-mapped file for labels # If nothing is written, it will be deleted in flush() self . labels_path = os . path . join ( self . root , \"labels.bin\" ) self . labels = np . memmap ( self . labels_path , mode = \"w+\" , dtype = np . uint8 , shape = ( self . n_imgs , self . n_patches_per_img ), ) self . logger . info ( \"Opened labels file ' %s '.\" , self . labels_path ) flush () Flush the memory-mapped file to disk if anything was written. Source code in src/saev/data/writers.py 284 285 286 287 288 def flush ( self ) -> None : \"\"\"Flush the memory-mapped file to disk if anything was written.\"\"\" if self . labels is not None and self . has_written : self . labels . flush () self . logger . info ( \"Flushed labels to ' %s '.\" , self . labels_path ) write_batch ( batch_labels , start_idx ) Write a batch of labels to the memory-mapped file. Parameters: batch_labels ( ndarray | Tensor ) \u2013 Array of shape (batch_size, n_patches_per_img) with uint8 dtype start_idx ( int ) \u2013 Starting index in the global labels array Source code in src/saev/data/writers.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 @beartype . beartype def write_batch ( self , batch_labels : np . ndarray | Tensor , start_idx : int ): \"\"\" Write a batch of labels to the memory-mapped file. Args: batch_labels: Array of shape (batch_size, n_patches_per_img) with uint8 dtype start_idx: Starting index in the global labels array \"\"\" # Convert to numpy if needed if isinstance ( batch_labels , torch . Tensor ): batch_labels = batch_labels . cpu () . numpy () batch_size = len ( batch_labels ) assert start_idx + batch_size <= self . n_imgs assert batch_labels . shape == ( batch_size , self . n_patches_per_img ) assert batch_labels . dtype == np . uint8 self . labels [ start_idx : start_idx + batch_size ] = batch_labels self . current_idx = start_idx + batch_size self . has_written = True Metadata ( vit_family , vit_ckpt , layers , n_patches_per_img , cls_token , d_vit , n_imgs , max_patches_per_shard , data , pixel_agg = None , dtype = 'float32' , protocol = '1.1' ) dataclass n_imgs_per_shard property Calculate the number of images per shard based on the protocol. Returns: int \u2013 Number of images that fit in a shard. Shard ( name , n_imgs ) dataclass A single shard entry in shards.json, recording the filename and number of images. ShardInfo ( shards = list ()) dataclass A read-only container for shard metadata as recorded in shards.json. ShardWriter ( cfg ) ShardWriter is a stateful object that handles sharded activation writing to disk. Source code in src/saev/data/writers.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , cfg : Config ): self . logger = logging . getLogger ( \"shard-writer\" ) self . root = get_acts_dir ( cfg ) n_patches_per_img = cfg . n_patches_per_img if cfg . cls_token : n_patches_per_img += 1 self . n_imgs_per_shard = ( cfg . max_patches_per_shard // len ( cfg . vit_layers ) // n_patches_per_img ) self . shape = ( self . n_imgs_per_shard , len ( cfg . vit_layers ), n_patches_per_img , cfg . d_vit , ) # builder for shard manifest self . _shards : ShardInfo = ShardInfo () # Always initialize labels writer (it handles non-seg datasets internally) self . labels_writer = LabelsWriter ( cfg ) self . shard = - 1 self . acts = None self . next_shard () __enter__ () Context manager entry. Source code in src/saev/data/writers.py 431 432 433 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) Context manager exit - handle cleanup. Source code in src/saev/data/writers.py 435 436 437 438 439 440 441 442 443 444 445 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit - handle cleanup.\"\"\" self . flush () # Delete empty labels file if nothing was written if not self . labels_writer . has_written : if os . path . exists ( self . labels_writer . labels_path ): os . remove ( self . labels_writer . labels_path ) self . logger . info ( \"Removed empty labels file ' %s '.\" , self . labels_writer . labels_path ) write_batch ( activations , start_idx , patch_labels = None ) Write a batch of activations and optionally patch labels. Parameters: activations ( Float [ Tensor , 'batch n_layers all_patches d_vit'] ) \u2013 Batch of activations to write. start_idx ( int ) \u2013 Starting index for this batch. patch_labels ( UInt8 [ Tensor , 'batch n_patches'] | None , default: None ) \u2013 Optional patch labels for segmentation datasets. Source code in src/saev/data/writers.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def write_batch ( self , activations : Float [ Tensor , \"batch n_layers all_patches d_vit\" ], start_idx : int , patch_labels : UInt8 [ Tensor , \"batch n_patches\" ] | None = None , ) -> None : \"\"\"Write a batch of activations and optionally patch labels. Args: activations: Batch of activations to write. start_idx: Starting index for this batch. patch_labels: Optional patch labels for segmentation datasets. \"\"\" batch_size = len ( activations ) end_idx = start_idx + batch_size # Write activations (handling sharding) offset = self . n_imgs_per_shard * self . shard if end_idx >= offset + self . n_imgs_per_shard : # We have run out of space in this mmap'ed file. Let's fill it as much as we can. n_fit = offset + self . n_imgs_per_shard - start_idx self . acts [ start_idx - offset : start_idx - offset + n_fit ] = activations [ : n_fit ] self . filled = start_idx - offset + n_fit # Write labels for the portion that fits if patch_labels is not None : # Convert to numpy uint8 if needed if isinstance ( patch_labels , torch . Tensor ): labels_to_write = ( patch_labels [: n_fit ] . cpu () . numpy () . astype ( np . uint8 ) ) elif not isinstance ( patch_labels , np . ndarray ): labels_to_write = np . array ( patch_labels [: n_fit ], dtype = np . uint8 ) else : labels_to_write = patch_labels [: n_fit ] self . labels_writer . write_batch ( labels_to_write , start_idx ) self . next_shard () # Recursively call write_batch for remaining data if n_fit < batch_size : self . write_batch ( activations [ n_fit :], start_idx + n_fit , patch_labels [ n_fit :] if patch_labels is not None else None , ) else : msg = f \"0 <= { start_idx } - { offset } <= { offset } + { self . n_imgs_per_shard } \" assert 0 <= start_idx - offset <= offset + self . n_imgs_per_shard , msg msg = f \"0 <= { end_idx } - { offset } <= { offset } + { self . n_imgs_per_shard } \" assert 0 <= end_idx - offset <= offset + self . n_imgs_per_shard , msg self . acts [ start_idx - offset : end_idx - offset ] = activations self . filled = end_idx - offset # Write labels if provided if patch_labels is not None : # Convert to numpy uint8 if needed if isinstance ( patch_labels , torch . Tensor ): patch_labels = patch_labels . cpu () . numpy () . astype ( np . uint8 ) elif not isinstance ( patch_labels , np . ndarray ): patch_labels = np . array ( patch_labels , dtype = np . uint8 ) self . labels_writer . write_batch ( patch_labels , start_idx ) get_acts_dir ( cfg ) Return the activations directory based on the relevant values of a config. Also saves a metadata.json file to that directory for human reference. Parameters: cfg ( Config ) \u2013 Config for experiment. Returns: str \u2013 Directory to where activations should be dumped/loaded from. Source code in src/saev/data/writers.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 @beartype . beartype def get_acts_dir ( cfg : Config ) -> str : \"\"\" Return the activations directory based on the relevant values of a config. Also saves a metadata.json file to that directory for human reference. Args: cfg: Config for experiment. Returns: Directory to where activations should be dumped/loaded from. \"\"\" metadata = Metadata . from_cfg ( cfg ) acts_dir = os . path . join ( cfg . dump_to , metadata . hash ) os . makedirs ( acts_dir , exist_ok = True ) metadata . dump ( acts_dir ) return acts_dir get_dataloader ( cfg , * , img_tr = None , seg_tr = None , sample_tr = None ) Get a dataloader for a default map-style dataset. Parameters: cfg ( Config ) \u2013 Config. img_tr ( Callable | None , default: None ) \u2013 Image transform to be applied to each image. seg_tr ( Callable | None , default: None ) \u2013 Segmentation transform to be applied to masks. sample_tr ( Callable | None , default: None ) \u2013 Transform to be applied to sample dicts. Returns: DataLoader \u2013 A PyTorch Dataloader that yields dictionaries with 'image' keys containing image batches, 'index' keys containing original dataset indices and 'label' keys containing label batches. Source code in src/saev/data/writers.py 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 @beartype . beartype def get_dataloader ( cfg : Config , * , img_tr : Callable | None = None , seg_tr : Callable | None = None , sample_tr : Callable | None = None , ) -> torch . utils . data . DataLoader : \"\"\" Get a dataloader for a default map-style dataset. Args: cfg: Config. img_tr: Image transform to be applied to each image. seg_tr: Segmentation transform to be applied to masks. sample_tr: Transform to be applied to sample dicts. Returns: A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches. \"\"\" dataset = datasets . get_dataset ( cfg . data , img_transform = img_tr , seg_transform = seg_tr , sample_transform = sample_tr ) dataloader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = cfg . vit_batch_size , drop_last = False , num_workers = cfg . n_workers , persistent_workers = cfg . n_workers > 0 , shuffle = False , pin_memory = False , ) return dataloader pixel_to_patch_labels ( seg , n_patches , patch_size , pixel_agg = 'majority' , bg_label = 0 , max_classes = 256 ) Convert pixel-level segmentation to patch-level labels using vectorized operations. Parameters: seg ( Image ) \u2013 Pixel-level segmentation mask as PIL Image n_patches ( int ) \u2013 Total number of patches expected patch_size ( int ) \u2013 Size of each patch in pixels pixel_agg ( Literal ['majority', 'prefer-fg'] , default: 'majority' ) \u2013 How to aggregate pixel labels into patch labels bg_label ( int , default: 0 ) \u2013 Background label index max_classes ( int , default: 256 ) \u2013 Maximum number of classes (for bincount) Returns: UInt8 [ Tensor , ' n_patches'] \u2013 Patch labels as uint8 tensor of shape (n_patches,) Source code in src/saev/data/writers.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @jaxtyped ( typechecker = beartype . beartype ) def pixel_to_patch_labels ( seg : Image . Image , n_patches : int , patch_size : int , pixel_agg : tp . Literal [ \"majority\" , \"prefer-fg\" ] = \"majority\" , bg_label : int = 0 , max_classes : int = 256 , ) -> UInt8 [ Tensor , \" n_patches\" ]: \"\"\" Convert pixel-level segmentation to patch-level labels using vectorized operations. Args: seg: Pixel-level segmentation mask as PIL Image n_patches: Total number of patches expected patch_size: Size of each patch in pixels pixel_agg: How to aggregate pixel labels into patch labels bg_label: Background label index max_classes: Maximum number of classes (for bincount) Returns: Patch labels as uint8 tensor of shape (n_patches,) \"\"\" # Convert to torch tensor for vectorized operations seg_tensor = torch . from_numpy ( np . array ( seg , dtype = np . uint8 )) assert seg_tensor . ndim == 2 h , w = seg_tensor . shape # Calculate patch grid dimensions patch_grid_h = h // patch_size patch_grid_w = w // patch_size assert patch_grid_w * patch_grid_h == n_patches , ( f \"Image size { w } x { h } with patch_size { patch_size } gives { patch_grid_w } x { patch_grid_h } = { patch_grid_w * patch_grid_h } patches, expected { n_patches } \" ) # Reshape into patches using einops: (n_patches, patch_size * patch_size) patches = einops . rearrange ( seg_tensor , \"(h p1) (w p2) -> (h w) (p1 p2)\" , p1 = patch_size , p2 = patch_size , h = patch_grid_h , w = patch_grid_w , ) # Use vectorized bincount approach to get class counts for all patches at once # counts[i, c] = number of times class c appears in patch i offsets = torch . arange ( n_patches , device = patches . device ) . unsqueeze ( 1 ) * max_classes flat = ( patches + offsets ) . reshape ( - 1 ) counts = torch . bincount ( flat , minlength = n_patches * max_classes ) . reshape ( n_patches , max_classes ) if pixel_agg == \"majority\" : # Take the most common label in each patch patch_labels = counts . argmax ( dim = 1 ) elif pixel_agg == \"prefer-fg\" : # Take the most common non-background label, or background if all background nonbg = counts . clone () nonbg [:, bg_label ] = 0 has_nonbg = nonbg . sum ( dim = 1 ) > 0 nonbg_arg = nonbg . argmax ( dim = 1 ) bg_tensor = torch . full_like ( nonbg_arg , bg_label ) patch_labels = torch . where ( has_nonbg , nonbg_arg , bg_tensor ) else : tp . assert_never ( pixel_agg ) return patch_labels . to ( torch . uint8 ) worker_fn ( cfg ) Parameters: cfg ( Config ) \u2013 Config for activations. Source code in src/saev/data/writers.py 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 @beartype . beartype def worker_fn ( cfg : Config ): \"\"\" Args: cfg: Config for activations. \"\"\" from . import models if torch . cuda . is_available (): # This enables tf32 on Ampere GPUs which is only 8% slower than # float16 and almost as accurate as float32 # This was a default in pytorch until 1.12 torch . backends . cuda . matmul . allow_tf32 = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . deterministic = False log_format = \"[ %(asctime)s ] [ %(levelname)s ] [ %(name)s ] %(message)s \" logging . basicConfig ( level = logging . INFO , format = log_format ) logger = logging . getLogger ( \"worker_fn\" ) if cfg . device == \"cuda\" and not torch . cuda . is_available (): logger . warning ( \"No CUDA device available, using CPU.\" ) cfg = dataclasses . replace ( cfg , device = \"cpu\" ) vit_cls = models . load_vit_cls ( cfg . vit_family ) vit_instance = vit_cls ( cfg . vit_ckpt ) . to ( cfg . device ) vit = RecordedVisionTransformer ( vit_instance , cfg . n_patches_per_img , cfg . cls_token , cfg . vit_layers ) img_tr , sample_tr = vit_cls . make_transforms ( cfg . vit_ckpt , cfg . n_patches_per_img ) seg_tr = None if _is_segmentation_dataset ( cfg . data ): # For segmentation datasets, create a transform that converts pixels to patches # Use make_resize with NEAREST interpolation for segmentation masks seg_resize_tr = vit_cls . make_resize ( cfg . vit_ckpt , cfg . n_patches_per_img , scale = 1.0 , resample = Image . NEAREST ) def seg_to_patches ( seg ): \"\"\"Transform that resizes segmentation and converts to patch labels.\"\"\" # Convert to patch labels return pixel_to_patch_labels ( seg_resize_tr ( seg ), cfg . n_patches_per_img , patch_size = vit_instance . patch_size , pixel_agg = cfg . pixel_agg , bg_label = cfg . data . bg_label , ) seg_tr = seg_to_patches dataloader = get_dataloader ( cfg , img_tr = img_tr , seg_tr = seg_tr , sample_tr = sample_tr ) n_batches = math . ceil ( cfg . data . n_imgs / cfg . vit_batch_size ) logger . info ( \"Dumping %d batches of %d examples.\" , n_batches , cfg . vit_batch_size ) vit = vit . to ( cfg . device ) # vit = torch.compile(vit) # Use context manager for proper cleanup with ShardWriter ( cfg ) as writer : i = 0 # Calculate and write ViT activations. with torch . inference_mode (): for batch in helpers . progress ( dataloader , total = n_batches ): imgs = batch . get ( \"image\" ) . to ( cfg . device ) grid = batch . get ( \"grid\" ) if grid is not None : grid = grid . to ( cfg . device ) out , cache = vit ( imgs , grid = grid ) else : out , cache = vit ( imgs ) # cache has shape [batch size, n layers, n patches + 1, d vit] del out # Write activations and labels (if present) in one call patch_labels = batch . get ( \"patch_labels\" ) if patch_labels is not None : logger . debug ( f \"Found patch_labels in batch: shape= { patch_labels . shape if hasattr ( patch_labels , 'shape' ) else 'unknown' } \" ) # Ensure correct shape assert patch_labels . shape == ( len ( cache ), cfg . n_patches_per_img ) else : logger . debug ( f \"No patch_labels in batch. Keys: { batch . keys () } \" ) writer . write_batch ( cache , i , patch_labels = patch_labels ) i += len ( cache )","title":"saev.data.writers"},{"location":"api/data/writers/#saev.data.writers","text":"","title":"writers"},{"location":"api/data/writers/#saev.data.writers.Config","text":"Configuration for calculating and saving ViT activations.","title":"Config"},{"location":"api/data/writers/#saev.data.writers.Config.cls_token","text":"Whether the model has a [CLS] token.","title":"cls_token"},{"location":"api/data/writers/#saev.data.writers.Config.d_vit","text":"Dimension of the ViT activations (depends on model).","title":"d_vit"},{"location":"api/data/writers/#saev.data.writers.Config.data","text":"Which dataset to use.","title":"data"},{"location":"api/data/writers/#saev.data.writers.Config.device","text":"Which device to use.","title":"device"},{"location":"api/data/writers/#saev.data.writers.Config.dump_to","text":"Where to write shards.","title":"dump_to"},{"location":"api/data/writers/#saev.data.writers.Config.log_to","text":"Where to log Slurm job stdout/stderr.","title":"log_to"},{"location":"api/data/writers/#saev.data.writers.Config.max_patches_per_shard","text":"Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations.","title":"max_patches_per_shard"},{"location":"api/data/writers/#saev.data.writers.Config.n_hours","text":"Slurm job length.","title":"n_hours"},{"location":"api/data/writers/#saev.data.writers.Config.n_patches_per_img","text":"Number of ViT patches per image (depends on model).","title":"n_patches_per_img"},{"location":"api/data/writers/#saev.data.writers.Config.n_workers","text":"Number of dataloader workers.","title":"n_workers"},{"location":"api/data/writers/#saev.data.writers.Config.slurm_acct","text":"Slurm account string.","title":"slurm_acct"},{"location":"api/data/writers/#saev.data.writers.Config.slurm_partition","text":"Slurm partition.","title":"slurm_partition"},{"location":"api/data/writers/#saev.data.writers.Config.ssl","text":"Whether to use SSL.","title":"ssl"},{"location":"api/data/writers/#saev.data.writers.Config.vit_batch_size","text":"Batch size for ViT inference.","title":"vit_batch_size"},{"location":"api/data/writers/#saev.data.writers.Config.vit_ckpt","text":"Specific model checkpoint.","title":"vit_ckpt"},{"location":"api/data/writers/#saev.data.writers.Config.vit_family","text":"Which model family.","title":"vit_family"},{"location":"api/data/writers/#saev.data.writers.Config.vit_layers","text":"Which layers to save. By default, the second-to-last layer.","title":"vit_layers"},{"location":"api/data/writers/#saev.data.writers.IndexLookup","text":"Index <-> shard helper. map() \u2013 turn a global dataset index into precise physical offsets. length() \u2013 dataset size for a particular (patches, layer) view.","title":"IndexLookup"},{"location":"api/data/writers/#saev.data.writers.IndexLookup--parameters","text":"metadata : Metadata Pre-computed dataset statistics (images, patches, layers, shard size). patches: 'cls' | 'image' | 'all' layer: int | 'all' Source code in src/saev/data/writers.py 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def __init__ ( self , metadata : Metadata , patches : tp . Literal [ \"cls\" , \"image\" , \"all\" ], layer : int | tp . Literal [ \"all\" ], ): if not metadata . cls_token and patches == \"cls\" : raise ValueError ( \"Cannot return [CLS] token if one isn't present.\" ) self . metadata = metadata self . patches = patches if isinstance ( layer , int ) and layer not in metadata . layers : raise ValueError ( f \"Layer { layer } not in { metadata . layers } .\" ) self . layer = layer self . layer_to_idx = { layer : i for i , layer in enumerate ( metadata . layers )}","title":"Parameters"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_global","text":"","title":"map_global"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_global--return","text":"( shard_i, index in shard (img_i_in_shard, layer_i, token_i) ) Source code in src/saev/data/writers.py 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 def map_global ( self , i : int ) -> tuple [ int , tuple [ int , int , int ]]: \"\"\" Return ------- ( shard_i, index in shard (img_i_in_shard, layer_i, token_i) ) \"\"\" n = self . length () if i < 0 or i >= n : raise IndexError ( f \" { i =} out of range [0, { n } )\" ) match ( self . patches , self . layer ): case ( \"cls\" , \"all\" ): # For CLS token with all layers, i represents (img_idx * n_layers + layer_idx) n_layers = len ( self . metadata . layers ) img_i = i // n_layers layer_idx = i % n_layers shard_i , img_i_in_shard = self . map_img ( img_i ) # CLS token is at position 0 return shard_i , ( img_i_in_shard , layer_idx , 0 ) case ( \"cls\" , int ()): # For CLS token with specific layer, i is the image index img_i = i shard_i , img_i_in_shard = self . map_img ( img_i ) # CLS token is at position 0 return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], 0 ) case ( \"image\" , int ()): # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx) img_i = i // self . metadata . n_patches_per_img token_i = i % self . metadata . n_patches_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], token_i ) case ( \"image\" , \"all\" ): # For image patches with all layers # Total patches per image across all layers total_patches_per_img = self . metadata . n_patches_per_img * len ( self . metadata . layers ) # Calculate which image and which patch within that image across all layers img_i = i // total_patches_per_img remainder = i % total_patches_per_img # Calculate which layer and which patch within that layer layer_idx = remainder // self . metadata . n_patches_per_img token_i = remainder % self . metadata . n_patches_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , layer_idx , token_i ) case ( \"all\" , int ()): n_tokens_per_img = self . metadata . n_patches_per_img + ( 1 if self . metadata . cls_token else 0 ) img_i = i // n_tokens_per_img token_i = i % n_tokens_per_img shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , self . layer_to_idx [ self . layer ], token_i ) case ( \"all\" , \"all\" ): # For all tokens (CLS + patches) with all layers # Calculate total tokens per image across all layers n_tokens_per_img = self . metadata . n_patches_per_img + ( 1 if self . metadata . cls_token else 0 ) total_tokens_per_img = n_tokens_per_img * len ( self . metadata . layers ) # Calculate which image and which token within that image img_i = i // total_tokens_per_img remainder = i % total_tokens_per_img # Calculate which layer and which token within that layer layer_idx = remainder // n_tokens_per_img token_i = remainder % n_tokens_per_img # Map to physical location shard_i , img_i_in_shard = self . map_img ( img_i ) return shard_i , ( img_i_in_shard , layer_idx , token_i ) case _ : tp . assert_never (( self . patches , self . layer ))","title":"Return"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_img","text":"","title":"map_img"},{"location":"api/data/writers/#saev.data.writers.IndexLookup.map_img--return","text":"(shard_i, img_i_in_shard) Source code in src/saev/data/writers.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 def map_img ( self , img_i : int ) -> tuple [ int , int ]: \"\"\" Return ------- (shard_i, img_i_in_shard) \"\"\" if img_i < 0 or img_i >= self . metadata . n_imgs : raise IndexError ( f \" { img_i =} out of range [0, { self . metadata . n_imgs } )\" ) # Calculate which shard contains this image shard_i = img_i // self . metadata . n_imgs_per_shard img_i_in_shard = img_i % self . metadata . n_imgs_per_shard return shard_i , img_i_in_shard","title":"Return"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter","text":"LabelsWriter handles writing patch-level segmentation labels to a single binary file. Source code in src/saev/data/writers.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def __init__ ( self , cfg : Config ): self . logger = logging . getLogger ( \"labels-writer\" ) self . root = get_acts_dir ( cfg ) self . n_patches_per_img = cfg . n_patches_per_img self . n_imgs = cfg . data . n_imgs self . has_written = False self . current_idx = 0 # Always create memory-mapped file for labels # If nothing is written, it will be deleted in flush() self . labels_path = os . path . join ( self . root , \"labels.bin\" ) self . labels = np . memmap ( self . labels_path , mode = \"w+\" , dtype = np . uint8 , shape = ( self . n_imgs , self . n_patches_per_img ), ) self . logger . info ( \"Opened labels file ' %s '.\" , self . labels_path )","title":"LabelsWriter"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter.flush","text":"Flush the memory-mapped file to disk if anything was written. Source code in src/saev/data/writers.py 284 285 286 287 288 def flush ( self ) -> None : \"\"\"Flush the memory-mapped file to disk if anything was written.\"\"\" if self . labels is not None and self . has_written : self . labels . flush () self . logger . info ( \"Flushed labels to ' %s '.\" , self . labels_path )","title":"flush"},{"location":"api/data/writers/#saev.data.writers.LabelsWriter.write_batch","text":"Write a batch of labels to the memory-mapped file. Parameters: batch_labels ( ndarray | Tensor ) \u2013 Array of shape (batch_size, n_patches_per_img) with uint8 dtype start_idx ( int ) \u2013 Starting index in the global labels array Source code in src/saev/data/writers.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 @beartype . beartype def write_batch ( self , batch_labels : np . ndarray | Tensor , start_idx : int ): \"\"\" Write a batch of labels to the memory-mapped file. Args: batch_labels: Array of shape (batch_size, n_patches_per_img) with uint8 dtype start_idx: Starting index in the global labels array \"\"\" # Convert to numpy if needed if isinstance ( batch_labels , torch . Tensor ): batch_labels = batch_labels . cpu () . numpy () batch_size = len ( batch_labels ) assert start_idx + batch_size <= self . n_imgs assert batch_labels . shape == ( batch_size , self . n_patches_per_img ) assert batch_labels . dtype == np . uint8 self . labels [ start_idx : start_idx + batch_size ] = batch_labels self . current_idx = start_idx + batch_size self . has_written = True","title":"write_batch"},{"location":"api/data/writers/#saev.data.writers.Metadata","text":"","title":"Metadata"},{"location":"api/data/writers/#saev.data.writers.Metadata.n_imgs_per_shard","text":"Calculate the number of images per shard based on the protocol. Returns: int \u2013 Number of images that fit in a shard.","title":"n_imgs_per_shard"},{"location":"api/data/writers/#saev.data.writers.Shard","text":"A single shard entry in shards.json, recording the filename and number of images.","title":"Shard"},{"location":"api/data/writers/#saev.data.writers.ShardInfo","text":"A read-only container for shard metadata as recorded in shards.json.","title":"ShardInfo"},{"location":"api/data/writers/#saev.data.writers.ShardWriter","text":"ShardWriter is a stateful object that handles sharded activation writing to disk. Source code in src/saev/data/writers.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , cfg : Config ): self . logger = logging . getLogger ( \"shard-writer\" ) self . root = get_acts_dir ( cfg ) n_patches_per_img = cfg . n_patches_per_img if cfg . cls_token : n_patches_per_img += 1 self . n_imgs_per_shard = ( cfg . max_patches_per_shard // len ( cfg . vit_layers ) // n_patches_per_img ) self . shape = ( self . n_imgs_per_shard , len ( cfg . vit_layers ), n_patches_per_img , cfg . d_vit , ) # builder for shard manifest self . _shards : ShardInfo = ShardInfo () # Always initialize labels writer (it handles non-seg datasets internally) self . labels_writer = LabelsWriter ( cfg ) self . shard = - 1 self . acts = None self . next_shard ()","title":"ShardWriter"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.__enter__","text":"Context manager entry. Source code in src/saev/data/writers.py 431 432 433 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self","title":"__enter__"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.__exit__","text":"Context manager exit - handle cleanup. Source code in src/saev/data/writers.py 435 436 437 438 439 440 441 442 443 444 445 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit - handle cleanup.\"\"\" self . flush () # Delete empty labels file if nothing was written if not self . labels_writer . has_written : if os . path . exists ( self . labels_writer . labels_path ): os . remove ( self . labels_writer . labels_path ) self . logger . info ( \"Removed empty labels file ' %s '.\" , self . labels_writer . labels_path )","title":"__exit__"},{"location":"api/data/writers/#saev.data.writers.ShardWriter.write_batch","text":"Write a batch of activations and optionally patch labels. Parameters: activations ( Float [ Tensor , 'batch n_layers all_patches d_vit'] ) \u2013 Batch of activations to write. start_idx ( int ) \u2013 Starting index for this batch. patch_labels ( UInt8 [ Tensor , 'batch n_patches'] | None , default: None ) \u2013 Optional patch labels for segmentation datasets. Source code in src/saev/data/writers.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def write_batch ( self , activations : Float [ Tensor , \"batch n_layers all_patches d_vit\" ], start_idx : int , patch_labels : UInt8 [ Tensor , \"batch n_patches\" ] | None = None , ) -> None : \"\"\"Write a batch of activations and optionally patch labels. Args: activations: Batch of activations to write. start_idx: Starting index for this batch. patch_labels: Optional patch labels for segmentation datasets. \"\"\" batch_size = len ( activations ) end_idx = start_idx + batch_size # Write activations (handling sharding) offset = self . n_imgs_per_shard * self . shard if end_idx >= offset + self . n_imgs_per_shard : # We have run out of space in this mmap'ed file. Let's fill it as much as we can. n_fit = offset + self . n_imgs_per_shard - start_idx self . acts [ start_idx - offset : start_idx - offset + n_fit ] = activations [ : n_fit ] self . filled = start_idx - offset + n_fit # Write labels for the portion that fits if patch_labels is not None : # Convert to numpy uint8 if needed if isinstance ( patch_labels , torch . Tensor ): labels_to_write = ( patch_labels [: n_fit ] . cpu () . numpy () . astype ( np . uint8 ) ) elif not isinstance ( patch_labels , np . ndarray ): labels_to_write = np . array ( patch_labels [: n_fit ], dtype = np . uint8 ) else : labels_to_write = patch_labels [: n_fit ] self . labels_writer . write_batch ( labels_to_write , start_idx ) self . next_shard () # Recursively call write_batch for remaining data if n_fit < batch_size : self . write_batch ( activations [ n_fit :], start_idx + n_fit , patch_labels [ n_fit :] if patch_labels is not None else None , ) else : msg = f \"0 <= { start_idx } - { offset } <= { offset } + { self . n_imgs_per_shard } \" assert 0 <= start_idx - offset <= offset + self . n_imgs_per_shard , msg msg = f \"0 <= { end_idx } - { offset } <= { offset } + { self . n_imgs_per_shard } \" assert 0 <= end_idx - offset <= offset + self . n_imgs_per_shard , msg self . acts [ start_idx - offset : end_idx - offset ] = activations self . filled = end_idx - offset # Write labels if provided if patch_labels is not None : # Convert to numpy uint8 if needed if isinstance ( patch_labels , torch . Tensor ): patch_labels = patch_labels . cpu () . numpy () . astype ( np . uint8 ) elif not isinstance ( patch_labels , np . ndarray ): patch_labels = np . array ( patch_labels , dtype = np . uint8 ) self . labels_writer . write_batch ( patch_labels , start_idx )","title":"write_batch"},{"location":"api/data/writers/#saev.data.writers.get_acts_dir","text":"Return the activations directory based on the relevant values of a config. Also saves a metadata.json file to that directory for human reference. Parameters: cfg ( Config ) \u2013 Config for experiment. Returns: str \u2013 Directory to where activations should be dumped/loaded from. Source code in src/saev/data/writers.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 @beartype . beartype def get_acts_dir ( cfg : Config ) -> str : \"\"\" Return the activations directory based on the relevant values of a config. Also saves a metadata.json file to that directory for human reference. Args: cfg: Config for experiment. Returns: Directory to where activations should be dumped/loaded from. \"\"\" metadata = Metadata . from_cfg ( cfg ) acts_dir = os . path . join ( cfg . dump_to , metadata . hash ) os . makedirs ( acts_dir , exist_ok = True ) metadata . dump ( acts_dir ) return acts_dir","title":"get_acts_dir"},{"location":"api/data/writers/#saev.data.writers.get_dataloader","text":"Get a dataloader for a default map-style dataset. Parameters: cfg ( Config ) \u2013 Config. img_tr ( Callable | None , default: None ) \u2013 Image transform to be applied to each image. seg_tr ( Callable | None , default: None ) \u2013 Segmentation transform to be applied to masks. sample_tr ( Callable | None , default: None ) \u2013 Transform to be applied to sample dicts. Returns: DataLoader \u2013 A PyTorch Dataloader that yields dictionaries with 'image' keys containing image batches, 'index' keys containing original dataset indices and 'label' keys containing label batches. Source code in src/saev/data/writers.py 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 @beartype . beartype def get_dataloader ( cfg : Config , * , img_tr : Callable | None = None , seg_tr : Callable | None = None , sample_tr : Callable | None = None , ) -> torch . utils . data . DataLoader : \"\"\" Get a dataloader for a default map-style dataset. Args: cfg: Config. img_tr: Image transform to be applied to each image. seg_tr: Segmentation transform to be applied to masks. sample_tr: Transform to be applied to sample dicts. Returns: A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches. \"\"\" dataset = datasets . get_dataset ( cfg . data , img_transform = img_tr , seg_transform = seg_tr , sample_transform = sample_tr ) dataloader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = cfg . vit_batch_size , drop_last = False , num_workers = cfg . n_workers , persistent_workers = cfg . n_workers > 0 , shuffle = False , pin_memory = False , ) return dataloader","title":"get_dataloader"},{"location":"api/data/writers/#saev.data.writers.pixel_to_patch_labels","text":"Convert pixel-level segmentation to patch-level labels using vectorized operations. Parameters: seg ( Image ) \u2013 Pixel-level segmentation mask as PIL Image n_patches ( int ) \u2013 Total number of patches expected patch_size ( int ) \u2013 Size of each patch in pixels pixel_agg ( Literal ['majority', 'prefer-fg'] , default: 'majority' ) \u2013 How to aggregate pixel labels into patch labels bg_label ( int , default: 0 ) \u2013 Background label index max_classes ( int , default: 256 ) \u2013 Maximum number of classes (for bincount) Returns: UInt8 [ Tensor , ' n_patches'] \u2013 Patch labels as uint8 tensor of shape (n_patches,) Source code in src/saev/data/writers.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @jaxtyped ( typechecker = beartype . beartype ) def pixel_to_patch_labels ( seg : Image . Image , n_patches : int , patch_size : int , pixel_agg : tp . Literal [ \"majority\" , \"prefer-fg\" ] = \"majority\" , bg_label : int = 0 , max_classes : int = 256 , ) -> UInt8 [ Tensor , \" n_patches\" ]: \"\"\" Convert pixel-level segmentation to patch-level labels using vectorized operations. Args: seg: Pixel-level segmentation mask as PIL Image n_patches: Total number of patches expected patch_size: Size of each patch in pixels pixel_agg: How to aggregate pixel labels into patch labels bg_label: Background label index max_classes: Maximum number of classes (for bincount) Returns: Patch labels as uint8 tensor of shape (n_patches,) \"\"\" # Convert to torch tensor for vectorized operations seg_tensor = torch . from_numpy ( np . array ( seg , dtype = np . uint8 )) assert seg_tensor . ndim == 2 h , w = seg_tensor . shape # Calculate patch grid dimensions patch_grid_h = h // patch_size patch_grid_w = w // patch_size assert patch_grid_w * patch_grid_h == n_patches , ( f \"Image size { w } x { h } with patch_size { patch_size } gives { patch_grid_w } x { patch_grid_h } = { patch_grid_w * patch_grid_h } patches, expected { n_patches } \" ) # Reshape into patches using einops: (n_patches, patch_size * patch_size) patches = einops . rearrange ( seg_tensor , \"(h p1) (w p2) -> (h w) (p1 p2)\" , p1 = patch_size , p2 = patch_size , h = patch_grid_h , w = patch_grid_w , ) # Use vectorized bincount approach to get class counts for all patches at once # counts[i, c] = number of times class c appears in patch i offsets = torch . arange ( n_patches , device = patches . device ) . unsqueeze ( 1 ) * max_classes flat = ( patches + offsets ) . reshape ( - 1 ) counts = torch . bincount ( flat , minlength = n_patches * max_classes ) . reshape ( n_patches , max_classes ) if pixel_agg == \"majority\" : # Take the most common label in each patch patch_labels = counts . argmax ( dim = 1 ) elif pixel_agg == \"prefer-fg\" : # Take the most common non-background label, or background if all background nonbg = counts . clone () nonbg [:, bg_label ] = 0 has_nonbg = nonbg . sum ( dim = 1 ) > 0 nonbg_arg = nonbg . argmax ( dim = 1 ) bg_tensor = torch . full_like ( nonbg_arg , bg_label ) patch_labels = torch . where ( has_nonbg , nonbg_arg , bg_tensor ) else : tp . assert_never ( pixel_agg ) return patch_labels . to ( torch . uint8 )","title":"pixel_to_patch_labels"},{"location":"api/data/writers/#saev.data.writers.worker_fn","text":"Parameters: cfg ( Config ) \u2013 Config for activations. Source code in src/saev/data/writers.py 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 @beartype . beartype def worker_fn ( cfg : Config ): \"\"\" Args: cfg: Config for activations. \"\"\" from . import models if torch . cuda . is_available (): # This enables tf32 on Ampere GPUs which is only 8% slower than # float16 and almost as accurate as float32 # This was a default in pytorch until 1.12 torch . backends . cuda . matmul . allow_tf32 = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . deterministic = False log_format = \"[ %(asctime)s ] [ %(levelname)s ] [ %(name)s ] %(message)s \" logging . basicConfig ( level = logging . INFO , format = log_format ) logger = logging . getLogger ( \"worker_fn\" ) if cfg . device == \"cuda\" and not torch . cuda . is_available (): logger . warning ( \"No CUDA device available, using CPU.\" ) cfg = dataclasses . replace ( cfg , device = \"cpu\" ) vit_cls = models . load_vit_cls ( cfg . vit_family ) vit_instance = vit_cls ( cfg . vit_ckpt ) . to ( cfg . device ) vit = RecordedVisionTransformer ( vit_instance , cfg . n_patches_per_img , cfg . cls_token , cfg . vit_layers ) img_tr , sample_tr = vit_cls . make_transforms ( cfg . vit_ckpt , cfg . n_patches_per_img ) seg_tr = None if _is_segmentation_dataset ( cfg . data ): # For segmentation datasets, create a transform that converts pixels to patches # Use make_resize with NEAREST interpolation for segmentation masks seg_resize_tr = vit_cls . make_resize ( cfg . vit_ckpt , cfg . n_patches_per_img , scale = 1.0 , resample = Image . NEAREST ) def seg_to_patches ( seg ): \"\"\"Transform that resizes segmentation and converts to patch labels.\"\"\" # Convert to patch labels return pixel_to_patch_labels ( seg_resize_tr ( seg ), cfg . n_patches_per_img , patch_size = vit_instance . patch_size , pixel_agg = cfg . pixel_agg , bg_label = cfg . data . bg_label , ) seg_tr = seg_to_patches dataloader = get_dataloader ( cfg , img_tr = img_tr , seg_tr = seg_tr , sample_tr = sample_tr ) n_batches = math . ceil ( cfg . data . n_imgs / cfg . vit_batch_size ) logger . info ( \"Dumping %d batches of %d examples.\" , n_batches , cfg . vit_batch_size ) vit = vit . to ( cfg . device ) # vit = torch.compile(vit) # Use context manager for proper cleanup with ShardWriter ( cfg ) as writer : i = 0 # Calculate and write ViT activations. with torch . inference_mode (): for batch in helpers . progress ( dataloader , total = n_batches ): imgs = batch . get ( \"image\" ) . to ( cfg . device ) grid = batch . get ( \"grid\" ) if grid is not None : grid = grid . to ( cfg . device ) out , cache = vit ( imgs , grid = grid ) else : out , cache = vit ( imgs ) # cache has shape [batch size, n layers, n patches + 1, d vit] del out # Write activations and labels (if present) in one call patch_labels = batch . get ( \"patch_labels\" ) if patch_labels is not None : logger . debug ( f \"Found patch_labels in batch: shape= { patch_labels . shape if hasattr ( patch_labels , 'shape' ) else 'unknown' } \" ) # Ensure correct shape assert patch_labels . shape == ( len ( cache ), cfg . n_patches_per_img ) else : logger . debug ( f \"No patch_labels in batch. Keys: { batch . keys () } \" ) writer . write_batch ( cache , i , patch_labels = patch_labels ) i += len ( cache )","title":"worker_fn"},{"location":"api/nn/modeling/","text":"saev.nn.modeling Neural network architectures for sparse autoencoders. BatchTopK ( top_k = 32 ) dataclass top_k = 32 class-attribute instance-attribute How many values are allowed to be non-zero per sample in the batch. BatchTopKActivation ( cfg = BatchTopK ()) Bases: Module Batch Top-K activation function. For use as activation function of sparse encoder. Applies top-k selection per sample in the batch. Source code in src/saev/nn/modeling.py 235 236 237 238 def __init__ ( self , cfg : BatchTopK = BatchTopK ()): super () . __init__ () self . cfg = cfg self . k = cfg . top_k forward ( x ) Apply top-k activation to each sample in the batch. Source code in src/saev/nn/modeling.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def forward ( self , x : Float [ Tensor , \"batch d_sae\" ]) -> Float [ Tensor , \"batch d_sae\" ]: \"\"\" Apply top-k activation to each sample in the batch. \"\"\" if self . k <= 0 : raise ValueError ( \"k must be a positive integer.\" ) # Handle case where k exceeds number of elements per sample k = min ( self . k , x . shape [ - 1 ]) # Apply top-k per sample (along the last dimension) k_vals , k_inds = torch . topk ( x , k , dim =- 1 , sorted = False ) mask = torch . zeros_like ( x ) . scatter_ ( dim =- 1 , index = k_inds , src = torch . ones_like ( x ) ) return torch . mul ( mask , x ) Relu () dataclass Vanilla ReLU SparseAutoencoder ( cfg ) Bases: Module Sparse auto-encoder (SAE) using L1 sparsity penalty. Source code in src/saev/nn/modeling.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , cfg : SparseAutoencoderConfig ): super () . __init__ () self . cfg = cfg self . logger = logging . getLogger ( f \"sae(seed= { cfg . seed } )\" ) self . W_enc = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_vit , cfg . d_sae )) ) self . b_enc = torch . nn . Parameter ( torch . zeros ( cfg . d_sae )) self . W_dec = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_sae , cfg . d_vit )) ) self . b_dec = torch . nn . Parameter ( torch . zeros ( cfg . d_vit )) self . normalize_w_dec () self . activation = get_activation ( cfg . activation ) decode ( f_x , * , prefixes = None ) Decode latent features to reconstructions. Parameters: f_x ( Float [ Tensor , 'batch d_sae'] ) \u2013 Latent features of shape (batch, d_sae) prefixes ( Int64 [ Tensor , ' n_prefixes'] | None , default: None ) \u2013 Optional tensor of prefix lengths for Matryoshka decoding. Returns: Float [ Tensor , 'batch n_prefixes d_model'] \u2013 Matryoshka reconstructions (batch, n_prefixes, d_model). Source code in src/saev/nn/modeling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def decode ( self , f_x : Float [ Tensor , \"batch d_sae\" ], * , prefixes : Int64 [ Tensor , \" n_prefixes\" ] | None = None , ) -> Float [ Tensor , \"batch n_prefixes d_model\" ]: \"\"\" Decode latent features to reconstructions. Args: f_x: Latent features of shape (batch, d_sae) prefixes: Optional tensor of prefix lengths for Matryoshka decoding. Returns: Matryoshka reconstructions (batch, n_prefixes, d_model). \"\"\" b , d_sae = f_x . shape # Matryoshka cumulative decode device = f_x . device if prefixes is None : prefixes = torch . tensor ([ d_sae ], dtype = torch . int64 ) assert torch . all ( prefixes [ 1 :] > prefixes [: - 1 ]) assert 1 <= int ( prefixes [ 0 ]) and int ( prefixes [ - 1 ]) == d_sae prefixes = prefixes . to ( device ) # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ... block_indices = torch . cat ([ torch . tensor ([ 0 ], dtype = prefixes . dtype , device = device ), prefixes , ]) blocks = list ( zip ( block_indices [: - 1 ], block_indices [ 1 :])) # Compute block outputs block_outputs = [] for i , ( start , end ) in enumerate ( blocks ): # Each block uses its portion of f_x and W_dec block_f_x = f_x [:, start : end ] block_W_dec = self . W_dec [ start : end , :] # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -> (batch, d_vit) # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit) block_output = einops . einsum ( block_f_x , block_W_dec , \"... d_sae_block, d_sae_block d_vit -> ... d_vit\" , ) # Add bias only to the first block if i == 0 : block_output = block_output + self . b_dec block_outputs . append ( block_output ) # Cumulative sum to get prefix reconstructions x_hats = torch . cumsum ( torch . stack ( block_outputs , dim =- 2 ), dim =- 2 ) return x_hats forward ( x ) Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Parameters: x ( Float [ Tensor , 'batch d_model'] ) \u2013 a batch of ViT activations. Source code in src/saev/nn/modeling.py 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : Float [ Tensor , \"batch d_model\" ] ) -> tuple [ Float [ Tensor , \"batch d_model\" ], Float [ Tensor , \"batch d_sae\" ]]: \"\"\" Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Arguments: x: a batch of ViT activations. \"\"\" f_x = self . encode ( x ) x_hat = self . decode ( f_x ) return x_hat , f_x normalize_w_dec () Set W_dec to unit-norm columns. Source code in src/saev/nn/modeling.py 172 173 174 175 176 177 178 @torch . no_grad () def normalize_w_dec ( self ): \"\"\" Set W_dec to unit-norm columns. \"\"\" if self . cfg . normalize_w_dec : self . W_dec . data /= torch . norm ( self . W_dec . data , dim = 1 , keepdim = True ) remove_parallel_grads () Update grads so that they remove the parallel component (d_sae, d_vit) shape Source code in src/saev/nn/modeling.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @torch . no_grad () def remove_parallel_grads ( self ): \"\"\" Update grads so that they remove the parallel component (d_sae, d_vit) shape \"\"\" if not self . cfg . remove_parallel_grads : return parallel_component = einops . einsum ( self . W_dec . grad , self . W_dec . data , \"d_sae d_vit, d_sae d_vit -> d_sae\" , ) self . W_dec . grad -= einops . einsum ( parallel_component , self . W_dec . data , \"d_sae, d_sae d_vit -> d_sae d_vit\" , ) SparseAutoencoderConfig ( d_vit = 1024 , exp_factor = 16 , n_reinit_samples = 1024 * 16 * 32 , remove_parallel_grads = True , normalize_w_dec = True , seed = 0 , activation = Relu ()) dataclass exp_factor = 16 class-attribute instance-attribute Expansion factor for SAE. n_reinit_samples = 1024 * 16 * 32 class-attribute instance-attribute Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean. normalize_w_dec = True class-attribute instance-attribute Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation. remove_parallel_grads = True class-attribute instance-attribute Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic. seed = 0 class-attribute instance-attribute Random seed. TopK ( top_k = 32 ) dataclass top_k = 32 class-attribute instance-attribute How many values are allowed to be non-zero. TopKActivation ( cfg = TopK ()) Bases: Module Top-K activation function. For use as activation function of sparse encoder. Source code in src/saev/nn/modeling.py 208 209 210 211 def __init__ ( self , cfg : TopK = TopK ()): super () . __init__ () self . cfg = cfg self . k = cfg . top_k forward ( x ) Apply top-k activation to the input tensor. Source code in src/saev/nn/modeling.py 213 214 215 216 217 218 219 220 221 222 223 224 225 def forward ( self , x : Float [ Tensor , \"batch d_sae\" ]) -> Float [ Tensor , \"batch d_sae\" ]: \"\"\" Apply top-k activation to the input tensor. \"\"\" if self . k <= 0 : raise ValueError ( \"k must be a positive integer.\" ) k_vals , k_inds = torch . topk ( x , self . k , dim =- 1 , sorted = False ) mask = torch . zeros_like ( x ) . scatter_ ( dim =- 1 , index = k_inds , src = torch . ones_like ( x ) ) return torch . mul ( mask , x ) dump ( fpath , sae ) Save an SAE checkpoint to disk along with configuration, using the trick from equinox . Parameters: fpath ( str ) \u2013 filepath to save checkpoint to. sae ( SparseAutoencoder ) \u2013 sparse autoencoder checkpoint to save. Source code in src/saev/nn/modeling.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @beartype . beartype def dump ( fpath : str , sae : SparseAutoencoder ): \"\"\" Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation). Arguments: fpath: filepath to save checkpoint to. sae: sparse autoencoder checkpoint to save. \"\"\" # Custom serialization to handle activation object cfg_dict = dataclasses . asdict ( sae . cfg ) # Replace activation dict with custom format activation = sae . cfg . activation cfg_dict [ \"activation\" ] = { \"cls\" : activation . __class__ . __name__ , \"params\" : dataclasses . asdict ( activation ), } header = { \"schema\" : 2 , \"cfg\" : cfg_dict , \"commit\" : helpers . current_git_commit () or \"unknown\" , \"lib\" : __version__ , } os . makedirs ( os . path . dirname ( fpath ), exist_ok = True ) with open ( fpath , \"wb\" ) as fd : header_str = json . dumps ( header ) fd . write (( header_str + \" \\n \" ) . encode ( \"utf-8\" )) torch . save ( sae . state_dict (), fd ) load ( fpath , * , device = 'cpu' ) Loads a sparse autoencoder from disk. Source code in src/saev/nn/modeling.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 @beartype . beartype def load ( fpath : str , * , device = \"cpu\" ) -> SparseAutoencoder : \"\"\" Loads a sparse autoencoder from disk. \"\"\" with open ( fpath , \"rb\" ) as fd : header = json . loads ( fd . readline ()) buffer = io . BytesIO ( fd . read ()) if \"schema\" not in header : # Original, pre-schema format: just raw config parameters # Remove old parameters that no longer exist for keyword in ( \"sparsity_coeff\" , \"ghost_grads\" , \"l1_coeff\" , \"use_ghost_grads\" ): header . pop ( keyword , None ) # Legacy format - create SparseAutoencoderConfig with Relu activation cfg = SparseAutoencoderConfig ( ** header , activation = Relu ()) elif header [ \"schema\" ] == 1 : # Schema version 1: A cautionary tale of poor version management # # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent! # # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.) # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict # # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format. # # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number! cls_name = header . get ( \"cls\" , \"SparseAutoencoderConfig\" ) cfg_dict = header [ \"cfg\" ] if cls_name in [ \"Relu\" , \"TopK\" , \"BatchTopK\" ]: # Format 1A: Old format where cls indicates the activation type activation_cls = globals ()[ cls_name ] if cls_name in [ \"TopK\" , \"BatchTopK\" ]: activation = activation_cls ( top_k = cfg_dict . get ( \"top_k\" , 32 )) else : activation = activation_cls () cfg = SparseAutoencoderConfig ( ** cfg_dict , activation = activation ) else : # Format 1B: Newer format with activation as dict if \"activation\" in cfg_dict : activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) elif header [ \"schema\" ] == 2 : # Schema version 2: cleaner format with activation serialization cfg_dict = header [ \"cfg\" ] activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) else : raise ValueError ( f \"Unknown schema version: { header [ 'schema' ] } \" ) model = SparseAutoencoder ( cfg ) model . load_state_dict ( torch . load ( buffer , weights_only = True , map_location = device )) return model","title":"saev.nn.modeling"},{"location":"api/nn/modeling/#saev.nn.modeling","text":"Neural network architectures for sparse autoencoders.","title":"modeling"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK","text":"","title":"BatchTopK"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK.top_k","text":"How many values are allowed to be non-zero per sample in the batch.","title":"top_k"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation","text":"Bases: Module Batch Top-K activation function. For use as activation function of sparse encoder. Applies top-k selection per sample in the batch. Source code in src/saev/nn/modeling.py 235 236 237 238 def __init__ ( self , cfg : BatchTopK = BatchTopK ()): super () . __init__ () self . cfg = cfg self . k = cfg . top_k","title":"BatchTopKActivation"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation.forward","text":"Apply top-k activation to each sample in the batch. Source code in src/saev/nn/modeling.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def forward ( self , x : Float [ Tensor , \"batch d_sae\" ]) -> Float [ Tensor , \"batch d_sae\" ]: \"\"\" Apply top-k activation to each sample in the batch. \"\"\" if self . k <= 0 : raise ValueError ( \"k must be a positive integer.\" ) # Handle case where k exceeds number of elements per sample k = min ( self . k , x . shape [ - 1 ]) # Apply top-k per sample (along the last dimension) k_vals , k_inds = torch . topk ( x , k , dim =- 1 , sorted = False ) mask = torch . zeros_like ( x ) . scatter_ ( dim =- 1 , index = k_inds , src = torch . ones_like ( x ) ) return torch . mul ( mask , x )","title":"forward"},{"location":"api/nn/modeling/#saev.nn.modeling.Relu","text":"Vanilla ReLU","title":"Relu"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder","text":"Bases: Module Sparse auto-encoder (SAE) using L1 sparsity penalty. Source code in src/saev/nn/modeling.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , cfg : SparseAutoencoderConfig ): super () . __init__ () self . cfg = cfg self . logger = logging . getLogger ( f \"sae(seed= { cfg . seed } )\" ) self . W_enc = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_vit , cfg . d_sae )) ) self . b_enc = torch . nn . Parameter ( torch . zeros ( cfg . d_sae )) self . W_dec = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_sae , cfg . d_vit )) ) self . b_dec = torch . nn . Parameter ( torch . zeros ( cfg . d_vit )) self . normalize_w_dec () self . activation = get_activation ( cfg . activation )","title":"SparseAutoencoder"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.decode","text":"Decode latent features to reconstructions. Parameters: f_x ( Float [ Tensor , 'batch d_sae'] ) \u2013 Latent features of shape (batch, d_sae) prefixes ( Int64 [ Tensor , ' n_prefixes'] | None , default: None ) \u2013 Optional tensor of prefix lengths for Matryoshka decoding. Returns: Float [ Tensor , 'batch n_prefixes d_model'] \u2013 Matryoshka reconstructions (batch, n_prefixes, d_model). Source code in src/saev/nn/modeling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def decode ( self , f_x : Float [ Tensor , \"batch d_sae\" ], * , prefixes : Int64 [ Tensor , \" n_prefixes\" ] | None = None , ) -> Float [ Tensor , \"batch n_prefixes d_model\" ]: \"\"\" Decode latent features to reconstructions. Args: f_x: Latent features of shape (batch, d_sae) prefixes: Optional tensor of prefix lengths for Matryoshka decoding. Returns: Matryoshka reconstructions (batch, n_prefixes, d_model). \"\"\" b , d_sae = f_x . shape # Matryoshka cumulative decode device = f_x . device if prefixes is None : prefixes = torch . tensor ([ d_sae ], dtype = torch . int64 ) assert torch . all ( prefixes [ 1 :] > prefixes [: - 1 ]) assert 1 <= int ( prefixes [ 0 ]) and int ( prefixes [ - 1 ]) == d_sae prefixes = prefixes . to ( device ) # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ... block_indices = torch . cat ([ torch . tensor ([ 0 ], dtype = prefixes . dtype , device = device ), prefixes , ]) blocks = list ( zip ( block_indices [: - 1 ], block_indices [ 1 :])) # Compute block outputs block_outputs = [] for i , ( start , end ) in enumerate ( blocks ): # Each block uses its portion of f_x and W_dec block_f_x = f_x [:, start : end ] block_W_dec = self . W_dec [ start : end , :] # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -> (batch, d_vit) # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit) block_output = einops . einsum ( block_f_x , block_W_dec , \"... d_sae_block, d_sae_block d_vit -> ... d_vit\" , ) # Add bias only to the first block if i == 0 : block_output = block_output + self . b_dec block_outputs . append ( block_output ) # Cumulative sum to get prefix reconstructions x_hats = torch . cumsum ( torch . stack ( block_outputs , dim =- 2 ), dim =- 2 ) return x_hats","title":"decode"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.forward","text":"Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Parameters: x ( Float [ Tensor , 'batch d_model'] ) \u2013 a batch of ViT activations. Source code in src/saev/nn/modeling.py 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : Float [ Tensor , \"batch d_model\" ] ) -> tuple [ Float [ Tensor , \"batch d_model\" ], Float [ Tensor , \"batch d_sae\" ]]: \"\"\" Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Arguments: x: a batch of ViT activations. \"\"\" f_x = self . encode ( x ) x_hat = self . decode ( f_x ) return x_hat , f_x","title":"forward"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.normalize_w_dec","text":"Set W_dec to unit-norm columns. Source code in src/saev/nn/modeling.py 172 173 174 175 176 177 178 @torch . no_grad () def normalize_w_dec ( self ): \"\"\" Set W_dec to unit-norm columns. \"\"\" if self . cfg . normalize_w_dec : self . W_dec . data /= torch . norm ( self . W_dec . data , dim = 1 , keepdim = True )","title":"normalize_w_dec"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.remove_parallel_grads","text":"Update grads so that they remove the parallel component (d_sae, d_vit) shape Source code in src/saev/nn/modeling.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @torch . no_grad () def remove_parallel_grads ( self ): \"\"\" Update grads so that they remove the parallel component (d_sae, d_vit) shape \"\"\" if not self . cfg . remove_parallel_grads : return parallel_component = einops . einsum ( self . W_dec . grad , self . W_dec . data , \"d_sae d_vit, d_sae d_vit -> d_sae\" , ) self . W_dec . grad -= einops . einsum ( parallel_component , self . W_dec . data , \"d_sae, d_sae d_vit -> d_sae d_vit\" , )","title":"remove_parallel_grads"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig","text":"","title":"SparseAutoencoderConfig"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.exp_factor","text":"Expansion factor for SAE.","title":"exp_factor"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples","text":"Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.","title":"n_reinit_samples"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec","text":"Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.","title":"normalize_w_dec"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads","text":"Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.","title":"remove_parallel_grads"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.seed","text":"Random seed.","title":"seed"},{"location":"api/nn/modeling/#saev.nn.modeling.TopK","text":"","title":"TopK"},{"location":"api/nn/modeling/#saev.nn.modeling.TopK.top_k","text":"How many values are allowed to be non-zero.","title":"top_k"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation","text":"Bases: Module Top-K activation function. For use as activation function of sparse encoder. Source code in src/saev/nn/modeling.py 208 209 210 211 def __init__ ( self , cfg : TopK = TopK ()): super () . __init__ () self . cfg = cfg self . k = cfg . top_k","title":"TopKActivation"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation.forward","text":"Apply top-k activation to the input tensor. Source code in src/saev/nn/modeling.py 213 214 215 216 217 218 219 220 221 222 223 224 225 def forward ( self , x : Float [ Tensor , \"batch d_sae\" ]) -> Float [ Tensor , \"batch d_sae\" ]: \"\"\" Apply top-k activation to the input tensor. \"\"\" if self . k <= 0 : raise ValueError ( \"k must be a positive integer.\" ) k_vals , k_inds = torch . topk ( x , self . k , dim =- 1 , sorted = False ) mask = torch . zeros_like ( x ) . scatter_ ( dim =- 1 , index = k_inds , src = torch . ones_like ( x ) ) return torch . mul ( mask , x )","title":"forward"},{"location":"api/nn/modeling/#saev.nn.modeling.dump","text":"Save an SAE checkpoint to disk along with configuration, using the trick from equinox . Parameters: fpath ( str ) \u2013 filepath to save checkpoint to. sae ( SparseAutoencoder ) \u2013 sparse autoencoder checkpoint to save. Source code in src/saev/nn/modeling.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @beartype . beartype def dump ( fpath : str , sae : SparseAutoencoder ): \"\"\" Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation). Arguments: fpath: filepath to save checkpoint to. sae: sparse autoencoder checkpoint to save. \"\"\" # Custom serialization to handle activation object cfg_dict = dataclasses . asdict ( sae . cfg ) # Replace activation dict with custom format activation = sae . cfg . activation cfg_dict [ \"activation\" ] = { \"cls\" : activation . __class__ . __name__ , \"params\" : dataclasses . asdict ( activation ), } header = { \"schema\" : 2 , \"cfg\" : cfg_dict , \"commit\" : helpers . current_git_commit () or \"unknown\" , \"lib\" : __version__ , } os . makedirs ( os . path . dirname ( fpath ), exist_ok = True ) with open ( fpath , \"wb\" ) as fd : header_str = json . dumps ( header ) fd . write (( header_str + \" \\n \" ) . encode ( \"utf-8\" )) torch . save ( sae . state_dict (), fd )","title":"dump"},{"location":"api/nn/modeling/#saev.nn.modeling.load","text":"Loads a sparse autoencoder from disk. Source code in src/saev/nn/modeling.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 @beartype . beartype def load ( fpath : str , * , device = \"cpu\" ) -> SparseAutoencoder : \"\"\" Loads a sparse autoencoder from disk. \"\"\" with open ( fpath , \"rb\" ) as fd : header = json . loads ( fd . readline ()) buffer = io . BytesIO ( fd . read ()) if \"schema\" not in header : # Original, pre-schema format: just raw config parameters # Remove old parameters that no longer exist for keyword in ( \"sparsity_coeff\" , \"ghost_grads\" , \"l1_coeff\" , \"use_ghost_grads\" ): header . pop ( keyword , None ) # Legacy format - create SparseAutoencoderConfig with Relu activation cfg = SparseAutoencoderConfig ( ** header , activation = Relu ()) elif header [ \"schema\" ] == 1 : # Schema version 1: A cautionary tale of poor version management # # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent! # # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.) # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict # # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format. # # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number! cls_name = header . get ( \"cls\" , \"SparseAutoencoderConfig\" ) cfg_dict = header [ \"cfg\" ] if cls_name in [ \"Relu\" , \"TopK\" , \"BatchTopK\" ]: # Format 1A: Old format where cls indicates the activation type activation_cls = globals ()[ cls_name ] if cls_name in [ \"TopK\" , \"BatchTopK\" ]: activation = activation_cls ( top_k = cfg_dict . get ( \"top_k\" , 32 )) else : activation = activation_cls () cfg = SparseAutoencoderConfig ( ** cfg_dict , activation = activation ) else : # Format 1B: Newer format with activation as dict if \"activation\" in cfg_dict : activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) elif header [ \"schema\" ] == 2 : # Schema version 2: cleaner format with activation serialization cfg_dict = header [ \"cfg\" ] activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) else : raise ValueError ( f \"Unknown schema version: { header [ 'schema' ] } \" ) model = SparseAutoencoder ( cfg ) model . load_state_dict ( torch . load ( buffer , weights_only = True , map_location = device )) return model","title":"load"},{"location":"api/nn/objectives/","text":"saev.nn.objectives Loss () dataclass The loss term for an autoencoder training batch. loss property Total loss. Matryoshka ( sparsity_coeff = 0.0004 , n_prefixes = 10 ) dataclass Config for the Matryoshka loss for another arbitrary SAE class. Reference code is here: https://github.com/noanabeshima/matryoshka-saes and the original reading is https://sparselatents.com/matryoshka.html and https://arxiv.org/pdf/2503.17547 n_prefixes = 10 class-attribute instance-attribute Number of random length prefixes to use for loss calculation. sparsity_coeff = 0.0004 class-attribute instance-attribute How much to weight sparsity loss term (if not using TopK/BatchTopK). MatryoshkaLoss ( mse , sparsity , l0 , l1 ) dataclass Bases: Loss The composite loss terms for an training batch. l0 instance-attribute Sum of L0 magnitudes of hidden activations for all prefix lengths. l1 instance-attribute Sum of L1 magnitudes of hidden activations for all prefix lengths. loss property Total loss. mse instance-attribute Average of reconstruction loss (mean squared error) for all prefix lengths. sparsity instance-attribute Sparsity loss, typically lambda * L1. MatryoshkaObjective ( cfg ) Bases: Objective Torch module for calculating the matryoshka loss for an SAE. Source code in src/saev/nn/objectives.py 147 148 149 150 151 def __init__ ( self , cfg : Matryoshka ): super () . __init__ () self . cfg = cfg # Keep sparsity_coeff as mutable attribute for scheduler compatibility self . sparsity_coeff = cfg . sparsity_coeff Vanilla ( sparsity_coeff = 0.0004 ) dataclass sparsity_coeff = 0.0004 class-attribute instance-attribute How much to weight sparsity loss term. VanillaLoss ( mse , sparsity , l0 , l1 ) dataclass Bases: Loss The vanilla loss terms for an training batch. l0 instance-attribute L0 magnitude of hidden activations. l1 instance-attribute L1 magnitude of hidden activations. loss property Total loss. mse instance-attribute Reconstruction loss (mean squared error). sparsity instance-attribute Sparsity loss, typically lambda * L1. sample_prefixes ( d_sae , n_prefixes , min_prefix_length = 1 , pareto_power = 0.5 ) Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547) Parameters: d_sae ( int ) \u2013 Total number of latent dimensions n_prefixes ( int ) \u2013 Number of prefixes to sample min_prefix_length ( int , default: 1 ) \u2013 Minimum length of any prefix pareto_power ( float , default: 0.5 ) \u2013 Power parameter for Pareto distribution (lower = more uniform) Returns: Int64 [ Tensor , ' n_prefixes'] \u2013 torch.Tensor: Sorted prefix lengths Source code in src/saev/nn/objectives.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @torch . no_grad () @jaxtyped ( typechecker = beartype . beartype ) def sample_prefixes ( d_sae : int , n_prefixes : int , min_prefix_length : int = 1 , pareto_power : float = 0.5 ) -> Int64 [ Tensor , \" n_prefixes\" ]: \"\"\" Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547) Args: d_sae: Total number of latent dimensions n_prefixes: Number of prefixes to sample min_prefix_length: Minimum length of any prefix pareto_power: Power parameter for Pareto distribution (lower = more uniform) Returns: torch.Tensor: Sorted prefix lengths \"\"\" if n_prefixes <= 1 : return torch . tensor ([ d_sae ], dtype = torch . int64 ) assert n_prefixes <= d_sae # Calculate probability distribution favoring shorter prefixes lengths = torch . arange ( 1 , d_sae ) pareto_cdf = 1 - (( min_prefix_length / lengths . float ()) ** pareto_power ) pareto_pdf = torch . cat ([ pareto_cdf [: 1 ], pareto_cdf [ 1 :] - pareto_cdf [: - 1 ]]) probability_dist = pareto_pdf / pareto_pdf . sum () # Sample and sort prefix lengths sampled_indices = torch . multinomial ( probability_dist , num_samples = n_prefixes - 1 , replacement = False ) # Convert indices to actual prefix lengths prefixes = lengths [ sampled_indices ] # Add n_latents as the final prefix prefixes = torch . cat (( prefixes . detach () . clone (), torch . tensor ([ d_sae ]))) prefixes , _ = torch . sort ( prefixes , descending = False ) return prefixes . to ( torch . int64 )","title":"saev.nn.objectives"},{"location":"api/nn/objectives/#saev.nn.objectives","text":"","title":"objectives"},{"location":"api/nn/objectives/#saev.nn.objectives.Loss","text":"The loss term for an autoencoder training batch.","title":"Loss"},{"location":"api/nn/objectives/#saev.nn.objectives.Loss.loss","text":"Total loss.","title":"loss"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka","text":"Config for the Matryoshka loss for another arbitrary SAE class. Reference code is here: https://github.com/noanabeshima/matryoshka-saes and the original reading is https://sparselatents.com/matryoshka.html and https://arxiv.org/pdf/2503.17547","title":"Matryoshka"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.n_prefixes","text":"Number of random length prefixes to use for loss calculation.","title":"n_prefixes"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.sparsity_coeff","text":"How much to weight sparsity loss term (if not using TopK/BatchTopK).","title":"sparsity_coeff"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss","text":"Bases: Loss The composite loss terms for an training batch.","title":"MatryoshkaLoss"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l0","text":"Sum of L0 magnitudes of hidden activations for all prefix lengths.","title":"l0"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l1","text":"Sum of L1 magnitudes of hidden activations for all prefix lengths.","title":"l1"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.loss","text":"Total loss.","title":"loss"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.mse","text":"Average of reconstruction loss (mean squared error) for all prefix lengths.","title":"mse"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.sparsity","text":"Sparsity loss, typically lambda * L1.","title":"sparsity"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaObjective","text":"Bases: Objective Torch module for calculating the matryoshka loss for an SAE. Source code in src/saev/nn/objectives.py 147 148 149 150 151 def __init__ ( self , cfg : Matryoshka ): super () . __init__ () self . cfg = cfg # Keep sparsity_coeff as mutable attribute for scheduler compatibility self . sparsity_coeff = cfg . sparsity_coeff","title":"MatryoshkaObjective"},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla","text":"","title":"Vanilla"},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla.sparsity_coeff","text":"How much to weight sparsity loss term.","title":"sparsity_coeff"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss","text":"Bases: Loss The vanilla loss terms for an training batch.","title":"VanillaLoss"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l0","text":"L0 magnitude of hidden activations.","title":"l0"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l1","text":"L1 magnitude of hidden activations.","title":"l1"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.loss","text":"Total loss.","title":"loss"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.mse","text":"Reconstruction loss (mean squared error).","title":"mse"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.sparsity","text":"Sparsity loss, typically lambda * L1.","title":"sparsity"},{"location":"api/nn/objectives/#saev.nn.objectives.sample_prefixes","text":"Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547) Parameters: d_sae ( int ) \u2013 Total number of latent dimensions n_prefixes ( int ) \u2013 Number of prefixes to sample min_prefix_length ( int , default: 1 ) \u2013 Minimum length of any prefix pareto_power ( float , default: 0.5 ) \u2013 Power parameter for Pareto distribution (lower = more uniform) Returns: Int64 [ Tensor , ' n_prefixes'] \u2013 torch.Tensor: Sorted prefix lengths Source code in src/saev/nn/objectives.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @torch . no_grad () @jaxtyped ( typechecker = beartype . beartype ) def sample_prefixes ( d_sae : int , n_prefixes : int , min_prefix_length : int = 1 , pareto_power : float = 0.5 ) -> Int64 [ Tensor , \" n_prefixes\" ]: \"\"\" Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547) Args: d_sae: Total number of latent dimensions n_prefixes: Number of prefixes to sample min_prefix_length: Minimum length of any prefix pareto_power: Power parameter for Pareto distribution (lower = more uniform) Returns: torch.Tensor: Sorted prefix lengths \"\"\" if n_prefixes <= 1 : return torch . tensor ([ d_sae ], dtype = torch . int64 ) assert n_prefixes <= d_sae # Calculate probability distribution favoring shorter prefixes lengths = torch . arange ( 1 , d_sae ) pareto_cdf = 1 - (( min_prefix_length / lengths . float ()) ** pareto_power ) pareto_pdf = torch . cat ([ pareto_cdf [: 1 ], pareto_cdf [ 1 :] - pareto_cdf [: - 1 ]]) probability_dist = pareto_pdf / pareto_pdf . sum () # Sample and sort prefix lengths sampled_indices = torch . multinomial ( probability_dist , num_samples = n_prefixes - 1 , replacement = False ) # Convert indices to actual prefix lengths prefixes = lengths [ sampled_indices ] # Add n_latents as the final prefix prefixes = torch . cat (( prefixes . detach () . clone (), torch . tensor ([ d_sae ]))) prefixes , _ = torch . sort ( prefixes , descending = False ) return prefixes . to ( torch . int64 )","title":"sample_prefixes"},{"location":"api/nn/saev.nn/","text":"saev.nn SparseAutoencoder ( cfg ) Bases: Module Sparse auto-encoder (SAE) using L1 sparsity penalty. Source code in src/saev/nn/modeling.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , cfg : SparseAutoencoderConfig ): super () . __init__ () self . cfg = cfg self . logger = logging . getLogger ( f \"sae(seed= { cfg . seed } )\" ) self . W_enc = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_vit , cfg . d_sae )) ) self . b_enc = torch . nn . Parameter ( torch . zeros ( cfg . d_sae )) self . W_dec = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_sae , cfg . d_vit )) ) self . b_dec = torch . nn . Parameter ( torch . zeros ( cfg . d_vit )) self . normalize_w_dec () self . activation = get_activation ( cfg . activation ) decode ( f_x , * , prefixes = None ) Decode latent features to reconstructions. Parameters: f_x ( Float [ Tensor , 'batch d_sae'] ) \u2013 Latent features of shape (batch, d_sae) prefixes ( Int64 [ Tensor , ' n_prefixes'] | None , default: None ) \u2013 Optional tensor of prefix lengths for Matryoshka decoding. Returns: Float [ Tensor , 'batch n_prefixes d_model'] \u2013 Matryoshka reconstructions (batch, n_prefixes, d_model). Source code in src/saev/nn/modeling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def decode ( self , f_x : Float [ Tensor , \"batch d_sae\" ], * , prefixes : Int64 [ Tensor , \" n_prefixes\" ] | None = None , ) -> Float [ Tensor , \"batch n_prefixes d_model\" ]: \"\"\" Decode latent features to reconstructions. Args: f_x: Latent features of shape (batch, d_sae) prefixes: Optional tensor of prefix lengths for Matryoshka decoding. Returns: Matryoshka reconstructions (batch, n_prefixes, d_model). \"\"\" b , d_sae = f_x . shape # Matryoshka cumulative decode device = f_x . device if prefixes is None : prefixes = torch . tensor ([ d_sae ], dtype = torch . int64 ) assert torch . all ( prefixes [ 1 :] > prefixes [: - 1 ]) assert 1 <= int ( prefixes [ 0 ]) and int ( prefixes [ - 1 ]) == d_sae prefixes = prefixes . to ( device ) # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ... block_indices = torch . cat ([ torch . tensor ([ 0 ], dtype = prefixes . dtype , device = device ), prefixes , ]) blocks = list ( zip ( block_indices [: - 1 ], block_indices [ 1 :])) # Compute block outputs block_outputs = [] for i , ( start , end ) in enumerate ( blocks ): # Each block uses its portion of f_x and W_dec block_f_x = f_x [:, start : end ] block_W_dec = self . W_dec [ start : end , :] # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -> (batch, d_vit) # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit) block_output = einops . einsum ( block_f_x , block_W_dec , \"... d_sae_block, d_sae_block d_vit -> ... d_vit\" , ) # Add bias only to the first block if i == 0 : block_output = block_output + self . b_dec block_outputs . append ( block_output ) # Cumulative sum to get prefix reconstructions x_hats = torch . cumsum ( torch . stack ( block_outputs , dim =- 2 ), dim =- 2 ) return x_hats forward ( x ) Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Parameters: x ( Float [ Tensor , 'batch d_model'] ) \u2013 a batch of ViT activations. Source code in src/saev/nn/modeling.py 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : Float [ Tensor , \"batch d_model\" ] ) -> tuple [ Float [ Tensor , \"batch d_model\" ], Float [ Tensor , \"batch d_sae\" ]]: \"\"\" Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Arguments: x: a batch of ViT activations. \"\"\" f_x = self . encode ( x ) x_hat = self . decode ( f_x ) return x_hat , f_x normalize_w_dec () Set W_dec to unit-norm columns. Source code in src/saev/nn/modeling.py 172 173 174 175 176 177 178 @torch . no_grad () def normalize_w_dec ( self ): \"\"\" Set W_dec to unit-norm columns. \"\"\" if self . cfg . normalize_w_dec : self . W_dec . data /= torch . norm ( self . W_dec . data , dim = 1 , keepdim = True ) remove_parallel_grads () Update grads so that they remove the parallel component (d_sae, d_vit) shape Source code in src/saev/nn/modeling.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @torch . no_grad () def remove_parallel_grads ( self ): \"\"\" Update grads so that they remove the parallel component (d_sae, d_vit) shape \"\"\" if not self . cfg . remove_parallel_grads : return parallel_component = einops . einsum ( self . W_dec . grad , self . W_dec . data , \"d_sae d_vit, d_sae d_vit -> d_sae\" , ) self . W_dec . grad -= einops . einsum ( parallel_component , self . W_dec . data , \"d_sae, d_sae d_vit -> d_sae d_vit\" , ) SparseAutoencoderConfig ( d_vit = 1024 , exp_factor = 16 , n_reinit_samples = 1024 * 16 * 32 , remove_parallel_grads = True , normalize_w_dec = True , seed = 0 , activation = Relu ()) dataclass exp_factor = 16 class-attribute instance-attribute Expansion factor for SAE. n_reinit_samples = 1024 * 16 * 32 class-attribute instance-attribute Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean. normalize_w_dec = True class-attribute instance-attribute Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation. remove_parallel_grads = True class-attribute instance-attribute Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic. seed = 0 class-attribute instance-attribute Random seed. dump ( fpath , sae ) Save an SAE checkpoint to disk along with configuration, using the trick from equinox . Parameters: fpath ( str ) \u2013 filepath to save checkpoint to. sae ( SparseAutoencoder ) \u2013 sparse autoencoder checkpoint to save. Source code in src/saev/nn/modeling.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @beartype . beartype def dump ( fpath : str , sae : SparseAutoencoder ): \"\"\" Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation). Arguments: fpath: filepath to save checkpoint to. sae: sparse autoencoder checkpoint to save. \"\"\" # Custom serialization to handle activation object cfg_dict = dataclasses . asdict ( sae . cfg ) # Replace activation dict with custom format activation = sae . cfg . activation cfg_dict [ \"activation\" ] = { \"cls\" : activation . __class__ . __name__ , \"params\" : dataclasses . asdict ( activation ), } header = { \"schema\" : 2 , \"cfg\" : cfg_dict , \"commit\" : helpers . current_git_commit () or \"unknown\" , \"lib\" : __version__ , } os . makedirs ( os . path . dirname ( fpath ), exist_ok = True ) with open ( fpath , \"wb\" ) as fd : header_str = json . dumps ( header ) fd . write (( header_str + \" \\n \" ) . encode ( \"utf-8\" )) torch . save ( sae . state_dict (), fd ) load ( fpath , * , device = 'cpu' ) Loads a sparse autoencoder from disk. Source code in src/saev/nn/modeling.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 @beartype . beartype def load ( fpath : str , * , device = \"cpu\" ) -> SparseAutoencoder : \"\"\" Loads a sparse autoencoder from disk. \"\"\" with open ( fpath , \"rb\" ) as fd : header = json . loads ( fd . readline ()) buffer = io . BytesIO ( fd . read ()) if \"schema\" not in header : # Original, pre-schema format: just raw config parameters # Remove old parameters that no longer exist for keyword in ( \"sparsity_coeff\" , \"ghost_grads\" , \"l1_coeff\" , \"use_ghost_grads\" ): header . pop ( keyword , None ) # Legacy format - create SparseAutoencoderConfig with Relu activation cfg = SparseAutoencoderConfig ( ** header , activation = Relu ()) elif header [ \"schema\" ] == 1 : # Schema version 1: A cautionary tale of poor version management # # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent! # # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.) # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict # # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format. # # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number! cls_name = header . get ( \"cls\" , \"SparseAutoencoderConfig\" ) cfg_dict = header [ \"cfg\" ] if cls_name in [ \"Relu\" , \"TopK\" , \"BatchTopK\" ]: # Format 1A: Old format where cls indicates the activation type activation_cls = globals ()[ cls_name ] if cls_name in [ \"TopK\" , \"BatchTopK\" ]: activation = activation_cls ( top_k = cfg_dict . get ( \"top_k\" , 32 )) else : activation = activation_cls () cfg = SparseAutoencoderConfig ( ** cfg_dict , activation = activation ) else : # Format 1B: Newer format with activation as dict if \"activation\" in cfg_dict : activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) elif header [ \"schema\" ] == 2 : # Schema version 2: cleaner format with activation serialization cfg_dict = header [ \"cfg\" ] activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) else : raise ValueError ( f \"Unknown schema version: { header [ 'schema' ] } \" ) model = SparseAutoencoder ( cfg ) model . load_state_dict ( torch . load ( buffer , weights_only = True , map_location = device )) return model","title":"saev.nn"},{"location":"api/nn/saev.nn/#saev.nn","text":"","title":"nn"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder","text":"Bases: Module Sparse auto-encoder (SAE) using L1 sparsity penalty. Source code in src/saev/nn/modeling.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , cfg : SparseAutoencoderConfig ): super () . __init__ () self . cfg = cfg self . logger = logging . getLogger ( f \"sae(seed= { cfg . seed } )\" ) self . W_enc = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_vit , cfg . d_sae )) ) self . b_enc = torch . nn . Parameter ( torch . zeros ( cfg . d_sae )) self . W_dec = torch . nn . Parameter ( torch . nn . init . kaiming_uniform_ ( torch . empty ( cfg . d_sae , cfg . d_vit )) ) self . b_dec = torch . nn . Parameter ( torch . zeros ( cfg . d_vit )) self . normalize_w_dec () self . activation = get_activation ( cfg . activation )","title":"SparseAutoencoder"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.decode","text":"Decode latent features to reconstructions. Parameters: f_x ( Float [ Tensor , 'batch d_sae'] ) \u2013 Latent features of shape (batch, d_sae) prefixes ( Int64 [ Tensor , ' n_prefixes'] | None , default: None ) \u2013 Optional tensor of prefix lengths for Matryoshka decoding. Returns: Float [ Tensor , 'batch n_prefixes d_model'] \u2013 Matryoshka reconstructions (batch, n_prefixes, d_model). Source code in src/saev/nn/modeling.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def decode ( self , f_x : Float [ Tensor , \"batch d_sae\" ], * , prefixes : Int64 [ Tensor , \" n_prefixes\" ] | None = None , ) -> Float [ Tensor , \"batch n_prefixes d_model\" ]: \"\"\" Decode latent features to reconstructions. Args: f_x: Latent features of shape (batch, d_sae) prefixes: Optional tensor of prefix lengths for Matryoshka decoding. Returns: Matryoshka reconstructions (batch, n_prefixes, d_model). \"\"\" b , d_sae = f_x . shape # Matryoshka cumulative decode device = f_x . device if prefixes is None : prefixes = torch . tensor ([ d_sae ], dtype = torch . int64 ) assert torch . all ( prefixes [ 1 :] > prefixes [: - 1 ]) assert 1 <= int ( prefixes [ 0 ]) and int ( prefixes [ - 1 ]) == d_sae prefixes = prefixes . to ( device ) # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ... block_indices = torch . cat ([ torch . tensor ([ 0 ], dtype = prefixes . dtype , device = device ), prefixes , ]) blocks = list ( zip ( block_indices [: - 1 ], block_indices [ 1 :])) # Compute block outputs block_outputs = [] for i , ( start , end ) in enumerate ( blocks ): # Each block uses its portion of f_x and W_dec block_f_x = f_x [:, start : end ] block_W_dec = self . W_dec [ start : end , :] # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_vit) -> (batch, d_vit) # Note: W_dec is (d_sae, d_vit), so block_W_dec is (block_size, d_vit) block_output = einops . einsum ( block_f_x , block_W_dec , \"... d_sae_block, d_sae_block d_vit -> ... d_vit\" , ) # Add bias only to the first block if i == 0 : block_output = block_output + self . b_dec block_outputs . append ( block_output ) # Cumulative sum to get prefix reconstructions x_hats = torch . cumsum ( torch . stack ( block_outputs , dim =- 2 ), dim =- 2 ) return x_hats","title":"decode"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.forward","text":"Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Parameters: x ( Float [ Tensor , 'batch d_model'] ) \u2013 a batch of ViT activations. Source code in src/saev/nn/modeling.py 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : Float [ Tensor , \"batch d_model\" ] ) -> tuple [ Float [ Tensor , \"batch d_model\" ], Float [ Tensor , \"batch d_sae\" ]]: \"\"\" Given x, calculates the reconstructed x_hat and the intermediate activations f_x. Arguments: x: a batch of ViT activations. \"\"\" f_x = self . encode ( x ) x_hat = self . decode ( f_x ) return x_hat , f_x","title":"forward"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.normalize_w_dec","text":"Set W_dec to unit-norm columns. Source code in src/saev/nn/modeling.py 172 173 174 175 176 177 178 @torch . no_grad () def normalize_w_dec ( self ): \"\"\" Set W_dec to unit-norm columns. \"\"\" if self . cfg . normalize_w_dec : self . W_dec . data /= torch . norm ( self . W_dec . data , dim = 1 , keepdim = True )","title":"normalize_w_dec"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.remove_parallel_grads","text":"Update grads so that they remove the parallel component (d_sae, d_vit) shape Source code in src/saev/nn/modeling.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @torch . no_grad () def remove_parallel_grads ( self ): \"\"\" Update grads so that they remove the parallel component (d_sae, d_vit) shape \"\"\" if not self . cfg . remove_parallel_grads : return parallel_component = einops . einsum ( self . W_dec . grad , self . W_dec . data , \"d_sae d_vit, d_sae d_vit -> d_sae\" , ) self . W_dec . grad -= einops . einsum ( parallel_component , self . W_dec . data , \"d_sae, d_sae d_vit -> d_sae d_vit\" , )","title":"remove_parallel_grads"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig","text":"","title":"SparseAutoencoderConfig"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.exp_factor","text":"Expansion factor for SAE.","title":"exp_factor"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.n_reinit_samples","text":"Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.","title":"n_reinit_samples"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.normalize_w_dec","text":"Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.","title":"normalize_w_dec"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.remove_parallel_grads","text":"Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.","title":"remove_parallel_grads"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.seed","text":"Random seed.","title":"seed"},{"location":"api/nn/saev.nn/#saev.nn.dump","text":"Save an SAE checkpoint to disk along with configuration, using the trick from equinox . Parameters: fpath ( str ) \u2013 filepath to save checkpoint to. sae ( SparseAutoencoder ) \u2013 sparse autoencoder checkpoint to save. Source code in src/saev/nn/modeling.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @beartype . beartype def dump ( fpath : str , sae : SparseAutoencoder ): \"\"\" Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation). Arguments: fpath: filepath to save checkpoint to. sae: sparse autoencoder checkpoint to save. \"\"\" # Custom serialization to handle activation object cfg_dict = dataclasses . asdict ( sae . cfg ) # Replace activation dict with custom format activation = sae . cfg . activation cfg_dict [ \"activation\" ] = { \"cls\" : activation . __class__ . __name__ , \"params\" : dataclasses . asdict ( activation ), } header = { \"schema\" : 2 , \"cfg\" : cfg_dict , \"commit\" : helpers . current_git_commit () or \"unknown\" , \"lib\" : __version__ , } os . makedirs ( os . path . dirname ( fpath ), exist_ok = True ) with open ( fpath , \"wb\" ) as fd : header_str = json . dumps ( header ) fd . write (( header_str + \" \\n \" ) . encode ( \"utf-8\" )) torch . save ( sae . state_dict (), fd )","title":"dump"},{"location":"api/nn/saev.nn/#saev.nn.load","text":"Loads a sparse autoencoder from disk. Source code in src/saev/nn/modeling.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 @beartype . beartype def load ( fpath : str , * , device = \"cpu\" ) -> SparseAutoencoder : \"\"\" Loads a sparse autoencoder from disk. \"\"\" with open ( fpath , \"rb\" ) as fd : header = json . loads ( fd . readline ()) buffer = io . BytesIO ( fd . read ()) if \"schema\" not in header : # Original, pre-schema format: just raw config parameters # Remove old parameters that no longer exist for keyword in ( \"sparsity_coeff\" , \"ghost_grads\" , \"l1_coeff\" , \"use_ghost_grads\" ): header . pop ( keyword , None ) # Legacy format - create SparseAutoencoderConfig with Relu activation cfg = SparseAutoencoderConfig ( ** header , activation = Relu ()) elif header [ \"schema\" ] == 1 : # Schema version 1: A cautionary tale of poor version management # # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent! # # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.) # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict # # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format. # # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number! cls_name = header . get ( \"cls\" , \"SparseAutoencoderConfig\" ) cfg_dict = header [ \"cfg\" ] if cls_name in [ \"Relu\" , \"TopK\" , \"BatchTopK\" ]: # Format 1A: Old format where cls indicates the activation type activation_cls = globals ()[ cls_name ] if cls_name in [ \"TopK\" , \"BatchTopK\" ]: activation = activation_cls ( top_k = cfg_dict . get ( \"top_k\" , 32 )) else : activation = activation_cls () cfg = SparseAutoencoderConfig ( ** cfg_dict , activation = activation ) else : # Format 1B: Newer format with activation as dict if \"activation\" in cfg_dict : activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) elif header [ \"schema\" ] == 2 : # Schema version 2: cleaner format with activation serialization cfg_dict = header [ \"cfg\" ] activation_info = cfg_dict [ \"activation\" ] activation_cls = globals ()[ activation_info [ \"cls\" ]] activation = activation_cls ( ** activation_info [ \"params\" ]) cfg_dict [ \"activation\" ] = activation cfg = SparseAutoencoderConfig ( ** cfg_dict ) else : raise ValueError ( f \"Unknown schema version: { header [ 'schema' ] } \" ) model = SparseAutoencoder ( cfg ) model . load_state_dict ( torch . load ( buffer , weights_only = True , map_location = device )) return model","title":"load"},{"location":"api/utils/saev.utils/","text":"saev.utils","title":"saev.utils"},{"location":"api/utils/saev.utils/#saev.utils","text":"","title":"utils"},{"location":"api/utils/scheduling/","text":"saev.utils.scheduling BatchLimiter ( dataloader , n_samples ) Limits the number of batches to only return n_samples total samples. Source code in src/saev/utils/scheduling.py 87 88 89 90 def __init__ ( self , dataloader : DataLoaderLike , n_samples : int ): self . dataloader = dataloader self . n_samples = n_samples self . batch_size = dataloader . batch_size __getattr__ ( name ) Pass through attribute access to the wrapped dataloader. Source code in src/saev/utils/scheduling.py 95 96 97 98 99 100 101 102 103 104 105 def __getattr__ ( self , name : str ) -> Any : \"\"\"Pass through attribute access to the wrapped dataloader.\"\"\" # __getattr__ is only called when the attribute wasn't found on self # So we delegate to the wrapped dataloader try : return getattr ( self . dataloader , name ) except AttributeError : # Re-raise with more context about where the attribute was not found raise AttributeError ( f \"' { self . __class__ . __name__ } ' object and its wrapped dataloader have no attribute ' { name } '\" ) Warmup ( init , final , n_steps ) Bases: Scheduler Linearly increases from init to final over n_warmup_steps steps. Source code in src/saev/utils/scheduling.py 24 25 26 27 28 def __init__ ( self , init : float , final : float , n_steps : int ): self . final = final self . init = init self . n_steps = n_steps self . _step = 0 WarmupCosine ( init , n_warmup , peak , n_steps , final ) Bases: Scheduler Linearly increases from init to peak over n_warmup steps, then decrease down to final using cosine decay over n_steps - n_warmup. Source code in src/saev/utils/scheduling.py 47 48 49 50 51 52 53 54 55 def __init__ ( self , init : float , n_warmup : int , peak : float , n_steps : int , final : float ): self . init = init self . peak = peak self . final = final self . n_warmup = n_warmup self . n_steps = n_steps self . _step = 0","title":"saev.utils.scheduling"},{"location":"api/utils/scheduling/#saev.utils.scheduling","text":"","title":"scheduling"},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter","text":"Limits the number of batches to only return n_samples total samples. Source code in src/saev/utils/scheduling.py 87 88 89 90 def __init__ ( self , dataloader : DataLoaderLike , n_samples : int ): self . dataloader = dataloader self . n_samples = n_samples self . batch_size = dataloader . batch_size","title":"BatchLimiter"},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter.__getattr__","text":"Pass through attribute access to the wrapped dataloader. Source code in src/saev/utils/scheduling.py 95 96 97 98 99 100 101 102 103 104 105 def __getattr__ ( self , name : str ) -> Any : \"\"\"Pass through attribute access to the wrapped dataloader.\"\"\" # __getattr__ is only called when the attribute wasn't found on self # So we delegate to the wrapped dataloader try : return getattr ( self . dataloader , name ) except AttributeError : # Re-raise with more context about where the attribute was not found raise AttributeError ( f \"' { self . __class__ . __name__ } ' object and its wrapped dataloader have no attribute ' { name } '\" )","title":"__getattr__"},{"location":"api/utils/scheduling/#saev.utils.scheduling.Warmup","text":"Bases: Scheduler Linearly increases from init to final over n_warmup_steps steps. Source code in src/saev/utils/scheduling.py 24 25 26 27 28 def __init__ ( self , init : float , final : float , n_steps : int ): self . final = final self . init = init self . n_steps = n_steps self . _step = 0","title":"Warmup"},{"location":"api/utils/scheduling/#saev.utils.scheduling.WarmupCosine","text":"Bases: Scheduler Linearly increases from init to peak over n_warmup steps, then decrease down to final using cosine decay over n_steps - n_warmup. Source code in src/saev/utils/scheduling.py 47 48 49 50 51 52 53 54 55 def __init__ ( self , init : float , n_warmup : int , peak : float , n_steps : int , final : float ): self . init = init self . peak = peak self . final = final self . n_warmup = n_warmup self . n_steps = n_steps self . _step = 0","title":"WarmupCosine"},{"location":"api/utils/statistics/","text":"saev.utils.statistics PercentileEstimator ( percentile , total , lr = 0.001 , shape = ()) Source code in src/saev/utils/statistics.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , percentile : float | int , total : int , lr : float = 1e-3 , shape : tuple [ int , ... ] = (), ): self . percentile = percentile self . total = total self . lr = lr self . _estimate = torch . zeros ( shape ) self . _step = 0 update ( x ) Update the estimator with a new value. This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions. Parameters: x \u2013 The new value to incorporate into the estimation Source code in src/saev/utils/statistics.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def update ( self , x ): \"\"\" Update the estimator with a new value. This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions. Arguments: x: The new value to incorporate into the estimation \"\"\" self . _step += 1 step_size = self . lr * ( self . total - self . _step ) / self . total # Is a no-op if it's already on the same device. if isinstance ( x , Tensor ): self . _estimate = self . _estimate . to ( x . device ) self . _estimate += step_size * ( torch . sign ( x - self . _estimate ) + 2 * self . percentile / 100 - 1.0 )","title":"saev.utils.statistics"},{"location":"api/utils/statistics/#saev.utils.statistics","text":"","title":"statistics"},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator","text":"Source code in src/saev/utils/statistics.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , percentile : float | int , total : int , lr : float = 1e-3 , shape : tuple [ int , ... ] = (), ): self . percentile = percentile self . total = total self . lr = lr self . _estimate = torch . zeros ( shape ) self . _step = 0","title":"PercentileEstimator"},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator.update","text":"Update the estimator with a new value. This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions. Parameters: x \u2013 The new value to incorporate into the estimation Source code in src/saev/utils/statistics.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def update ( self , x ): \"\"\" Update the estimator with a new value. This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions. Arguments: x: The new value to incorporate into the estimation \"\"\" self . _step += 1 step_size = self . lr * ( self . total - self . _step ) / self . total # Is a no-op if it's already on the same device. if isinstance ( x , Tensor ): self . _estimate = self . _estimate . to ( x . device ) self . _estimate += step_size * ( torch . sign ( x - self . _estimate ) + 2 * self . percentile / 100 - 1.0 )","title":"update"},{"location":"api/utils/wandb/","text":"saev.utils.wandb ParallelWandbRun ( project , cfgs , mode , tags , dir = '.wandb' ) Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387 Source code in src/saev/utils/wandb.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , project : str , cfgs : list [ dict [ str , object ]], mode : str , tags : list [ str ], dir : str = \".wandb\" , ): cfg , * cfgs = cfgs self . project = project self . cfgs = cfgs self . mode = mode self . tags = tags self . dir = dir self . live_run = wandb . init ( project = project , config = cfg , mode = mode , tags = tags , dir = dir ) self . metric_queues : list [ MetricQueue ] = [[] for _ in self . cfgs ]","title":"saev.utils.wandb"},{"location":"api/utils/wandb/#saev.utils.wandb","text":"","title":"wandb"},{"location":"api/utils/wandb/#saev.utils.wandb.ParallelWandbRun","text":"Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387 Source code in src/saev/utils/wandb.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , project : str , cfgs : list [ dict [ str , object ]], mode : str , tags : list [ str ], dir : str = \".wandb\" , ): cfg , * cfgs = cfgs self . project = project self . cfgs = cfgs self . mode = mode self . tags = tags self . dir = dir self . live_run = wandb . init ( project = project , config = cfg , mode = mode , tags = tags , dir = dir ) self . metric_queues : list [ MetricQueue ] = [[] for _ in self . cfgs ]","title":"ParallelWandbRun"},{"location":"developers/contributing/","text":"Contributing Project layout docs/ mkdocs.yml # The configuration file. src/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Contributing"},{"location":"developers/contributing/#contributing","text":"","title":"Contributing"},{"location":"developers/contributing/#project-layout","text":"docs/ mkdocs.yml # The configuration file. src/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"users/guide/","text":"Guide Record ViT activations and save them to disk. Train SAEs on the activations. Visualize the learned features from the trained SAEs. (your job) Propose trends and patterns in the visualized features. (your job, supported by code) Construct datasets to test your hypothesized trends. Confirm/reject hypotheses using probing package. saev helps with steps 1, 2 and 3. `saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X. Record ViT Activations to Disk To save activations to disk, we need to specify: Which model we would like to use Which layers we would like to save. Where on disk and how we would like to save activations. Which images we want to save activations for. The saev.activations module does all of this for us. Run uv run python -m saev activations --help to see all the configuration. In practice, you might run: uv run python -m saev.data \\ --vit-family siglip \\ --vit-ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \\ --d-vit 1024 \\ --n-patches-per-img 256 \\ --no-cls-token \\ --vit-layers 13 15 17 19 21 23 \\ --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \\ --max-patches-per-shard 500_000 \\ --slurm-acct PAS2136 \\ --n-hours 48 \\ --slurm-partition nextgen \\ data:image-folder \\ --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/ Let's break down these arguments. This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49). It will save the second-to-last layer ( --layer -2 ). It will write 2.4M patches per shard, and save shards to a new directory /local/scratch/$USER/cache/saev . .. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a total of 723GB for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger. This script will also save a metadata.json file that will record the relevant metadata for these activations, which will be read by future steps. The activations will be in .bin files, numbered starting from 000000. To add your own models, see the guide to extending in saev.activations . Train SAEs on Activations To train an SAE, we need to specify: Which activations to use as input. SAE architectural stuff. Optimization-related stuff. The saev.training module handles this. Run uv run python -m saev train --help to see all the configuration. Continuing on from our example before, you might want to run something like: uv run python -m saev train \\ --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\ --data.layer -2 \\ --data.patches patches \\ --data.no-scale-mean \\ --data.no-scale-norm \\ --sae.d-vit 768 \\ --lr 5e-4 uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-vit 1024 --data.* flags describe which activations to use. --data.shard-root should point to a directory with *.bin files and the metadata.json file. --data.layer specifies the layer, and --data.patches says that want to train on individual patch activations, rather than the [CLS] token activation. --data.no-scale-mean and --data.no-scale-norm mean not to scale the activation mean or L2 norm. Anthropic's and OpenAI's papers suggest normalizing these factors, but saev still has a bug with this, so I suggest not scaling these factors. --sae.* flags are about the SAE itself. --sae.d-vit is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L. Finally, choose a slightly larger learning rate than the default with --lr 5e-4 . This will train one (1) sparse autoencoder on the data. See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU. Visualize the Learned Features Now that you've trained an SAE, you probably want to look at its learned features. One way to visualize an individual learned feature (f) is by picking out images that maximize the activation of feature (f). Since we train SAEs on patch-level activations, we try to find the top patches for each feature (f). Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values. .. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in saev unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to contrib/ or another location. saev.visuals records these maximally activating images for us. You can see all the options with uv run python -m saev visuals --help . The most important configuration options: The SAE checkpoint that you want to use ( --ckpt ). The ViT activations that you want to use ( --data.* options, should be roughly the same as the options you used to train your SAE, like the same layer, same --data.patches ). The images that produced the ViT activations that you want to use ( images and --images.* options, should be the same as what you used to generate your ViT activtions). Some filtering options on which SAE latents to include ( --log-freq-range , --log-value-range , --include-latents , --n-latents ). Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on. .. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of (f)) are equally interesting. Some latents are dead, some are dense , some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents ( --include-latents ). I recommend using saev.interactive.metrics to figure out good thresholds. So you might run: uv run python -m saev visuals \\ --ckpt checkpoints/abcdefg/sae.pt \\ --dump-to /nfs/$USER/saev/webapp/abcdefg \\ --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\ --data.layer -2 \\ --data.patches patches \\ images:imagenet-dataset This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE. It will cache these best activations to disk, then start saving images to visualize later on. saev.interactive.features is a small web application based on marimo to interactively look at these images. You can run it with uv run marimo edit saev/interactive/features.py . Sweeps tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU. Why Parallel Sweeps SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation. When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself. A single set of ImageNet activations for a vision transformer can require terabytes of storage. Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient. Parallelized Training Architecture To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch: flowchart TD A[Pre-computed ViT Activations] -->|Slow I/O| B[Memory Buffer] B -->|Shared Batch| C[SAE Model 1] B -->|Shared Batch| D[SAE Model 2] B -->|Shared Batch| E[SAE Model 3] B -->|Shared Batch| F[...] import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs'; This approach: Loads each batch of activations once from disk Uses that same batch for multiple SAE models with different hyperparameters Amortizes the slow I/O cost across all models in the sweep Running a Sweep The train command accepts a --sweep parameter that points to a TOML file defining the hyperparameter grid: uv run python -m saev train --sweep configs/my_sweep.toml Here's an example sweep configuration file: [sae] sparsity_coeff = [1e-4, 2e-4, 3e-4] d_vit = 768 exp_factor = [8, 16] [data] scale_mean = true This would train 6 models (3 sparsity coefficients \u00d7 2 expansion factors), each sharing the same data loading operation. Limitations Not all parameters can be swept in parallel. Parameters that affect data loading (like batch_size or dataset configuration) will cause the sweep to split into separate parallel groups. The system automatically handles this division to maximize efficiency. Training Metrics and Visualizations When you train a sweep of SAEs, you probably want to understand which checkpoint is best. saev provides some tools to help with that. First, we offer a tool to look at some basic summary statistics of all your trained checkpoints. saev.interactive.metrics is a marimo notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB. However, there are some pieces of code that need to be changed for you to use it. .. todo:: Explain how to use the saev.interactive.metrics notebook. Need to change your wandb username from samuelstevens to USERNAME from wandb Tag filter Need to run the notebook on the same machine as the original ViT shards and the shards need to be there. Think of better ways to do model and data keys Look at examples run visuals before features How to run visuals faster? explain how these features are visualized","title":"Guide"},{"location":"users/guide/#guide","text":"Record ViT activations and save them to disk. Train SAEs on the activations. Visualize the learned features from the trained SAEs. (your job) Propose trends and patterns in the visualized features. (your job, supported by code) Construct datasets to test your hypothesized trends. Confirm/reject hypotheses using probing package. saev helps with steps 1, 2 and 3. `saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X.","title":"Guide"},{"location":"users/guide/#record-vit-activations-to-disk","text":"To save activations to disk, we need to specify: Which model we would like to use Which layers we would like to save. Where on disk and how we would like to save activations. Which images we want to save activations for. The saev.activations module does all of this for us. Run uv run python -m saev activations --help to see all the configuration. In practice, you might run: uv run python -m saev.data \\ --vit-family siglip \\ --vit-ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \\ --d-vit 1024 \\ --n-patches-per-img 256 \\ --no-cls-token \\ --vit-layers 13 15 17 19 21 23 \\ --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \\ --max-patches-per-shard 500_000 \\ --slurm-acct PAS2136 \\ --n-hours 48 \\ --slurm-partition nextgen \\ data:image-folder \\ --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/ Let's break down these arguments. This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49). It will save the second-to-last layer ( --layer -2 ). It will write 2.4M patches per shard, and save shards to a new directory /local/scratch/$USER/cache/saev . .. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a total of 723GB for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger. This script will also save a metadata.json file that will record the relevant metadata for these activations, which will be read by future steps. The activations will be in .bin files, numbered starting from 000000. To add your own models, see the guide to extending in saev.activations .","title":"Record ViT Activations to Disk"},{"location":"users/guide/#train-saes-on-activations","text":"To train an SAE, we need to specify: Which activations to use as input. SAE architectural stuff. Optimization-related stuff. The saev.training module handles this. Run uv run python -m saev train --help to see all the configuration. Continuing on from our example before, you might want to run something like: uv run python -m saev train \\ --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\ --data.layer -2 \\ --data.patches patches \\ --data.no-scale-mean \\ --data.no-scale-norm \\ --sae.d-vit 768 \\ --lr 5e-4 uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-vit 1024 --data.* flags describe which activations to use. --data.shard-root should point to a directory with *.bin files and the metadata.json file. --data.layer specifies the layer, and --data.patches says that want to train on individual patch activations, rather than the [CLS] token activation. --data.no-scale-mean and --data.no-scale-norm mean not to scale the activation mean or L2 norm. Anthropic's and OpenAI's papers suggest normalizing these factors, but saev still has a bug with this, so I suggest not scaling these factors. --sae.* flags are about the SAE itself. --sae.d-vit is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L. Finally, choose a slightly larger learning rate than the default with --lr 5e-4 . This will train one (1) sparse autoencoder on the data. See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.","title":"Train SAEs on Activations"},{"location":"users/guide/#visualize-the-learned-features","text":"Now that you've trained an SAE, you probably want to look at its learned features. One way to visualize an individual learned feature (f) is by picking out images that maximize the activation of feature (f). Since we train SAEs on patch-level activations, we try to find the top patches for each feature (f). Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values. .. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in saev unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to contrib/ or another location. saev.visuals records these maximally activating images for us. You can see all the options with uv run python -m saev visuals --help . The most important configuration options: The SAE checkpoint that you want to use ( --ckpt ). The ViT activations that you want to use ( --data.* options, should be roughly the same as the options you used to train your SAE, like the same layer, same --data.patches ). The images that produced the ViT activations that you want to use ( images and --images.* options, should be the same as what you used to generate your ViT activtions). Some filtering options on which SAE latents to include ( --log-freq-range , --log-value-range , --include-latents , --n-latents ). Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on. .. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of (f)) are equally interesting. Some latents are dead, some are dense , some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents ( --include-latents ). I recommend using saev.interactive.metrics to figure out good thresholds. So you might run: uv run python -m saev visuals \\ --ckpt checkpoints/abcdefg/sae.pt \\ --dump-to /nfs/$USER/saev/webapp/abcdefg \\ --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\ --data.layer -2 \\ --data.patches patches \\ images:imagenet-dataset This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE. It will cache these best activations to disk, then start saving images to visualize later on. saev.interactive.features is a small web application based on marimo to interactively look at these images. You can run it with uv run marimo edit saev/interactive/features.py .","title":"Visualize the Learned Features"},{"location":"users/guide/#sweeps","text":"tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU.","title":"Sweeps"},{"location":"users/guide/#why-parallel-sweeps","text":"SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation. When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself. A single set of ImageNet activations for a vision transformer can require terabytes of storage. Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient.","title":"Why Parallel Sweeps"},{"location":"users/guide/#parallelized-training-architecture","text":"To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch: flowchart TD A[Pre-computed ViT Activations] -->|Slow I/O| B[Memory Buffer] B -->|Shared Batch| C[SAE Model 1] B -->|Shared Batch| D[SAE Model 2] B -->|Shared Batch| E[SAE Model 3] B -->|Shared Batch| F[...] import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs'; This approach: Loads each batch of activations once from disk Uses that same batch for multiple SAE models with different hyperparameters Amortizes the slow I/O cost across all models in the sweep","title":"Parallelized Training Architecture"},{"location":"users/guide/#running-a-sweep","text":"The train command accepts a --sweep parameter that points to a TOML file defining the hyperparameter grid: uv run python -m saev train --sweep configs/my_sweep.toml Here's an example sweep configuration file: [sae] sparsity_coeff = [1e-4, 2e-4, 3e-4] d_vit = 768 exp_factor = [8, 16] [data] scale_mean = true This would train 6 models (3 sparsity coefficients \u00d7 2 expansion factors), each sharing the same data loading operation.","title":"Running a Sweep"},{"location":"users/guide/#limitations","text":"Not all parameters can be swept in parallel. Parameters that affect data loading (like batch_size or dataset configuration) will cause the sweep to split into separate parallel groups. The system automatically handles this division to maximize efficiency.","title":"Limitations"},{"location":"users/guide/#training-metrics-and-visualizations","text":"When you train a sweep of SAEs, you probably want to understand which checkpoint is best. saev provides some tools to help with that. First, we offer a tool to look at some basic summary statistics of all your trained checkpoints. saev.interactive.metrics is a marimo notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB. However, there are some pieces of code that need to be changed for you to use it. .. todo:: Explain how to use the saev.interactive.metrics notebook. Need to change your wandb username from samuelstevens to USERNAME from wandb Tag filter Need to run the notebook on the same machine as the original ViT shards and the shards need to be there. Think of better ways to do model and data keys Look at examples run visuals before features How to run visuals faster? explain how these features are visualized","title":"Training Metrics and Visualizations"},{"location":"users/inference/","text":"Inference Briefly, you need to: Download a checkpoint. Get the code. Load the checkpoint. Get activations. Details are below. Download a Checkpoint First, download an SAE checkpoint from the Huggingface collection . For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations here . You can use wget if you want: wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt Get the Code The easiest way to do this is to clone the code: git clone https://github.com/OSU-NLP-Group/saev You can also install the package from git if you use uv (not sure about pip or cuda): uv add git+https://github.com/OSU-NLP-Group/saev Or clone it and install it as an editable with pip, lik pip install -e . in your virtual environment. Then you can do things like from saev import ... . .. note:: If you struggle to get saev installed, open an issue on GitHub and I will figure out how to make it easier. Load the Checkpoint import saev.nn sae = saev.nn.load(\"PATH_TO_YOUR_SAE_CKPT.pt\") Now you have a pretrained SAE. Get Activations This is the hardest part. We need to: Pass an image into a ViT Record the dense ViT activations at the same layer that the SAE was trained on. Pass the activations into the SAE to get sparse activations. Do something interesting with the sparse SAE activations. There are examples of this in the demo code: for classification and semantic segmentation . If the permalinks change, you are looking for the get_sae_latents() functions in both files. Below is example code to do it using the saev package. import saev.nn import saev.activations img_transform = saev.activations.make_img_transform(\"clip\", \"ViT-B-16/openai\") vit = saev.activations.make_vit(\"clip\", \"ViT-B-16/openai\") recorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10]) img = Image.open(\"example.jpg\") x = img_transform(img) # Add a batch dimension x = x[None, ...] _, vit_acts = recorded_vit(x) # Select the only layer in the batch and ignore the CLS token. vit_acts = vit_acts[:, 0, 1:, :] x_hat, f_x, loss = sae(vit_acts) Now you have the reconstructed x ( x_hat ) and the sparse representation of all patches in the image ( f_x ). You might select the dimensions with maximal values for each patch and see what other images are maximimally activating. .. todo:: Provide documentation for how get maximally activating images.","title":"Inference"},{"location":"users/inference/#inference","text":"Briefly, you need to: Download a checkpoint. Get the code. Load the checkpoint. Get activations. Details are below.","title":"Inference"},{"location":"users/inference/#download-a-checkpoint","text":"First, download an SAE checkpoint from the Huggingface collection . For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations here . You can use wget if you want: wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt","title":"Download a Checkpoint"},{"location":"users/inference/#get-the-code","text":"The easiest way to do this is to clone the code: git clone https://github.com/OSU-NLP-Group/saev You can also install the package from git if you use uv (not sure about pip or cuda): uv add git+https://github.com/OSU-NLP-Group/saev Or clone it and install it as an editable with pip, lik pip install -e . in your virtual environment. Then you can do things like from saev import ... . .. note:: If you struggle to get saev installed, open an issue on GitHub and I will figure out how to make it easier.","title":"Get the Code"},{"location":"users/inference/#load-the-checkpoint","text":"import saev.nn sae = saev.nn.load(\"PATH_TO_YOUR_SAE_CKPT.pt\") Now you have a pretrained SAE.","title":"Load the Checkpoint"},{"location":"users/inference/#get-activations","text":"This is the hardest part. We need to: Pass an image into a ViT Record the dense ViT activations at the same layer that the SAE was trained on. Pass the activations into the SAE to get sparse activations. Do something interesting with the sparse SAE activations. There are examples of this in the demo code: for classification and semantic segmentation . If the permalinks change, you are looking for the get_sae_latents() functions in both files. Below is example code to do it using the saev package. import saev.nn import saev.activations img_transform = saev.activations.make_img_transform(\"clip\", \"ViT-B-16/openai\") vit = saev.activations.make_vit(\"clip\", \"ViT-B-16/openai\") recorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10]) img = Image.open(\"example.jpg\") x = img_transform(img) # Add a batch dimension x = x[None, ...] _, vit_acts = recorded_vit(x) # Select the only layer in the batch and ignore the CLS token. vit_acts = vit_acts[:, 0, 1:, :] x_hat, f_x, loss = sae(vit_acts) Now you have the reconstructed x ( x_hat ) and the sparse representation of all patches in the image ( f_x ). You might select the dimensions with maximal values for each patch and see what other images are maximimally activating. .. todo:: Provide documentation for how get maximally activating images.","title":"Get Activations"}]}