{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"saev","text":"<p>saev is a framework for training and evaluating Sparse autoencoders (SAEs) for vision transformers (ViTs), implemented in PyTorch.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is supported with uv. saev will likely work with pure pip, conda, etc. but I will not formally support it.</p> <p>Clone this repository, then from the root directory:</p> <pre><code>uv run scripts/launch.py --help\n</code></pre> <p>This will create a virtual environment and display the help for all the provided framework scripts.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Save some activations to disk:</p> <pre><code>uv run scripts/launch.py shards \\\n  --shards-root /$SCRATCH/saev/shards \\\n  --family clip \\\n  --ckpt ViT-B-32/openai \\\n  --d-model 768 \\\n  --layers 11 \\\n  --patches-per-ex 49 \\\n  --batch-size 256 \\\n  data:cifar10\n</code></pre> <p>Read the guide for details.</p>"},{"location":"#why-saev","title":"Why saev?","text":"<p>There are plenty of alternative libraries for SAEs:</p> <ul> <li>Overcomplete, primarily developed by Thomas Fel.</li> </ul> <p>However, saev has some benefits:</p> <ol> <li>saev is more of a framework, rather than a library. The reason for this is that SAEs require lots of activations to train a relatively small neural network; while you can implement it with a simple inference loop, efficient training requires some caching on disk. This means using saev is a little more like Keras or PyTorch Lightning than Huggingface's Transformers or Datasets libraries.</li> <li>saev offers lots of tools for interacting with sparse autoencoders after training, including interactive notebooks and evaluations.</li> <li>saev includes complete code from preprints in the <code>contrib/</code> directory, along with logbooks describing how the authors used and developed saev.</li> </ol>"},{"location":"api/colors/","title":"saev.colors","text":"<p>Utility color palettes used across saev visualizations.</p>"},{"location":"api/configs/","title":"saev.configs","text":""},{"location":"api/configs/#saev.configs.dict_to_dataclass","title":"<code>dict_to_dataclass(data, cls)</code>","text":"<p>Recursively convert a dictionary to a dataclass instance.</p> Source code in <code>src/saev/configs.py</code> <pre><code>@beartype.beartype\ndef dict_to_dataclass(data: dict, cls: type[T]) -&gt; T:\n    \"\"\"Recursively convert a dictionary to a dataclass instance.\"\"\"\n    if not dataclasses.is_dataclass(cls):\n        return data\n\n    field_types = {f.name: f.type for f in dataclasses.fields(cls)}\n    kwargs = {}\n\n    for field_name, field_type in field_types.items():\n        if field_name not in data:\n            continue\n\n        value = data[field_name]\n\n        # Handle Optional types\n        origin = tp.get_origin(field_type)\n        args = tp.get_args(field_type)\n\n        # Handle tuple[str, ...]\n        if origin is tuple and args:\n            kwargs[field_name] = tuple(value) if isinstance(value, list) else value\n        # Handle list[DataclassType]\n        elif origin is list and args and dataclasses.is_dataclass(args[0]):\n            kwargs[field_name] = [dict_to_dataclass(item, args[0]) for item in value]\n        # Handle regular dataclass fields\n        elif dataclasses.is_dataclass(field_type):\n            kwargs[field_name] = dict_to_dataclass(value, field_type)\n        # Handle pathlib.Path\n        elif field_type is pathlib.Path:\n            # Required Path field - always convert\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        elif origin is tp.Union and pathlib.Path in args:\n            # Optional Path field (typing.Union style)\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        elif origin is types.UnionType and pathlib.Path in args:\n            # Optional Path field (Python 3.10+ union style with |)\n            kwargs[field_name] = pathlib.Path(value) if value is not None else value\n        else:\n            kwargs[field_name] = value\n\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/configs/#saev.configs.expand","title":"<code>expand(config)</code>","text":"<p>Expand a nested dict that may contain lists into many dicts.</p> Source code in <code>src/saev/configs.py</code> <pre><code>@beartype.beartype\ndef expand(config: dict[str, object]) -&gt; Iterator[dict[str, object]]:\n    \"\"\"Expand a nested dict that may contain lists into many dicts.\"\"\"\n    yield from _expand_discrete(dict(config))\n</code></pre>"},{"location":"api/configs/#saev.configs.get_non_default_values","title":"<code>get_non_default_values(obj, default_obj)</code>","text":"<p>Recursively find fields that differ from defaults.</p> Source code in <code>src/saev/configs.py</code> <pre><code>@beartype.beartype\ndef get_non_default_values(obj: T, default_obj: T) -&gt; dict:\n    \"\"\"Recursively find fields that differ from defaults.\"\"\"\n    # Check that obj and default_obj are instances of a dataclass.\n    assert dataclasses.is_dataclass(obj) and not isinstance(obj, type)\n    assert dataclasses.is_dataclass(default_obj) and not isinstance(default_obj, type)\n\n    diff = {}\n    for field in dataclasses.fields(obj):\n        obj_value = getattr(obj, field.name)\n        default_value = getattr(default_obj, field.name)\n\n        if obj_value == default_value:\n            continue\n\n        # If both are dataclasses of the same type, recurse to find nested differences\n        if (\n            dataclasses.is_dataclass(obj_value)\n            and dataclasses.is_dataclass(default_value)\n            and type(obj_value) is type(default_value)\n        ):\n            nested_diff = get_non_default_values(obj_value, default_value)\n            if nested_diff:\n                diff[field.name] = nested_diff\n        else:\n            # For non-dataclass fields or different types, just record the value\n            diff[field.name] = obj_value\n\n    return diff\n</code></pre>"},{"location":"api/configs/#saev.configs.load_cfgs","title":"<code>load_cfgs(override, *, default, sweep_dcts)</code>","text":"<p>Load a list of configs from a combination of sources.</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>T</code> <p>Command-line overridden values.</p> required <code>default</code> <code>T</code> <p>The default values for a config.</p> required <code>sweep_dcts</code> <code>list[dict]</code> <p>A list of dictionaries from Python sweep files. Each dictionary may contain list values that will be expanded.</p> required <p>Returns:</p> Type Description <code>tuple[list[T], list[str]]</code> <p>A list of configs and a list of errors.</p> Source code in <code>src/saev/configs.py</code> <pre><code>@beartype.beartype\ndef load_cfgs(\n    override: T, *, default: T, sweep_dcts: list[dict]\n) -&gt; tuple[list[T], list[str]]:\n    \"\"\"\n    Load a list of configs from a combination of sources.\n\n    Args:\n        override: Command-line overridden values.\n        default: The default values for a config.\n        sweep_dcts: A list of dictionaries from Python sweep files. Each dictionary may contain list values that will be expanded.\n\n    Returns:\n        A list of configs and a list of errors.\n    \"\"\"\n    # Check that override and default are instances of a dataclass.\n    assert dataclasses.is_dataclass(override) and not isinstance(override, type)\n    assert dataclasses.is_dataclass(default) and not isinstance(default, type)\n\n    # If there's nothing to sweep, return just the override\n    if not sweep_dcts:\n        return [override], []\n\n    # Find which fields were overridden (differ from default)\n    overridden_fields = get_non_default_values(override, default)\n\n    cfgs: list[T] = []\n    errs: list[str] = []\n\n    d = 0  # Global counter for seed incrementing across all expanded configs\n\n    for sweep_dct in sweep_dcts:\n        # Filter out overridden fields from this sweep dict\n        filtered_dct = _filter_overridden_fields(sweep_dct, overridden_fields)\n\n        # If there's nothing to sweep after filtering, just use override\n        if not filtered_dct:\n            cfgs.append(override)\n            d += 1\n            continue\n\n        # Apply the sweep dict to create a config\n        try:\n            updates = _recursive_dataclass_update(override, filtered_dct, override, d)\n\n            if hasattr(override, \"seed\") and \"seed\" not in updates:\n                updates[\"seed\"] = getattr(override, \"seed\", 0) + d\n\n            cfgs.append(dataclasses.replace(override, **updates))\n            d += 1\n        except Exception as err:\n            errs.append(str(err))\n            d += 1\n\n    return cfgs, errs\n</code></pre>"},{"location":"api/configs/#saev.configs.load_sweep","title":"<code>load_sweep(sweep_fpath)</code>","text":"<p>Load a sweep file and return the list of config dicts.</p> <p>Parameters:</p> Name Type Description Default <code>sweep_fpath</code> <code>Path</code> <p>Path to a Python file with a <code>make_cfgs()</code> function.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of config dictionaries from <code>make_cfgs()</code>. Returns empty list if any error occurs.</p> Source code in <code>src/saev/configs.py</code> <pre><code>@beartype.beartype\ndef load_sweep(sweep_fpath: pathlib.Path) -&gt; list[dict]:\n    \"\"\"\n    Load a sweep file and return the list of config dicts.\n\n    Args:\n        sweep_fpath: Path to a Python file with a `make_cfgs()` function.\n\n    Returns:\n        List of config dictionaries from `make_cfgs()`. Returns empty list if any error occurs.\n    \"\"\"\n    try:\n        namespace = {}\n        exec(sweep_fpath.read_text(), namespace)\n        result = namespace[\"make_cfgs\"]()\n        if not isinstance(result, list):\n            logger.warning(\n                f\"make_cfgs() in {sweep_fpath} returned {type(result).__name__}, expected list\"\n            )\n            return []\n        return result\n    except Exception as err:\n        logger.warning(f\"Failed to load sweep from {sweep_fpath}: {err}\")\n        return []\n</code></pre>"},{"location":"api/disk/","title":"saev.disk","text":"<p>Helpers for sticking with the layout described in disk-layout.md.</p>"},{"location":"api/disk/#saev.disk.Run","title":"<code>Run(root)</code>","text":"<p>Represents an SAE training run and some associated data.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path</code> <p>Root directory, should be $SAEV_NFS/saev/runs/. Assumes the run already exists and validates the structure. Use <code>Run.new()</code> to create a new run. required Source code in <code>src/saev/disk.py</code> <pre><code>def __init__(self, root: pathlib.Path):\n    self.root = root\n\n    if not self.root.exists():\n        raise FileNotFoundError(\n            f\"Run directory does not exist: {self.root}. Use Run.new() to create a new run.\"\n        )\n    if not (self.root / \"checkpoint\").exists():\n        raise FileNotFoundError(\n            f\"Checkpoint directory does not exist: {self.root / 'checkpoint'}. Use Run.new() to create a new run.\"\n        )\n    if not (self.root / \"links\").exists():\n        raise FileNotFoundError(\n            f\"Links directory does not exist: {self.root / 'links'}. Use Run.new() to create a new run.\"\n        )\n    if not (self.root / \"inference\").exists():\n        raise FileNotFoundError(\n            f\"Inference directory does not exist: {self.root / 'inference'}. Use Run.new() to create a new run.\"\n        )\n</code></pre>"},{"location":"api/disk/#saev.disk.Run.ckpt","title":"<code>ckpt</code>  <code>property</code>","text":"<p>Path to the sae.pt checkpoint.</p>"},{"location":"api/disk/#saev.disk.Run.config","title":"<code>config</code>  <code>property</code>","text":"<p>The training run config. Not a train.Config object because we don't want to import from train.py.</p>"},{"location":"api/disk/#saev.disk.Run.inference","title":"<code>inference</code>  <code>property</code>","text":"<p>Path to the inference/ directory.</p>"},{"location":"api/disk/#saev.disk.Run.run_id","title":"<code>run_id</code>  <code>property</code>","text":"<p>The run ID, created by wandb.</p>"},{"location":"api/disk/#saev.disk.Run.train_dataset","title":"<code>train_dataset</code>  <code>property</code>","text":"<p>Path to dataset root.</p>"},{"location":"api/disk/#saev.disk.Run.train_shards","title":"<code>train_shards</code>  <code>property</code>","text":"<p>Path to shard root with metadata.json and acts*.bin files.</p>"},{"location":"api/disk/#saev.disk.Run.val_dataset","title":"<code>val_dataset</code>  <code>property</code>","text":"<p>Path to dataset root.</p>"},{"location":"api/disk/#saev.disk.Run.val_shards","title":"<code>val_shards</code>  <code>property</code>","text":"<p>Path to shard root with metadata.json and acts*.bin files.</p>"},{"location":"api/disk/#saev.disk.Run.new","title":"<code>new(run_id, *, train_shards_dir, train_dataset, val_shards_dir, val_dataset, runs_root)</code>  <code>classmethod</code>","text":"<p>Create a new run with directory structure and symlinks.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The run ID (typically from wandb).</p> required <code>train_shards_dir</code> <code>Path</code> <p>Absolute path to the train shards directory (typically $SAEV_SCRATCH/saev/shards/). required <code>train_dataset</code> <code>Path</code> <p>Absolute path to the train dataset directory.</p> required <code>val_shards_dir</code> <code>Path</code> <p>Absolute path to the val shards directory (typically $SAEV_SCRATCH/saev/shards/). required <code>val_dataset</code> <code>Path</code> <p>Absolute path to the val dataset directory.</p> required <code>runs_root</code> <code>Path</code> <p>Root directory for runs (typically $SAEV_NFS/saev/runs).</p> required <p>Returns:</p> Type Description <code>Run</code> <p>A new Run instance with all directories and symlinks created.</p> Source code in <code>src/saev/disk.py</code> <pre><code>@classmethod\ndef new(\n    cls,\n    run_id: str,\n    *,\n    train_shards_dir: pathlib.Path,\n    train_dataset: pathlib.Path,\n    val_shards_dir: pathlib.Path,\n    val_dataset: pathlib.Path,\n    runs_root: pathlib.Path,\n) -&gt; \"Run\":\n    \"\"\"\n    Create a new run with directory structure and symlinks.\n\n    Args:\n        run_id: The run ID (typically from wandb).\n        train_shards_dir: Absolute path to the train shards directory (typically $SAEV_SCRATCH/saev/shards/&lt;shard_hash&gt;).\n        train_dataset: Absolute path to the train dataset directory.\n        val_shards_dir: Absolute path to the val shards directory (typically $SAEV_SCRATCH/saev/shards/&lt;shard_hash&gt;).\n        val_dataset: Absolute path to the val dataset directory.\n        runs_root: Root directory for runs (typically $SAEV_NFS/saev/runs).\n\n    Returns:\n        A new Run instance with all directories and symlinks created.\n    \"\"\"\n    root = runs_root / run_id\n    root.mkdir(parents=True)\n    (root / \"checkpoint\").mkdir()\n    (root / \"links\").mkdir()\n    (root / \"inference\").mkdir()\n\n    (root / \"links\" / \"train-shards\").symlink_to(train_shards_dir)\n    (root / \"links\" / \"train-dataset\").symlink_to(train_dataset)\n    (root / \"links\" / \"val-shards\").symlink_to(val_shards_dir)\n    (root / \"links\" / \"val-dataset\").symlink_to(val_dataset)\n\n    return cls(root)\n</code></pre>"},{"location":"api/disk/#saev.disk.is_run_root","title":"<code>is_run_root(path)</code>","text":"<p>Check if <code>path</code> is a valid run root directory.</p> <p>A valid run root ends with <code>saev/runs</code> and exists as a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory ending in saev/runs.</p> Source code in <code>src/saev/disk.py</code> <pre><code>@beartype.beartype\ndef is_run_root(path: pathlib.Path) -&gt; bool:\n    \"\"\"\n    Check if `path` is a valid run root directory.\n\n    A valid run root ends with `saev/runs` and exists as a directory.\n\n    Args:\n        path: Path to check.\n\n    Returns:\n        True if path is a directory ending in saev/runs.\n    \"\"\"\n    return path.is_dir() and path.parts[-2:] == (\"saev\", \"runs\")\n</code></pre>"},{"location":"api/disk/#saev.disk.is_shards_dir","title":"<code>is_shards_dir(path)</code>","text":"<p>Check if <code>path</code> is a specific shards directory.</p> <p>A valid shards directory ends with <code>saev/shards/&lt;hash&gt;</code> for any hash value, exists as a directory, and contains the required files (metadata.json, shards.json, labels.bin).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory ending in saev/shards/ with required files. Source code in <code>src/saev/disk.py</code> <pre><code>@beartype.beartype\ndef is_shards_dir(path: pathlib.Path) -&gt; bool:\n    \"\"\"\n    Check if `path` is a specific shards directory.\n\n    A valid shards directory ends with `saev/shards/&lt;hash&gt;` for any hash value, exists as a directory, and contains the required files (metadata.json, shards.json, labels.bin).\n\n    Args:\n        path: Path to check.\n\n    Returns:\n        True if path is a directory ending in saev/shards/&lt;hash&gt; with required files.\n    \"\"\"\n    if not path.is_dir():\n        return False\n\n    if len(path.parts) &lt; 3 or path.parts[-3:-1] != (\"saev\", \"shards\"):\n        return False\n\n    return True\n</code></pre>"},{"location":"api/disk/#saev.disk.is_shards_root","title":"<code>is_shards_root(path)</code>","text":"<p>Check if <code>path</code> is a valid shards root directory.</p> <p>A valid shards root ends with <code>saev/shards</code> and exists as a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory ending in saev/shards.</p> Source code in <code>src/saev/disk.py</code> <pre><code>@beartype.beartype\ndef is_shards_root(path: pathlib.Path) -&gt; bool:\n    \"\"\"\n    Check if `path` is a valid shards root directory.\n\n    A valid shards root ends with `saev/shards` and exists as a directory.\n\n    Args:\n        path: Path to check.\n\n    Returns:\n        True if path is a directory ending in saev/shards.\n    \"\"\"\n    return path.is_dir() and path.parts[-2:] == (\"saev\", \"shards\")\n</code></pre>"},{"location":"api/helpers/","title":"saev.helpers","text":""},{"location":"api/helpers/#saev.helpers.RemovedFeatureError","title":"<code>RemovedFeatureError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Feature existed before but is no longer supported.</p>"},{"location":"api/helpers/#saev.helpers.batched_idx","title":"<code>batched_idx(total_size, batch_size)</code>","text":"<p>Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>total_size</code> <code>int</code> <p>total number of examples</p> required <code>batch_size</code> <code>int</code> <p>maximum distance between the generated indices.</p> required <p>Returns:</p> Type Description <p>A generator of (int, int) tuples that can slice up a list or a tensor.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __init__(self, total_size: int, batch_size: int):\n    self.total_size = total_size\n    self.batch_size = batch_size\n</code></pre>"},{"location":"api/helpers/#saev.helpers.batched_idx.__iter__","title":"<code>__iter__()</code>","text":"<p>Yield (start, end) index pairs for batching.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __iter__(self) -&gt; Iterator[tuple[int, int]]:\n    \"\"\"Yield (start, end) index pairs for batching.\"\"\"\n    for start in range(0, self.total_size, self.batch_size):\n        stop = min(start + self.batch_size, self.total_size)\n        yield start, stop\n</code></pre>"},{"location":"api/helpers/#saev.helpers.batched_idx.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of batches.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of batches.\"\"\"\n    return (self.total_size + self.batch_size - 1) // self.batch_size\n</code></pre>"},{"location":"api/helpers/#saev.helpers.progress","title":"<code>progress(it, *, every=10, desc='progress', total=0)</code>","text":"<p>Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.</p> <p>Parameters:</p> Name Type Description Default <code>it</code> <code>Iterable</code> <p>Iterable to wrap.</p> required <code>every</code> <code>int</code> <p>How many iterations between logging progress.</p> <code>10</code> <code>desc</code> <code>str</code> <p>What to name the logger.</p> <code>'progress'</code> <code>total</code> <code>int</code> <p>If non-zero, how long the iterable is.</p> <code>0</code> Source code in <code>src/saev/helpers.py</code> <pre><code>def __init__(\n    self, it: Iterable, *, every: int = 10, desc: str = \"progress\", total: int = 0\n):\n    self.it = it\n    self.every = max(every, 1)\n    self.logger = logging.getLogger(desc)\n    self.total = total\n</code></pre>"},{"location":"api/helpers/#saev.helpers.current_git_commit","title":"<code>current_git_commit()</code>","text":"<p>Best-effort short SHA of the repo containing this file.</p> <p>Returns <code>None</code> when * <code>git</code> executable is missing, * we\u2019re not inside a git repo (e.g. installed wheel), * or any git call errors out.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef current_git_commit() -&gt; str | None:\n    \"\"\"\n    Best-effort short SHA of the repo containing *this* file.\n\n    Returns `None` when\n    * `git` executable is missing,\n    * we\u2019re not inside a git repo (e.g. installed wheel),\n    * or any git call errors out.\n    \"\"\"\n    try:\n        # Walk up until we either hit a .git dir or the FS root\n        here = pathlib.Path(__file__).resolve()\n        for parent in (here, *here.parents):\n            if (parent / \".git\").exists():\n                break\n        else:  # no .git found\n            return None\n\n        result = subprocess.run(\n            [\"git\", \"-C\", str(parent), \"rev-parse\", \"--short\", \"HEAD\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.DEVNULL,\n            text=True,\n            check=True,\n        )\n        return result.stdout.strip() or None\n    except (FileNotFoundError, subprocess.CalledProcessError):\n        return None\n</code></pre>"},{"location":"api/helpers/#saev.helpers.flattened","title":"<code>flattened(dct, *, sep='.')</code>","text":"<p>Flatten a potentially nested dict to a single-level dict with <code>.</code>-separated keys.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef flattened(\n    dct: dict[str, object], *, sep: str = \".\"\n) -&gt; dict[str, str | int | float | bool | None]:\n    \"\"\"\n    Flatten a potentially nested dict to a single-level dict with `.`-separated keys.\n    \"\"\"\n    new = {}\n    for key, value in dct.items():\n        if isinstance(value, dict):\n            for nested_key, nested_value in flattened(value).items():\n                new[key + \".\" + nested_key] = nested_value\n            continue\n\n        new[key] = value\n\n    return new\n</code></pre>"},{"location":"api/helpers/#saev.helpers.fssafe","title":"<code>fssafe(s)</code>","text":"<p>Convert a string to be filesystem-safe by replacing special characters.</p> <p>This is particularly useful for checkpoint names that contain characters like 'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like 'hf-hub_timm_ViT-L-16-SigLIP2-256'.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>String to make filesystem-safe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Filesystem-safe version of the string.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef fssafe(s: str) -&gt; str:\n    \"\"\"\n    Convert a string to be filesystem-safe by replacing special characters.\n\n    This is particularly useful for checkpoint names that contain characters like\n    'hf-hub:timm/ViT-L-16-SigLIP2-256' which need to be converted to something like\n    'hf-hub_timm_ViT-L-16-SigLIP2-256'.\n\n    Args:\n        s: String to make filesystem-safe.\n\n    Returns:\n        Filesystem-safe version of the string.\n    \"\"\"\n    # Replace common problematic characters with underscores\n    replacements = {\n        \"/\": \"_\",\n        \"\\\\\": \"_\",\n        \":\": \"_\",\n        \"*\": \"_\",\n        \"?\": \"_\",\n        '\"': \"_\",\n        \"&lt;\": \"_\",\n        \"&gt;\": \"_\",\n        \"|\": \"_\",\n        \" \": \"_\",\n    }\n    for old, new in replacements.items():\n        s = s.replace(old, new)\n    # Remove any remaining non-alphanumeric characters except - _ .\n    return \"\".join(c if c.isalnum() or c in \"-_.\" else \"_\" for c in s)\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_cache_dir","title":"<code>get_cache_dir()</code>","text":"<p>Get cache directory from environment variables, defaulting to the current working directory (.)</p> <p>Returns:</p> Type Description <code>str</code> <p>A path to a cache directory (might not exist yet).</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_cache_dir() -&gt; str:\n    \"\"\"\n    Get cache directory from environment variables, defaulting to the current working directory (.)\n\n    Returns:\n        A path to a cache directory (might not exist yet).\n    \"\"\"\n    cache_dir = \"\"\n    for var in (\"SAEV_CACHE\", \"HF_HOME\", \"HF_HUB_CACHE\"):\n        cache_dir = cache_dir or os.environ.get(var, \"\")\n    return cache_dir or \".\"\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_job_count","title":"<code>get_slurm_job_count()</code>","text":"<p>Get the current number of jobs in the queue for the current user.</p> <p>Uses squeue's -r flag to properly count job array elements individually. For example, a job array 12345_[0-99] will be counted as 100 jobs.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_job_count() -&gt; int:\n    \"\"\"\n    Get the current number of jobs in the queue for the current user.\n\n    Uses squeue's -r flag to properly count job array elements individually.\n    For example, a job array 12345_[0-99] will be counted as 100 jobs.\n    \"\"\"\n    try:\n        # Use -r to display each array element on its own line\n        result = subprocess.run(\n            [\"squeue\", \"--me\", \"-h\", \"-r\"], capture_output=True, text=True, check=True\n        )\n\n        # Count non-empty lines\n        lines = result.stdout.strip().split(\"\\n\")\n        return len([line for line in lines if line.strip()])\n\n    except (subprocess.SubprocessError, FileNotFoundError):\n        # If we can't check, assume no jobs\n        return 0\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_max_array_size","title":"<code>get_slurm_max_array_size()</code>","text":"<p>Get the MaxArraySize configuration from the current Slurm cluster.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_max_array_size() -&gt; int:\n    \"\"\"\n    Get the MaxArraySize configuration from the current Slurm cluster.\n\n    Returns:\n        int: The maximum array size allowed on the cluster. Returns 1000 as fallback if unable to determine.\n    \"\"\"\n    logger = logging.getLogger(\"helpers.slurm\")\n    try:\n        # Run scontrol command to get config information\n        result = subprocess.run(\n            [\"scontrol\", \"show\", \"config\"], capture_output=True, text=True, check=True\n        )\n\n        # Search for MaxArraySize in the output\n        match = re.search(r\"MaxArraySize\\s*=\\s*(\\d+)\", result.stdout)\n        if match:\n            max_array_size = int(match.group(1))\n            logger.info(\"Detected MaxArraySize = %d\", max_array_size)\n            return max_array_size\n        else:\n            logger.warning(\n                \"Could not find MaxArraySize in scontrol output, using default of 1000\"\n            )\n            return 1000\n\n    except subprocess.SubprocessError as e:\n        logger.error(\"Error running scontrol: %s\", e)\n        return 1000  # Safe default\n    except ValueError as e:\n        logger.error(\"Error parsing MaxArraySize: %s\", e)\n        return 1000  # Safe default\n    except FileNotFoundError:\n        logger.warning(\n            \"scontrol command not found. Assuming not in Slurm environment. Returning default MaxArraySize=1000.\"\n        )\n        return 1000\n</code></pre>"},{"location":"api/helpers/#saev.helpers.get_slurm_max_submit_jobs","title":"<code>get_slurm_max_submit_jobs()</code>","text":"<p>Get the MaxSubmitJobs limit from the current user's QOS.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The maximum number of jobs that can be submitted at once. Returns 1000 as fallback.</p> Source code in <code>src/saev/helpers.py</code> <pre><code>@beartype.beartype\ndef get_slurm_max_submit_jobs() -&gt; int:\n    \"\"\"\n    Get the MaxSubmitJobs limit from the current user's QOS.\n\n    Returns:\n        int: The maximum number of jobs that can be submitted at once. Returns 1000 as fallback.\n    \"\"\"\n    logger = logging.getLogger(\"helpers.slurm\")\n    try:\n        # First, try to get the QOS from a recent job\n        result = subprocess.run(\n            [\"scontrol\", \"show\", \"job\", \"-o\"],\n            capture_output=True,\n            text=True,\n            check=False,\n        )\n\n        qos_name = None\n        if result.returncode == 0 and result.stdout:\n            # Extract QOS from job info\n            match = re.search(r\"QOS=(\\S+)\", result.stdout)\n            if match:\n                qos_name = match.group(1)\n\n        if not qos_name:\n            # If no jobs, try to get default QOS from association\n            # This is less reliable but better than nothing\n            logger.warning(\"No active jobs to determine QOS, using default of 1000\")\n            return 1000\n\n        # Get the MaxSubmitJobs for this QOS\n        result = subprocess.run(\n            [\"sacctmgr\", \"show\", \"qos\", qos_name, \"format=maxsubmitjobs\", \"-n\", \"-P\"],\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n\n        max_submit = result.stdout.strip()\n        if max_submit and max_submit.isdigit():\n            limit = int(max_submit)\n            logger.info(\"Detected MaxSubmitJobs = %d for QOS %s\", limit, qos_name)\n            return limit\n        else:\n            logger.warning(\"Could not parse MaxSubmitJobs, using default of 1000\")\n            return 1000\n\n    except subprocess.SubprocessError as e:\n        logger.error(\"Error getting MaxSubmitJobs: %s\", e)\n        return 1000\n    except (ValueError, FileNotFoundError) as e:\n        logger.error(\"Error: %s\", e)\n        return 1000\n</code></pre>"},{"location":"api/saev/","title":"saev","text":"<p>saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.</p>"},{"location":"api/summary/","title":"Summary","text":"<ul> <li>saev</li> <li>saev.colors</li> <li>saev.configs</li> <li>saev.data</li> <li>saev.data.buffers</li> <li>saev.data.clip</li> <li>saev.data.datasets</li> <li>saev.data.dinov2</li> <li>saev.data.dinov3</li> <li>saev.data.fake_clip</li> <li>saev.data.indexed</li> <li>saev.data.models</li> <li>saev.data.ordered</li> <li>saev.data.shards</li> <li>saev.data.shuffled</li> <li>saev.data.siglip</li> <li>saev.data.transforms</li> <li>saev.disk</li> <li>saev.helpers</li> <li>saev.nn</li> <li>saev.nn.modeling</li> <li>saev.nn.objectives</li> <li>saev.utils</li> <li>saev.utils.scheduling</li> <li>saev.utils.statistics</li> <li>saev.utils.wandb</li> <li>saev.viz</li> </ul>"},{"location":"api/viz/","title":"saev.viz","text":""},{"location":"api/data/buffers/","title":"saev.data.buffers","text":""},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer","title":"<code>ReservoirBuffer(capacity, shape, *, dtype=torch.float32, meta_shape=(2,), meta_dtype=torch.int32, seed=0, collate_fn=None)</code>","text":"<p>Pool of (tensor, meta) pairs. Multiple producers call put(batch_x, batch_meta). Multiple consumers call get(batch_size) -&gt; (x, meta). Random order, each sample delivered once, blocking semantics.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def __init__(\n    self,\n    capacity: int,\n    shape: tuple[int, ...],\n    *,\n    dtype: torch.dtype = torch.float32,\n    meta_shape: tuple[int, ...] = (2,),\n    meta_dtype: torch.dtype = torch.int32,\n    seed: int = 0,\n    collate_fn: collections.abc.Callable | None = None,\n):\n    self.capacity = capacity\n    self._empty = 123456789\n\n    self.data = torch.full((capacity, *shape), self._empty, dtype=dtype)\n    self.data.share_memory_()\n\n    self.meta = torch.full((capacity, *meta_shape), self._empty, dtype=meta_dtype)\n    self.meta.share_memory_()\n\n    self.ctx = mp.get_context()\n\n    self.size = self.ctx.Value(\"L\", 0)  # current live items\n    self.lock = self.ctx.Lock()  # guards size+swap\n    self.free = self.ctx.Semaphore(capacity)\n    self.full = self.ctx.Semaphore(0)\n    # Each process has its own RNG.\n    self.rng = np.random.default_rng(seed)\n\n    self.collate_fn = collate_fn\n\n    self.logger = logging.getLogger(f\"reservoir({os.getpid()})\")\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.close","title":"<code>close()</code>","text":"<p>Release the shared-memory backing store (call once in the parent).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\"\n    try:\n        self.data.untyped_storage()._free_shared_mem()\n    except (AttributeError, FileNotFoundError):\n        pass  # already freed or never allocated\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.fill","title":"<code>fill()</code>","text":"<p>Approximate proportion of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def fill(self) -&gt; float:\n    \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\"\n    return self.qsize() / self.capacity\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.ReservoirBuffer.qsize","title":"<code>qsize()</code>","text":"<p>Approximate number of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def qsize(self) -&gt; int:\n    \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\"\n    return self.size.value\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer","title":"<code>RingBuffer(slots, shape, dtype)</code>","text":"<p>Fixed-capacity, multiple-producer / multiple-consumer queue backed by a shared-memory tensor.</p>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer--parameters","title":"Parameters","text":"<p>slots  : int           capacity in number of items (tensor rows) shape  : tuple[int]    shape of one item, e.g. (batch, dim) dtype  : torch.dtype   tensor dtype</p> <p>put(tensor)  : blocks if full get() -&gt; tensor  : blocks if empty qsize() -&gt; int        advisory size (approximate) close()               frees shared storage (call in the main process)</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def __init__(self, slots: int, shape: tuple[int, ...], dtype: torch.dtype):\n    assert slots &gt; 0, \"slots must be positive\"\n    self.slots = slots\n    # 123456789 -&gt; Should make you very worried.\n    self.buf = torch.full((slots, *shape), 123456789, dtype=dtype)\n    self.buf.share_memory_()\n\n    ctx = mp.get_context()  # obeys the global start method (\"spawn\")\n\n    # shared, lock-free counters\n    self.head = ctx.Value(\"L\", 0, lock=False)  # next free slot\n    self.tail = ctx.Value(\"L\", 0, lock=False)  # next occupied slot\n\n    # semaphores for blocking semantics\n    self.free = ctx.Semaphore(slots)  # initially all slots free\n    self.fill = ctx.Semaphore(0)  # no filled slots yet\n\n    # one mutex for pointer updates\n    self.mutex = ctx.Lock()\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.close","title":"<code>close()</code>","text":"<p>Release the shared-memory backing store (call once in the parent).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release the shared-memory backing store (call once in the parent).\"\"\"\n    try:\n        self.buf.untyped_storage()._free_shared_mem()\n    except (AttributeError, FileNotFoundError):\n        pass  # already freed or never allocated\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.fill","title":"<code>fill()</code>","text":"<p>Approximate proportion of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def fill(self) -&gt; float:\n    \"\"\"Approximate proportion of filled slots (race-safe enough for tests).\"\"\"\n    return self.qsize() / self.capacity\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.get","title":"<code>get()</code>","text":"<p>Return a view of the next item; blocks if the queue is empty.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def get(self) -&gt; torch.Tensor:\n    \"\"\"Return a view of the next item; blocks if the queue is empty.\"\"\"\n    self.fill.acquire()  # wait for data\n    with self.mutex:  # exclusive update of tail\n        idx = self.tail.value % self.slots\n        out = self.buf[idx].clone()\n        self.tail.value += 1\n    self.free.release()  # signal one more free slot\n    return out\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.put","title":"<code>put(tensor)</code>","text":"<p>Copy <code>tensor</code> into the next free slot; blocks if the queue is full.</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def put(self, tensor: torch.Tensor) -&gt; None:\n    \"\"\"Copy `tensor` into the next free slot; blocks if the queue is full.\"\"\"\n    if tensor.shape != self.buf.shape[1:] or tensor.dtype != self.buf.dtype:\n        raise ValueError(\"tensor shape / dtype mismatch\")\n\n    self.free.acquire()  # wait for a free slot\n    with self.mutex:  # exclusive update of head\n        idx = self.head.value % self.slots\n        self.buf[idx].copy_(tensor)\n        self.head.value += 1\n    self.fill.release()  # signal there is data\n</code></pre>"},{"location":"api/data/buffers/#saev.data.buffers.RingBuffer.qsize","title":"<code>qsize()</code>","text":"<p>Approximate number of filled slots (race-safe enough for tests).</p> Source code in <code>src/saev/data/buffers.py</code> <pre><code>def qsize(self) -&gt; int:\n    \"\"\"Approximate number of filled slots (race-safe enough for tests).\"\"\"\n    return (self.head.value - self.tail.value) % (1 &lt;&lt; 64)\n</code></pre>"},{"location":"api/data/clip/","title":"saev.data.clip","text":""},{"location":"api/data/clip/#saev.data.clip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>VisionTransformer</code>, <code>Module</code></p> Source code in <code>src/saev/data/clip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    import open_clip\n\n    from .. import helpers\n\n    if ckpt.startswith(\"hf-hub:\"):\n        clip, _ = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n        _, ckpt = ckpt.split(\"hf-hub:\")\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        clip, _ = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    self._ckpt = ckpt\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model.eval()\n\n    assert not isinstance(self.model, open_clip.timm_model.TimmModel)\n</code></pre>"},{"location":"api/data/clip/#saev.data.clip.Vit.patch_size","title":"<code>patch_size</code>  <code>property</code>","text":"<p>Get patch size for CLIP models.</p>"},{"location":"api/data/clip/#saev.data.clip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/clip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    import open_clip\n\n    from .. import helpers\n\n    if ckpt.startswith(\"hf-hub:\"):\n        _, img_transform = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        _, img_transform = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/datasets/","title":"saev.data.datasets","text":""},{"location":"api/data/datasets/#saev.data.datasets.Cifar10","title":"<code>Cifar10(name='uoft-cs/cifar10', split='train')</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>Configuration for HuggingFace CIFAR-10.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Cifar10.n_examples","title":"<code>n_examples</code>  <code>property</code>","text":"<p>Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Cifar10.name","title":"<code>name = 'uoft-cs/cifar10'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset name on HuggingFace. Don't need to change this.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Cifar10.root","title":"<code>root</code>  <code>property</code>","text":"<p>Root directory path for the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Cifar10.split","title":"<code>split = 'train'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset split. Can be 'train' or 'test'.</p>"},{"location":"api/data/datasets/#saev.data.datasets.DatasetConfig","title":"<code>DatasetConfig</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset configurations.</p>"},{"location":"api/data/datasets/#saev.data.datasets.DatasetConfig.n_examples","title":"<code>n_examples</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of examples in the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.DatasetConfig.root","title":"<code>root</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Root directory path for the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImg","title":"<code>FakeImg(n_examples=10)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImg.root","title":"<code>root</code>  <code>property</code>","text":"<p>Root directory path for the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg","title":"<code>FakeImgSeg(n_examples=10, content_tokens_per_example=16, n_classes=3, bg_label=0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>Tiny synthetic segmentation dataset for tests.</p> <p>Generates dummy RGB images and pixel-level segmentation masks, mimicking the behavior of real segmentation datasets like ImgSegFolder.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg.bg_label","title":"<code>bg_label = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which class index is considered background.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg.content_tokens_per_example","title":"<code>content_tokens_per_example = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of content tokens per example.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg.n_classes","title":"<code>n_classes = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of segmentation classes.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg.n_examples","title":"<code>n_examples = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of examples.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSeg.root","title":"<code>root</code>  <code>property</code>","text":"<p>Root directory path for the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.FakeImgSegDataset","title":"<code>FakeImgSegDataset(cfg, *, img_transform=None, seg_transform=None, sample_transform=None)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Synthetic segmentation dataset providing pixel-level segmentation masks.</p> <p>Mimics ImgSegFolderDataset by providing: - image: a dummy RGB PIL image - segmentation: a PIL image with pixel-level class labels - index, target, label</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __init__(\n    self,\n    cfg: FakeImgSeg,\n    *,\n    img_transform=None,\n    seg_transform=None,\n    sample_transform=None,\n):\n    self.cfg = cfg\n    self.img_transform = img_transform\n    self.seg_transform = seg_transform\n    self.sample_transform = sample_transform\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet","title":"<code>Imagenet(name='ILSVRC/imagenet-1k', split='train')</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>Configuration for HuggingFace Imagenet.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.n_examples","title":"<code>n_examples</code>  <code>property</code>","text":"<p>Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.name","title":"<code>name = 'ILSVRC/imagenet-1k'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset name on HuggingFace. Don't need to change this..</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.root","title":"<code>root</code>  <code>property</code>","text":"<p>Root directory path for the dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.Imagenet.split","title":"<code>split = 'train'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgFolder","title":"<code>ImgFolder(root=pathlib.Path('./data/split'))</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p> <p>Configuration for a generic image folder dataset.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgFolder.n_examples","title":"<code>n_examples</code>  <code>property</code>","text":"<p>Number of examples in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgFolder.root","title":"<code>root = pathlib.Path('./data/split')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where the class folders with images are stored. Can be a glob pattern to match multiple directories.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgFolderDataset","title":"<code>ImgFolderDataset(*args, sample_transform=None, **kwargs)</code>","text":"<p>               Bases: <code>ImageFolder</code></p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __init__(self, *args, sample_transform: Callable | None = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.sample_transform = sample_transform\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.ImgFolderDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index</p> required <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>dict with keys 'image', 'index', 'target' and 'label'.</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict[str, object]:\n    \"\"\"\n    Args:\n        index: Index\n\n    Returns:\n        dict with keys 'image', 'index', 'target' and 'label'.\n    \"\"\"\n    path, target = self.samples[index]\n    image = self.loader(path)\n    if self.transform is not None:\n        image = self.transform(image)\n    if self.target_transform is not None:\n        target = self.target_transform(target)\n\n    sample = {\n        \"image\": image,\n        \"target\": target,\n        \"label\": self.classes[target],\n        \"index\": index,\n    }\n\n    if self.sample_transform is not None:\n        sample = self.sample_transform(sample)\n\n    return sample\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder","title":"<code>ImgSegFolder(root=pathlib.Path('./data/segdataset'), split='training', img_label_fname='sceneCategories.txt', bg_label=0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetConfig</code></p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder.bg_label","title":"<code>bg_label = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Background label.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder.img_label_fname","title":"<code>img_label_fname = 'sceneCategories.txt'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Image labels filename.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder.n_examples","title":"<code>n_examples</code>  <code>property</code>","text":"<p>Number of examples in the dataset. Calculated on the fly by counting image files in root/images/split.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder.root","title":"<code>root = pathlib.Path('./data/segdataset')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Where the class folders with images are stored.</p>"},{"location":"api/data/datasets/#saev.data.datasets.ImgSegFolder.split","title":"<code>split = 'training'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Data split.</p>"},{"location":"api/data/datasets/#saev.data.datasets.get_dataset","title":"<code>get_dataset(cfg, *, img_transform, seg_transform=None, sample_transform=None)</code>","text":"<p>Gets the dataset for the current experiment; delegates construction to dataset-specific functions.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DatasetConfig</code> <p>Experiment config.</p> required <code>img_transform</code> <code>Callable</code> <p>Image transform to be applied to each image.</p> required <code>seg_transform</code> <code>Callable | None</code> <p>Segmentation transform to be applied to masks (for segmentation datasets).</p> <code>None</code> <code>sample_transform</code> <code>Callable | None</code> <p>Transform to be applied to each sample dict.</p> <code>None</code> <p>Returns:     A dataset that has dictionaries with <code>'image'</code>, <code>'index'</code>, <code>'target'</code>, and <code>'label'</code> keys containing examples.</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>@beartype.beartype\ndef get_dataset(\n    cfg: DatasetConfig,\n    *,\n    img_transform: Callable,\n    seg_transform: Callable | None = None,\n    sample_transform: Callable | None = None,\n):\n    \"\"\"\n    Gets the dataset for the current experiment; delegates construction to dataset-specific functions.\n\n    Args:\n        cfg: Experiment config.\n        img_transform: Image transform to be applied to each image.\n        seg_transform: Segmentation transform to be applied to masks (for segmentation datasets).\n        sample_transform: Transform to be applied to each sample dict.\n    Returns:\n        A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples.\n    \"\"\"\n    # TODO: Can we reduce duplication? Or is it nice to see that there is no magic here?\n    if isinstance(cfg, Imagenet):\n        return ImagenetDataset(\n            cfg, img_transform=img_transform, sample_transform=sample_transform\n        )\n    elif isinstance(cfg, Cifar10):\n        return Cifar10Dataset(\n            cfg, img_transform=img_transform, sample_transform=sample_transform\n        )\n    elif isinstance(cfg, ImgSegFolder):\n        return ImgSegFolderDataset(\n            cfg,\n            img_transform=img_transform,\n            seg_transform=seg_transform,\n            sample_transform=sample_transform,\n        )\n    elif isinstance(cfg, ImgFolder):\n        ds = [\n            ImgFolderDataset(\n                root, transform=img_transform, sample_transform=sample_transform\n            )\n            for root in glob.glob(cfg.root, recursive=True)\n        ]\n        if len(ds) == 1:\n            return ds[0]\n        else:\n            return torch.utils.data.ConcatDataset(ds)\n    elif isinstance(cfg, FakeImg):\n        return FakeImgDataset(\n            cfg, img_transform=img_transform, sample_transform=sample_transform\n        )\n    elif isinstance(cfg, FakeImgSeg):\n        return FakeImgSegDataset(\n            cfg,\n            img_transform=img_transform,\n            seg_transform=seg_transform,\n            sample_transform=sample_transform,\n        )\n    else:\n        typing.assert_never(cfg)\n</code></pre>"},{"location":"api/data/datasets/#saev.data.datasets.is_img_seg_dataset","title":"<code>is_img_seg_dataset(data_cfg)</code>","text":"<p>Check if a dataset configuration is for an image segmentation dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_cfg</code> <code>DatasetConfig</code> <p>Dataset configuration</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this is an image segmentation dataset that should have labels.bin</p> Source code in <code>src/saev/data/datasets.py</code> <pre><code>@beartype.beartype\ndef is_img_seg_dataset(data_cfg: DatasetConfig) -&gt; bool:\n    \"\"\"\n    Check if a dataset configuration is for an image segmentation dataset.\n\n    Args:\n        data_cfg: Dataset configuration\n\n    Returns:\n        True if this is an image segmentation dataset that should have labels.bin\n    \"\"\"\n    return isinstance(data_cfg, (FakeImgSeg, ImgSegFolder))\n</code></pre>"},{"location":"api/data/dinov2/","title":"saev.data.dinov2","text":""},{"location":"api/data/dinov3/","title":"saev.data.dinov3","text":""},{"location":"api/data/dinov3/#saev.data.dinov3.Config","title":"<code>Config(img_size=224, patch_size=16, in_chans=3, pos_embed_rope_base=100.0, pos_embed_rope_min_period=None, pos_embed_rope_max_period=None, pos_embed_rope_normalize_coords='separate', pos_embed_rope_dtype='bf16', embed_dim=768, depth=12, num_heads=12, ffn_ratio=4.0, qkv_bias=True, ffn_layer='mlp', ffn_bias=True, proj_bias=True, n_storage_tokens=0, mask_k_bias=False, untie_global_and_local_cls_norm=False, device=None)</code>  <code>dataclass</code>","text":""},{"location":"api/data/dinov3/#saev.data.dinov3.Config.depth","title":"<code>depth = 12</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of transformer blocks.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.device","title":"<code>device = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device for tensor operations.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.embed_dim","title":"<code>embed_dim = 768</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Embedding dimension for transformer.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_bias","title":"<code>ffn_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in feed-forward network.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_layer","title":"<code>ffn_layer = 'mlp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of feed-forward network layer.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.ffn_ratio","title":"<code>ffn_ratio = 4.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Feed-forward network expansion ratio.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.img_size","title":"<code>img_size = 224</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Image width and height in pixels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.in_chans","title":"<code>in_chans = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of input image channels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.mask_k_bias","title":"<code>mask_k_bias = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to mask K bias in attention.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.n_storage_tokens","title":"<code>n_storage_tokens = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of storage/register tokens.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.num_heads","title":"<code>num_heads = 12</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of attention heads.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.patch_size","title":"<code>patch_size = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of each patch in pixels.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_base","title":"<code>pos_embed_rope_base = 100.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base frequency for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_dtype","title":"<code>pos_embed_rope_dtype = 'bf16'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Data type for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_max_period","title":"<code>pos_embed_rope_max_period = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum period for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_min_period","title":"<code>pos_embed_rope_min_period = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum period for RoPE positional encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.pos_embed_rope_normalize_coords","title":"<code>pos_embed_rope_normalize_coords = 'separate'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinate normalization method for RoPE encoding.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.proj_bias","title":"<code>proj_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in output projection.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.qkv_bias","title":"<code>qkv_bias = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use bias in QKV projection.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.Config.untie_global_and_local_cls_norm","title":"<code>untie_global_and_local_cls_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use separate norms for global and local CLS tokens.</p>"},{"location":"api/data/dinov3/#saev.data.dinov3.PatchEmbed","title":"<code>PatchEmbed(img_size=224, patch_size=16, in_chans=3, embed_dim=768, flatten_embedding=True)</code>","text":"<p>               Bases: <code>Module</code></p> <p>2D image to patch embedding: (B,C,H,W) -&gt; (B,N,D)</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int | tuple[int, int]</code> <p>Image size.</p> <code>224</code> <code>patch_size</code> <code>int | tuple[int, int]</code> <p>Patch token size.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input image channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Number of linear projection output channels.</p> <code>768</code> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>def __init__(\n    self,\n    img_size: int | tuple[int, int] = 224,\n    patch_size: int | tuple[int, int] = 16,\n    in_chans: int = 3,\n    embed_dim: int = 768,\n    flatten_embedding: bool = True,\n) -&gt; None:\n    super().__init__()\n\n    image_hw = make_2tuple(img_size)\n    patch_hw = make_2tuple(patch_size)\n\n    self.image_hw = image_hw\n    self.patch_hw = patch_hw\n\n    self.in_chans = in_chans\n    self.embed_dim = embed_dim\n\n    self.proj = nn.Conv2d(\n        in_chans, embed_dim, kernel_size=patch_hw, stride=patch_hw\n    )\n    self.k = patch_hw[0]\n    assert self.proj.kernel_size == (self.k, self.k)\n    assert self.proj.stride == (self.k, self.k)\n    assert self.proj.padding == (0, 0)\n    assert self.proj.groups == 1\n    assert self.proj.dilation == (1, 1)\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>Module</code>, <code>VisionTransformer</code></p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n    name = self._parse_name(ckpt)\n    self.model = load(name, ckpt)\n\n    self._ckpt = name\n    self.logger = logging.getLogger(f\"dinov3/{name}\")\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n    import functools\n\n    return functools.partial(\n        transforms.resize_to_patch_grid,\n        p=int(16 * scale),\n        n=n_patches_per_img,\n        resample=resample,\n    )\n</code></pre>"},{"location":"api/data/dinov3/#saev.data.dinov3.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/dinov3.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    img_transform = v2.Compose([\n        transforms.FlexResize(patch_size=16, n_patches=n_patches_per_img),\n        v2.ToImage(),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),\n    ])\n    sample_transform = transforms.Patchify(\n        patch_size=16, n_patches=n_patches_per_img\n    )\n    return img_transform, sample_transform\n</code></pre>"},{"location":"api/data/fake_clip/","title":"saev.data.fake_clip","text":"<p>Fake CLIP model for testing with tiny-open-clip-model.</p> <p>This module provides a test-only vision transformer that works with the tiny-open-clip-model from HuggingFace, which uses 8x8 images and 2x2 patches instead of the standard 224x224 images with 16x16 patches.</p>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>VisionTransformer</code>, <code>Module</code></p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    # Only support the tiny test model\n    assert ckpt == \"hf-hub:hf-internal-testing/tiny-open-clip-model\", (\n        f\"FakeClip only supports tiny-open-clip-model, got {ckpt}\"\n    )\n\n    clip, _ = open_clip.create_model_from_pretrained(\n        ckpt, cache_dir=helpers.get_cache_dir()\n    )\n    self._ckpt = ckpt\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model.eval()\n</code></pre>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.patch_size","title":"<code>patch_size</code>  <code>property</code>","text":"<p>Tiny model uses 2x2 patches.</p>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img=-1, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for tiny model (8x8 images).</p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int = -1,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for tiny model (8x8 images).\"\"\"\n\n    def resize(img: Image.Image) -&gt; Image.Image:\n        # Tiny model uses 8x8 images\n        size_px = (int(8 * scale), int(8 * scale))\n        return img.resize(size_px, resample=resample)\n\n    return resize\n</code></pre>"},{"location":"api/data/fake_clip/#saev.data.fake_clip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/fake_clip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    _, img_transform = open_clip.create_model_from_pretrained(\n        ckpt, cache_dir=helpers.get_cache_dir()\n    )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/indexed/","title":"saev.data.indexed","text":""},{"location":"api/data/indexed/#saev.data.indexed.Config","title":"<code>Config(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading indexed activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['special', 'content', 'all']</code> <p>Which kinds of tokens to use. 'special' indicates the special tokens token (if any). 'content' returns content tokens. 'all' returns both content and special tokens.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset","title":"<code>Dataset(cfg)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset of activations from disk.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration set via CLI args.</p> <code>md</code> <code>Metadata</code> <p>Activations metadata; automatically loaded from disk.</p> <code>layer_idx</code> <code>int</code> <p>Layer index into the shards if we are choosing a specific layer.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    self.md = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    # Check if labels.bin exists\n    labels_path = os.path.join(self.cfg.shards, \"labels.bin\")\n    self.labels_mmap = None\n    if os.path.exists(labels_path):\n        self.labels_mmap = np.memmap(\n            labels_path,\n            mode=\"r\",\n            dtype=np.uint8,\n            shape=(self.md.n_examples, self.md.content_tokens_per_example),\n        )\n\n    self.index_map = shards.IndexMap(self.md, self.cfg.tokens, self.cfg.layer)\n</code></pre>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.d_model","title":"<code>d_model</code>  <code>property</code>","text":"<p>Dimension of the underlying vision transformer's embedding space.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.Example","title":"<code>Example</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/indexed/#saev.data.indexed.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Dataset length depends on <code>patches</code> and <code>layer</code>.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Dataset length depends on `patches` and `layer`.\n    \"\"\"\n    return len(self.index_map)\n</code></pre>"},{"location":"api/data/models/","title":"saev.data.models","text":""},{"location":"api/data/models/#saev.data.models.VisionTransformer","title":"<code>VisionTransformer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Protocol defining the interface for all Vision Transformer models.</p>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.patch_size","title":"<code>patch_size</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Patch size in pixels (e.g., 14 or 16).</p>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.forward","title":"<code>forward(batch)</code>  <code>abstractmethod</code>","text":"<p>Run forward pass on batch of images.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef forward(\n    self, batch: Float[Tensor, \"batch 3 width height\"]\n) -&gt; Float[Tensor, \"batch patches dim\"]:\n    \"\"\"Run forward pass on batch of images.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_residuals","title":"<code>get_residuals()</code>  <code>abstractmethod</code>","text":"<p>Return the list of residual blocks/layers for hook registration.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef get_residuals(self) -&gt; list[torch.nn.Module]:\n    \"\"\"Return the list of residual blocks/layers for hook registration.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.get_token_i","title":"<code>get_token_i(content_tokens_per_example)</code>  <code>abstractmethod</code>","text":"<p>Return indices for selecting relevant tokens from activations.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@abc.abstractmethod\ndef get_token_i(self, content_tokens_per_example: int) -&gt; slice | torch.Tensor:\n    \"\"\"Return indices for selecting relevant tokens from activations.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.VisionTransformer.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n</code></pre>"},{"location":"api/data/models/#saev.data.models.list_families","title":"<code>list_families()</code>","text":"<p>List all ViT family names.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>def list_families() -&gt; list[str]:\n    \"\"\"List all ViT family names.\"\"\"\n    return list(_global_model_registry.keys())\n</code></pre>"},{"location":"api/data/models/#saev.data.models.load_model_cls","title":"<code>load_model_cls(family)</code>","text":"<p>Load a ViT family class.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@beartype.beartype\ndef load_model_cls(family: str) -&gt; type[VisionTransformer]:\n    \"\"\"Load a ViT family class.\"\"\"\n    if family not in _global_model_registry:\n        raise ValueError(f\"Family '{family}' not found.\")\n\n    return _global_model_registry[family]\n</code></pre>"},{"location":"api/data/models/#saev.data.models.register_family","title":"<code>register_family(cls)</code>","text":"<p>Register a new ViT family class.</p> Source code in <code>src/saev/data/models.py</code> <pre><code>@beartype.beartype\ndef register_family(cls: type[VisionTransformer]):\n    \"\"\"Register a new ViT family class.\"\"\"\n    if cls.family in _global_model_registry:\n        logger.warning(\"Overwriting key '%s' in registry.\", cls.family)\n    _global_model_registry[cls.family] = cls\n</code></pre>"},{"location":"api/data/ordered/","title":"saev.data.ordered","text":"<p>Ordered (sequential) dataloader for activation data.</p> <p>This module provides a high-throughput dataloader that reads activation data from disk shards in sequential order, without shuffling. The implementation uses a single-threaded manager process to ensure data is delivered in the exact order it appears on disk.</p> <p>Patch labels are provided if there is a labels.bin file on disk.</p> <p>See the design decisions in src/saev/data/performance.md.</p> Usage <p>cfg = Config(shards=\"./shards\", layer=13, batch_size=4096) dataloader = DataLoader(cfg) for batch in dataloader: ...     activations = batch[\"act\"]  # [batch_size, d_model] ...     image_indices = batch[\"example_idx\"]  # [batch_size] ...     patch_indices = batch[\"token_idx\"]  # [batch_size] ...     patch_labels = batch[\"patch_labels\"]  # [batch_size]</p>"},{"location":"api/data/ordered/#saev.data.ordered.Config","title":"<code>Config(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, batch_size=1024 * 16, batch_timeout_s=30.0, drop_last=False, buffer_size=64, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading ordered (non-shuffled) activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['content']</code> <p>Which kinds of tokens to use. 'special' indicates the special tokens token (if any). 'content' returns content tokens. 'all' returns both content and special tokens.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>batch_timeout_s</code> <code>float</code> <p>How long to wait for at least one batch.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch if it's smaller than the others.</p> <code>buffer_size</code> <code>int</code> <p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p> <code>log_every_s</code> <code>float</code> <p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader","title":"<code>DataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that reads data from disk shards in order (no shuffling).</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    self.md = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self.logger = logging.getLogger(\"ordered.DataLoader\")\n    self.ctx = mp.get_context()\n    self.manager_proc = None\n    self.batch_queue = None\n    self.stop_event = None\n    self._n_samples = self._calculate_n_samples()\n    self.logger.info(\n        \"Initialized ordered.DataLoader with %d samples. (debug=%s)\",\n        self.n_samples,\n        self.cfg.debug,\n    )\n</code></pre>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches in order.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterable[ExampleBatch]:\n    \"\"\"Yields batches in order.\"\"\"\n    self._start_manager()\n    n = 0\n\n    try:\n        while n &lt; self.n_samples:\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                batch = self.batch_queue.get(timeout=self.cfg.batch_timeout_s)\n                actual_batch_size = batch[\"act\"].shape[0]\n\n                # Handle drop_last\n                if (\n                    self.cfg.drop_last\n                    and actual_batch_size &lt; self.cfg.batch_size\n                    and n + actual_batch_size &gt;= self.n_samples\n                ):\n                    break\n\n                n += actual_batch_size\n                yield self.ExampleBatch(**batch)\n                continue\n            except queue.Empty:\n                self.logger.info(\n                    \"Did not get a batch from manager process in %.1fs seconds.\",\n                    self.cfg.batch_timeout_s,\n                )\n            except FileNotFoundError:\n                self.logger.info(\"Manager process (probably) closed.\")\n                continue\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {n}/{self.n_samples} samples.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/ordered/#saev.data.ordered.DataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    if self.cfg.drop_last:\n        return self.n_samples // self.cfg.batch_size\n    else:\n        return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/","title":"saev.data","text":"<p>.. include:: ./protocol.md</p> <p>.. include:: ./performance.md</p>"},{"location":"api/data/saev.data/#saev.data.IndexedConfig","title":"<code>IndexedConfig(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, debug=False)</code>  <code>dataclass</code>","text":"<p>Configuration for loading indexed activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['special', 'content', 'all']</code> <p>Which kinds of tokens to use. 'special' indicates the special tokens token (if any). 'content' returns content tokens. 'all' returns both content and special tokens.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/saev.data/#saev.data.IndexedDataset","title":"<code>IndexedDataset(cfg)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset of activations from disk.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration set via CLI args.</p> <code>md</code> <code>Metadata</code> <p>Activations metadata; automatically loaded from disk.</p> <code>layer_idx</code> <code>int</code> <p>Layer index into the shards if we are choosing a specific layer.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    self.md = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    # Check if labels.bin exists\n    labels_path = os.path.join(self.cfg.shards, \"labels.bin\")\n    self.labels_mmap = None\n    if os.path.exists(labels_path):\n        self.labels_mmap = np.memmap(\n            labels_path,\n            mode=\"r\",\n            dtype=np.uint8,\n            shape=(self.md.n_examples, self.md.content_tokens_per_example),\n        )\n\n    self.index_map = shards.IndexMap(self.md, self.cfg.tokens, self.cfg.layer)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.IndexedDataset.d_model","title":"<code>d_model</code>  <code>property</code>","text":"<p>Dimension of the underlying vision transformer's embedding space.</p>"},{"location":"api/data/saev.data/#saev.data.IndexedDataset.Example","title":"<code>Example</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.IndexedDataset.__len__","title":"<code>__len__()</code>","text":"<p>Dataset length depends on <code>patches</code> and <code>layer</code>.</p> Source code in <code>src/saev/data/indexed.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Dataset length depends on `patches` and `layer`.\n    \"\"\"\n    return len(self.index_map)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.Metadata","title":"<code>Metadata(*, family, ckpt, layers, content_tokens_per_example, cls_token, d_model, n_examples, max_tokens_per_shard, data, dataset, pixel_agg=None, dtype='float32', protocol='2.0')</code>  <code>dataclass</code>","text":"<p>Metadata for a sharded set of transformer activations.</p> <p>Parameters:</p> Name Type Description Default <code>family</code> <code>Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip']</code> <p>The transformer family.</p> required <code>ckpt</code> <code>str</code> <p>The transformer checkpoint.</p> required <code>layers</code> <code>tuple[int, ...]</code> <p>Which layers were saved.</p> required <code>content_tokens_per_example</code> <code>int</code> <p>The number of content tokens per example.</p> required <code>cls_token</code> <code>bool</code> <p>Whether the transformer has a [CLS] token as well.</p> required <code>d_model</code> <code>int</code> <p>Model hidden dimension.</p> required <code>n_examples</code> <code>int</code> <p>Number of examples.</p> required <code>max_tokens_per_shard</code> <code>int</code> <p>The maximum number of tokens per shard.</p> required <code>data</code> <code>dict[str, object]</code> <p>A dictionary describing the original dataset.</p> required <code>dataset</code> <code>Path</code> <p>Absolute path to the root directory of the original dataset.</p> required <code>pixel_agg</code> <code>Literal['majority', 'prefer-fg', None]</code> <p>(only for image segmentation datasets) how the pixel-level segmentation labels were aggregated to token-level labels.</p> <code>None</code> <code>dtype</code> <code>Literal['float32']</code> <p>How activations are stored.</p> <code>'float32'</code> <code>protocol</code> <code>Literal['1.0.0', '1.1', '2.0']</code> <p>Protocol version.</p> <code>'2.0'</code>"},{"location":"api/data/saev.data/#saev.data.Metadata.examples_per_shard","title":"<code>examples_per_shard</code>  <code>property</code>","text":"<p>The number of examples per shard based on the protocol.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of examples that fit in a shard.</p>"},{"location":"api/data/saev.data/#saev.data.Metadata.hash","title":"<code>hash</code>  <code>property</code>","text":"<p>SHA256 hash of the metadata configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal hash string uniquely identifying this configuration.</p>"},{"location":"api/data/saev.data/#saev.data.Metadata.n_shards","title":"<code>n_shards</code>  <code>property</code>","text":"<p>Total number of shards needed to store all examples.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of shards required.</p>"},{"location":"api/data/saev.data/#saev.data.Metadata.shard_shape","title":"<code>shard_shape</code>  <code>property</code>","text":"<p>Shape of each shard file.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>Tuple of (examples_per_shard, n_layers, tokens_per_example, d_model).</p>"},{"location":"api/data/saev.data/#saev.data.Metadata.tokens_per_example","title":"<code>tokens_per_example</code>  <code>property</code>","text":"<p>Total number of tokens per example including [CLS] token if present.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens plus one if [CLS] token is included.</p>"},{"location":"api/data/saev.data/#saev.data.Metadata.dump","title":"<code>dump(shards_root)</code>","text":"<p>Dumps a Metadata object to a metadata.json file in shards_root / hash.</p> <p>Parameters:</p> Name Type Description Default <code>shards_root</code> <code>Path</code> <p>Path to $SAEV_SCRATCH/saev/shards as described in disk-layout.md.</p> required Source code in <code>src/saev/data/shards.py</code> <pre><code>def dump(self, shards_root: pathlib.Path):\n    \"\"\"\n    Dumps a Metadata object to a metadata.json file in shards_root / hash.\n\n    Args:\n        shards_root: Path to $SAEV_SCRATCH/saev/shards as described in [disk-layout.md](../../developers/disk-layout.md).\n    \"\"\"\n    assert disk.is_shards_root(shards_root)\n    (shards_root / self.hash).mkdir(exist_ok=True)\n    with open(shards_root / self.hash / \"metadata.json\", \"wb\") as fd:\n        helpers.dump(self, fd, option=orjson.OPT_INDENT_2)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.Metadata.load","title":"<code>load(shards_dir)</code>  <code>classmethod</code>","text":"<p>Loads a Metadata object from a metadata.json file in shards_dir.</p> <p>Parameters:</p> Name Type Description Default <code>shards_dir</code> <code>Path</code> <p>Path to $SAEV_SCRATCH/saev/shards/ as described in disk-layout.md. required Source code in <code>src/saev/data/shards.py</code> <pre><code>@classmethod\ndef load(cls, shards_dir: pathlib.Path) -&gt; \"Metadata\":\n    \"\"\"\n    Loads a Metadata object from a metadata.json file in shards_dir.\n\n    Args:\n        shards_dir: Path to $SAEV_SCRATCH/saev/shards/&lt;hash&gt; as described in [disk-layout.md](../../developers/disk-layout.md).\n    \"\"\"\n    assert disk.is_shards_dir(shards_dir)\n\n    with open(shards_dir / \"metadata.json\") as fd:\n        dct = json.load(fd)\n    dct[\"layers\"] = tuple(dct.pop(\"layers\"))\n    dct[\"dataset\"] = pathlib.Path(dct[\"dataset\"])\n    return cls(**dct)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.OrderedConfig","title":"<code>OrderedConfig(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, batch_size=1024 * 16, batch_timeout_s=30.0, drop_last=False, buffer_size=64, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading ordered (non-shuffled) activation data from disk</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['content']</code> <p>Which kinds of tokens to use. 'special' indicates the special tokens token (if any). 'content' returns content tokens. 'all' returns both content and special tokens.</p> <code>layer</code> <code>int | Literal['all']</code> <p>Which ViT layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>batch_timeout_s</code> <code>float</code> <p>How long to wait for at least one batch.</p> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch if it's smaller than the others.</p> <code>buffer_size</code> <code>int</code> <p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p> <code>debug</code> <code>bool</code> <p>Whether the dataloader process should log debug messages.</p> <code>log_every_s</code> <code>float</code> <p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader","title":"<code>OrderedDataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that reads data from disk shards in order (no shuffling).</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    self.md = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self.logger = logging.getLogger(\"ordered.DataLoader\")\n    self.ctx = mp.get_context()\n    self.manager_proc = None\n    self.batch_queue = None\n    self.stop_event = None\n    self._n_samples = self._calculate_n_samples()\n    self.logger.info(\n        \"Initialized ordered.DataLoader with %d samples. (debug=%s)\",\n        self.n_samples,\n        self.cfg.debug,\n    )\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches in order.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterable[ExampleBatch]:\n    \"\"\"Yields batches in order.\"\"\"\n    self._start_manager()\n    n = 0\n\n    try:\n        while n &lt; self.n_samples:\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                batch = self.batch_queue.get(timeout=self.cfg.batch_timeout_s)\n                actual_batch_size = batch[\"act\"].shape[0]\n\n                # Handle drop_last\n                if (\n                    self.cfg.drop_last\n                    and actual_batch_size &lt; self.cfg.batch_size\n                    and n + actual_batch_size &gt;= self.n_samples\n                ):\n                    break\n\n                n += actual_batch_size\n                yield self.ExampleBatch(**batch)\n                continue\n            except queue.Empty:\n                self.logger.info(\n                    \"Did not get a batch from manager process in %.1fs seconds.\",\n                    self.cfg.batch_timeout_s,\n                )\n            except FileNotFoundError:\n                self.logger.info(\"Manager process (probably) closed.\")\n                continue\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {n}/{self.n_samples} samples.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.OrderedDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/ordered.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    if self.cfg.drop_last:\n        return self.n_samples // self.cfg.batch_size\n    else:\n        return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig","title":"<code>ShuffledConfig(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, batch_size=1024 * 16, drop_last=False, scale_norm=False, ignore_labels=list(), n_threads=4, buffer_size=64, batch_timeout_s=30.0, seed=17, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading shuffled activation data from disk.</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['special', 'content', 'all']</code> <p>Which subset of tokens to use. 'special' indicates the special tokens (if any). 'content' indicates it will return content tokens. 'all' returns all tokens.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.ignore_labels","title":"<code>ignore_labels = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If provided, exclude tokens with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which transformer layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.log_every_s","title":"<code>log_every_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.n_threads","title":"<code>n_threads = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of dataloading threads.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.scale_norm","title":"<code>scale_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to scale norms to sqrt(D).</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledConfig.seed","title":"<code>seed = 17</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader","title":"<code>ShuffledDataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that deterministically shuffles data from disk shards.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n\n    self.manager_proc = None\n    self.reservoir = None\n    self.stop_event = None\n\n    self.logger = logging.getLogger(\"shuffled.DataLoader\")\n    self.ctx = mp.get_context()\n\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    if self.cfg.scale_norm:\n        raise NotImplementedError(\"scale_norm not implemented.\")\n\n    self.metadata = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self._n_samples = self._calculate_n_samples()\n\n    # Check if labels.bin exists for filtering\n    self.labels_mmap = None\n    if self.cfg.ignore_labels:\n        labels_path = os.path.join(self.cfg.shards, \"labels.bin\")\n        if not os.path.exists(labels_path):\n            raise FileNotFoundError(\n                f\"ignore_labels filtering requested but labels.bin not found at {labels_path}\"\n            )\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterator[ExampleBatch]:\n    \"\"\"Yields batches.\"\"\"\n    self._start_manager()\n    n, b = 0, 0\n\n    try:\n        while n &lt; self.n_samples:\n            need = min(self.cfg.batch_size, self.n_samples - n)\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                act, meta = self.reservoir.get(\n                    need, timeout=self.cfg.batch_timeout_s\n                )\n                n += need\n                b += 1\n                example_idx, token_idx = meta.T\n                yield self.ExampleBatch(\n                    act=act, example_idx=example_idx, token_idx=token_idx\n                )\n                continue\n            except TimeoutError:\n                if self.cfg.ignore_labels:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds. This can happen when filtering out many labels.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n                else:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {b}/{len(self)} batches.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.ShuffledDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/saev.data/#saev.data.make_ordered_config","title":"<code>make_ordered_config(shuffled_cfg, **overrides)</code>","text":"<p>Create an <code>OrderedConfig</code> from a <code>ShuffledConfig</code>, with optional overrides.</p> <p>Defaults come from <code>shuffled_cfg</code> for fields present in <code>OrderedConfig</code>, and <code>overrides</code> take precedence. Unknown override fields raise <code>TypeError</code> from the <code>OrderedConfig</code> constructor, mirroring <code>dataclasses.replace</code>.</p> Source code in <code>src/saev/data/__init__.py</code> <pre><code>@beartype.beartype\ndef make_ordered_config(\n    shuffled_cfg: ShuffledConfig, **overrides: object\n) -&gt; OrderedConfig:\n    \"\"\"Create an `OrderedConfig` from a `ShuffledConfig`, with optional overrides.\n\n    Defaults come from `shuffled_cfg` for fields present in `OrderedConfig`, and `overrides` take precedence. Unknown override fields raise `TypeError` from the `OrderedConfig` constructor, mirroring `dataclasses.replace`.\n    \"\"\"\n    params: dict[str, object] = {}\n    for f in dataclasses.fields(OrderedConfig):\n        name = f.name\n        if hasattr(shuffled_cfg, name):\n            params[name] = getattr(shuffled_cfg, name)\n    params.update(overrides)\n    return OrderedConfig(**params)\n</code></pre>"},{"location":"api/data/shards/","title":"saev.data.shards","text":"<p>Library code for reading and writing sharded activations to disk.</p>"},{"location":"api/data/shards/#saev.data.shards.Index","title":"<code>Index(*, idx, example_idx, content_token_idx, shard_idx, example_idx_in_shard, layer_idx_in_shard, token_idx_in_shard)</code>  <code>dataclass</code>","text":"<p>Attributes:</p> Name Type Description <code>idx</code> <code>int</code> <p>The index of the activation.</p> <code>example_idx</code> <code>int</code> <p>The index of the original example (image, audio clip etc).</p> <code>content_token_idx</code> <code>int</code> <p>The token's index within an example's content. -1 for all special tokens.</p> <code>shard_idx</code> <code>int</code> <p>The shard index.</p> <code>example_idx_in_shard</code> <code>int</code> <p>The example index along the examples axis in a shard.</p> <code>token_idx_in_shard</code> <code>int</code> <p>The token index along the tokens axis in a shard.</p>"},{"location":"api/data/shards/#saev.data.shards.IndexMap","title":"<code>IndexMap(md, tokens, layer)</code>","text":"<p>Attributes:</p> Name Type Description <code>md</code> <code>Metadata</code> <p>Metadata</p> <code>tokens</code> <code>Literal['special', 'content', 'all']</code> <p>Which subset of tokens to load.</p> <code>layer</code> <code>int</code> <p>Which layer to load.</p> <code>layer_idx_lookup</code> <code>dict[int, int]</code> <p>The lookup from a transformer layer to the layer idx in the shard.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __init__(\n    self,\n    md: Metadata,\n    tokens: tp.Literal[\"special\", \"content\", \"all\"],\n    layer: int | tp.Literal[\"all\"],\n):\n    if tokens == \"special\":\n        assert md.cls_token\n\n    self.md = md\n    self.tokens = tokens\n    self.layer = layer\n\n    if isinstance(layer, int):\n        err_msg = f\"No matche for layer; {layer} not in {md.layers}.\"\n        assert layer in md.layers, err_msg\n\n    self.layer_idx_lookup = {layer: i for i, layer in enumerate(md.layers)}\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.IndexMap.__len__","title":"<code>__len__()</code>","text":"<p>Dataset length depends on <code>patches</code> and <code>layer</code>.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Dataset length depends on `patches` and `layer`.\n    \"\"\"\n    match (self.tokens, self.layer):\n        case (\"special\", \"all\"):\n            # Return a CLS token from a random example and random layer.\n            return self.md.n_examples * len(self.md.layers)\n        case (\"special\", int()):\n            # Return a CLS token from a random example and fixed layer.\n            return self.md.n_examples\n        case (\"content\", int()):\n            # Return a patch from a random example, fixed layer, and random patch.\n            return self.md.n_examples * self.md.content_tokens_per_example\n        case (\"content\", \"all\"):\n            # Return a patch from a random example, random layer and random patch.\n            return (\n                self.md.n_examples\n                * len(self.md.layers)\n                * self.md.content_tokens_per_example\n            )\n        case (\"all\", int()):\n            # Return a token from a random example, fixed layer, and random token (including special).\n            return self.md.n_examples * self.md.tokens_per_example\n        case (\"all\", \"all\"):\n            # Return a token from a random example, random layer and random token (including special).\n            return (\n                self.md.n_examples\n                * len(self.md.layers)\n                * self.md.tokens_per_example\n            )\n        case _:\n            tp.assert_never((self.cfg.tokens, self.cfg.layer))\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.LabelsWriter","title":"<code>LabelsWriter(shards_dir, md)</code>","text":"<p>LabelsWriter handles writing patch-level segmentation labels to a single binary file.</p> <p>Parameters:</p> Name Type Description Default <code>shards_dir</code> <code>Path</code> <p>The shard directory; $SAEV_SCRATCH/saev/shards/ required <code>md</code> <code>Metadata</code> <p>The Metadata object.</p> required <p>Attributes:</p> Name Type Description <code>labels</code> <code>UInt8[ndarray, 'n_examples n_patches']</code> <p>The integer patch labels.</p> <code>labels_path</code> <code>Path</code> <p>Where the integer patch labels are stored.</p> <code>md</code> <code>Metadata</code> <p>The dataset metadata.</p> <code>has_written</code> <code>bool</code> <p>Whether we have written any data to <code>self.labels</code>.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __init__(self, shards_dir: pathlib.Path, md: Metadata):\n    assert disk.is_shards_dir(shards_dir)\n    self.logger = logging.getLogger(\"labels-writer\")\n    self.md = md\n    self.has_written = False\n\n    # Always create memory-mapped file for labels\n    # If nothing is written, it will be deleted in flush()\n    self.labels_path = shards_dir / \"labels.bin\"\n    self.labels = np.memmap(\n        self.labels_path,\n        mode=\"w+\",\n        dtype=np.uint8,\n        shape=(self.md.n_examples, self.md.content_tokens_per_example),\n    )\n    self.logger.info(\"Opened labels file '%s'.\", self.labels_path)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.LabelsWriter.flush","title":"<code>flush()</code>","text":"<p>Flush the memory-mapped file to disk if anything was written.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush the memory-mapped file to disk if anything was written.\"\"\"\n    if self.has_written:\n        self.labels.flush()\n        self.logger.info(\"Flushed labels to '%s'.\", self.labels_path)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.LabelsWriter.write_batch","title":"<code>write_batch(batch_labels, start_idx)</code>","text":"<p>Write a batch of labels to the memory-mapped file.</p> <p>Parameters:</p> Name Type Description Default <code>batch_labels</code> <code>ndarray | Tensor</code> <p>Array of shape (batch_size, content_tokens_per_example) with uint8 dtype</p> required <code>start_idx</code> <code>int</code> <p>Starting index in the global labels array</p> required Source code in <code>src/saev/data/shards.py</code> <pre><code>@beartype.beartype\ndef write_batch(self, batch_labels: np.ndarray | Tensor, start_idx: int):\n    \"\"\"\n    Write a batch of labels to the memory-mapped file.\n\n    Args:\n        batch_labels: Array of shape (batch_size, content_tokens_per_example) with uint8 dtype\n        start_idx: Starting index in the global labels array\n    \"\"\"\n    # Convert to numpy if needed\n    if isinstance(batch_labels, torch.Tensor):\n        batch_labels = batch_labels.cpu().numpy()\n\n    batch_size = len(batch_labels)\n    assert start_idx + batch_size &lt;= self.md.n_examples\n    assert batch_labels.shape == (batch_size, self.md.content_tokens_per_example)\n    assert batch_labels.dtype == np.uint8\n\n    self.labels[start_idx : start_idx + batch_size] = batch_labels\n    self.has_written = True\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.Metadata","title":"<code>Metadata(*, family, ckpt, layers, content_tokens_per_example, cls_token, d_model, n_examples, max_tokens_per_shard, data, dataset, pixel_agg=None, dtype='float32', protocol='2.0')</code>  <code>dataclass</code>","text":"<p>Metadata for a sharded set of transformer activations.</p> <p>Parameters:</p> Name Type Description Default <code>family</code> <code>Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip']</code> <p>The transformer family.</p> required <code>ckpt</code> <code>str</code> <p>The transformer checkpoint.</p> required <code>layers</code> <code>tuple[int, ...]</code> <p>Which layers were saved.</p> required <code>content_tokens_per_example</code> <code>int</code> <p>The number of content tokens per example.</p> required <code>cls_token</code> <code>bool</code> <p>Whether the transformer has a [CLS] token as well.</p> required <code>d_model</code> <code>int</code> <p>Model hidden dimension.</p> required <code>n_examples</code> <code>int</code> <p>Number of examples.</p> required <code>max_tokens_per_shard</code> <code>int</code> <p>The maximum number of tokens per shard.</p> required <code>data</code> <code>dict[str, object]</code> <p>A dictionary describing the original dataset.</p> required <code>dataset</code> <code>Path</code> <p>Absolute path to the root directory of the original dataset.</p> required <code>pixel_agg</code> <code>Literal['majority', 'prefer-fg', None]</code> <p>(only for image segmentation datasets) how the pixel-level segmentation labels were aggregated to token-level labels.</p> <code>None</code> <code>dtype</code> <code>Literal['float32']</code> <p>How activations are stored.</p> <code>'float32'</code> <code>protocol</code> <code>Literal['1.0.0', '1.1', '2.0']</code> <p>Protocol version.</p> <code>'2.0'</code>"},{"location":"api/data/shards/#saev.data.shards.Metadata.examples_per_shard","title":"<code>examples_per_shard</code>  <code>property</code>","text":"<p>The number of examples per shard based on the protocol.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of examples that fit in a shard.</p>"},{"location":"api/data/shards/#saev.data.shards.Metadata.hash","title":"<code>hash</code>  <code>property</code>","text":"<p>SHA256 hash of the metadata configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal hash string uniquely identifying this configuration.</p>"},{"location":"api/data/shards/#saev.data.shards.Metadata.n_shards","title":"<code>n_shards</code>  <code>property</code>","text":"<p>Total number of shards needed to store all examples.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of shards required.</p>"},{"location":"api/data/shards/#saev.data.shards.Metadata.shard_shape","title":"<code>shard_shape</code>  <code>property</code>","text":"<p>Shape of each shard file.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int, int]</code> <p>Tuple of (examples_per_shard, n_layers, tokens_per_example, d_model).</p>"},{"location":"api/data/shards/#saev.data.shards.Metadata.tokens_per_example","title":"<code>tokens_per_example</code>  <code>property</code>","text":"<p>Total number of tokens per example including [CLS] token if present.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens plus one if [CLS] token is included.</p>"},{"location":"api/data/shards/#saev.data.shards.Metadata.dump","title":"<code>dump(shards_root)</code>","text":"<p>Dumps a Metadata object to a metadata.json file in shards_root / hash.</p> <p>Parameters:</p> Name Type Description Default <code>shards_root</code> <code>Path</code> <p>Path to $SAEV_SCRATCH/saev/shards as described in disk-layout.md.</p> required Source code in <code>src/saev/data/shards.py</code> <pre><code>def dump(self, shards_root: pathlib.Path):\n    \"\"\"\n    Dumps a Metadata object to a metadata.json file in shards_root / hash.\n\n    Args:\n        shards_root: Path to $SAEV_SCRATCH/saev/shards as described in [disk-layout.md](../../developers/disk-layout.md).\n    \"\"\"\n    assert disk.is_shards_root(shards_root)\n    (shards_root / self.hash).mkdir(exist_ok=True)\n    with open(shards_root / self.hash / \"metadata.json\", \"wb\") as fd:\n        helpers.dump(self, fd, option=orjson.OPT_INDENT_2)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.Metadata.load","title":"<code>load(shards_dir)</code>  <code>classmethod</code>","text":"<p>Loads a Metadata object from a metadata.json file in shards_dir.</p> <p>Parameters:</p> Name Type Description Default <code>shards_dir</code> <code>Path</code> <p>Path to $SAEV_SCRATCH/saev/shards/ as described in disk-layout.md. required Source code in <code>src/saev/data/shards.py</code> <pre><code>@classmethod\ndef load(cls, shards_dir: pathlib.Path) -&gt; \"Metadata\":\n    \"\"\"\n    Loads a Metadata object from a metadata.json file in shards_dir.\n\n    Args:\n        shards_dir: Path to $SAEV_SCRATCH/saev/shards/&lt;hash&gt; as described in [disk-layout.md](../../developers/disk-layout.md).\n    \"\"\"\n    assert disk.is_shards_dir(shards_dir)\n\n    with open(shards_dir / \"metadata.json\") as fd:\n        dct = json.load(fd)\n    dct[\"layers\"] = tuple(dct.pop(\"layers\"))\n    dct[\"dataset\"] = pathlib.Path(dct[\"dataset\"])\n    return cls(**dct)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.RecordedTransformer","title":"<code>RecordedTransformer(model, content_tokens_per_example, cls_token, layers)</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper around a transformer model that records intermediate layer activations during forward passes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The transformer model to wrap.</p> required <code>content_tokens_per_example</code> <code>int</code> <p>Number of content tokens per example.</p> required <code>cls_token</code> <code>bool</code> <p>Whether to record the [CLS] token in addition to content tokens.</p> required <code>layers</code> <code>Sequence[int]</code> <p>Which transformer layers to record activations from.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>The wrapped transformer model.</p> <code>content_tokens_per_example</code> <code>int</code> <p>Number of content tokens per example.</p> <code>cls_token</code> <code>bool</code> <p>Whether the [CLS] token is included in recorded activations.</p> <code>layers</code> <code>Sequence[int]</code> <p>Tuple of layer indices being recorded.</p> <code>token_i</code> <code>slice</code> <p>Token indices to extract from model outputs.</p> <code>logger</code> <p>Logger instance for this recorder.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    content_tokens_per_example: int,\n    cls_token: bool,\n    layers: Sequence[int],\n):\n    super().__init__()\n\n    self.model = model\n\n    self.content_tokens_per_example = content_tokens_per_example\n    self.cls_token = cls_token\n    self.layers = layers\n\n    self.token_i = model.get_token_i(content_tokens_per_example)\n\n    self._storage = None\n    self._i = 0\n\n    self.logger = logging.getLogger(f\"recorder({model.name})\")\n\n    for i in self.layers:\n        self.model.get_residuals()[i].register_forward_hook(self.hook)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.Shard","title":"<code>Shard(name, n_examples)</code>  <code>dataclass</code>","text":"<p>A single shard entry in shards.json, recording the filename and number of examples.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The filename of the shard (e.g., \"acts000000.bin\").</p> <code>n_examples</code> <code>int</code> <p>Number of examples stored in this shard.</p>"},{"location":"api/data/shards/#saev.data.shards.ShardInfo","title":"<code>ShardInfo(shards=list())</code>  <code>dataclass</code>","text":"<p>A container for shard metadata as recorded in shards.json.</p> <p>Parameters:</p> Name Type Description Default <code>shards</code> <code>list[Shard]</code> <p>A list of Shard objects.</p> <code>list()</code>"},{"location":"api/data/shards/#saev.data.shards.ShardWriter","title":"<code>ShardWriter(shards_root, md)</code>","text":"<p>ShardWriter is a stateful object that handles sharded activation writing to disk.</p> <p>Parameters:</p> Name Type Description Default <code>shards_root</code> <code>Path</code> <p>The $SAEV_SCRATCH/saev/shards path.</p> required <code>md</code> <code>Metadata</code> <p>The Metadata object for these shards.</p> required <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>The  $SAEV_SCRATCH/saev/shards/. <code>shard</code> <code>int</code> <code>acts_path</code> <code>Path</code> <code>acts</code> <code>Float[ndarray, 'examples_per_shard n_layers all_patches d_model'] | None</code> <code>filled</code> <code>int</code> <code>labels_writer</code> <code>LabelsWriter</code> <p>The LabelsWriter writer.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __init__(self, shards_root: pathlib.Path, md: Metadata):\n    assert disk.is_shards_root(shards_root)\n    self.md = md\n\n    self.logger = logging.getLogger(\"shard-writer\")\n\n    self.shards_dir = shards_root / md.hash\n    self.shards_dir.mkdir(exist_ok=True)\n\n    # builder for shard manifest\n    self._shards: ShardInfo = ShardInfo()\n\n    # Always initialize labels writer (it handles non-seg datasets internally)\n    self.labels_writer = LabelsWriter(self.shards_dir, md)\n\n    self.shard = -1\n    self.acts = None\n    self.next_shard()\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.ShardWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.ShardWriter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit - handle cleanup.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - handle cleanup.\"\"\"\n    self.flush()\n\n    # Delete empty labels file if nothing was written\n    if not self.labels_writer.has_written:\n        if os.path.exists(self.labels_writer.labels_path):\n            os.remove(self.labels_writer.labels_path)\n            self.logger.info(\n                \"Removed empty labels file '%s'.\", self.labels_writer.labels_path\n            )\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.ShardWriter.write_batch","title":"<code>write_batch(activations, start_idx, patch_labels=None)</code>","text":"<p>Write a batch of activations and (optionally) patch labels.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>Float[Tensor, 'batch n_layers all_patches d_model']</code> <p>Batch of activations to write.</p> required <code>start_idx</code> <code>int</code> <p>Starting index for this batch.</p> required <code>patch_labels</code> <code>UInt8[Tensor, 'batch n_patches'] | None</code> <p>Optional patch labels for segmentation datasets.</p> <code>None</code> Source code in <code>src/saev/data/shards.py</code> <pre><code>def write_batch(\n    self,\n    activations: Float[Tensor, \"batch n_layers all_patches d_model\"],\n    start_idx: int,\n    patch_labels: UInt8[Tensor, \"batch n_patches\"] | None = None,\n) -&gt; None:\n    \"\"\"Write a batch of activations and (optionally) patch labels.\n\n    Args:\n        activations: Batch of activations to write.\n        start_idx: Starting index for this batch.\n        patch_labels: Optional patch labels for segmentation datasets.\n    \"\"\"\n    batch_size = len(activations)\n    end_idx = start_idx + batch_size\n\n    # Write activations (handling sharding)\n    offset = self.md.examples_per_shard * self.shard\n\n    if end_idx &gt;= offset + self.md.examples_per_shard:\n        # We have run out of space in this mmap'ed file. Let's fill it as much as we can.\n        n_fit = offset + self.md.examples_per_shard - start_idx\n        self.acts[start_idx - offset : start_idx - offset + n_fit] = activations[\n            :n_fit\n        ]\n        self.filled = start_idx - offset + n_fit\n\n        # Write labels for the portion that fits\n        if patch_labels is not None:\n            # Convert to numpy uint8 if needed\n            if isinstance(patch_labels, torch.Tensor):\n                labels_to_write = (\n                    patch_labels[:n_fit].cpu().numpy().astype(np.uint8)\n                )\n            elif not isinstance(patch_labels, np.ndarray):\n                labels_to_write = np.array(patch_labels[:n_fit], dtype=np.uint8)\n            else:\n                labels_to_write = patch_labels[:n_fit]\n\n            self.labels_writer.write_batch(labels_to_write, start_idx)\n\n        self.next_shard()\n\n        # Recursively call write_batch for remaining data\n        if n_fit &lt; batch_size:\n            self.write_batch(\n                activations[n_fit:],\n                start_idx + n_fit,\n                patch_labels[n_fit:] if patch_labels is not None else None,\n            )\n    else:\n        msg = f\"0 &lt;= {start_idx} - {offset} &lt;= {offset} + {self.md.examples_per_shard}\"\n        assert 0 &lt;= start_idx - offset &lt;= offset + self.md.examples_per_shard, msg\n        msg = (\n            f\"0 &lt;= {end_idx} - {offset} &lt;= {offset} + {self.md.examples_per_shard}\"\n        )\n        assert 0 &lt;= end_idx - offset &lt;= offset + self.md.examples_per_shard, msg\n        self.acts[start_idx - offset : end_idx - offset] = activations\n        self.filled = end_idx - offset\n\n        # Write labels if provided\n        if patch_labels is not None:\n            # Convert to numpy uint8 if needed\n            if isinstance(patch_labels, torch.Tensor):\n                patch_labels = patch_labels.cpu().numpy().astype(np.uint8)\n            elif not isinstance(patch_labels, np.ndarray):\n                patch_labels = np.array(patch_labels, dtype=np.uint8)\n\n            self.labels_writer.write_batch(patch_labels, start_idx)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.get_dataloader","title":"<code>get_dataloader(data, *, batch_size, n_workers, img_tr=None, seg_tr=None, sample_tr=None)</code>","text":"<p>Get a dataloader for a default map-style dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Config</code> <p>Config for the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>n_workers</code> <code>int</code> <p>Number of dataloader workers.</p> required <code>img_tr</code> <code>Callable | None</code> <p>Image transform to be applied to each image.</p> <code>None</code> <code>seg_tr</code> <code>Callable | None</code> <p>Segmentation transform to be applied to masks.</p> <code>None</code> <code>sample_tr</code> <code>Callable | None</code> <p>Transform to be applied to sample dicts.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A PyTorch Dataloader that yields dictionaries with <code>'image'</code> keys containing image batches, <code>'index'</code> keys containing original dataset indices and <code>'label'</code> keys containing label batches.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>@beartype.beartype\ndef get_dataloader(\n    data: datasets.Config,\n    *,\n    batch_size: int,\n    n_workers: int,\n    img_tr: Callable | None = None,\n    seg_tr: Callable | None = None,\n    sample_tr: Callable | None = None,\n) -&gt; torch.utils.data.DataLoader:\n    \"\"\"\n    Get a dataloader for a default map-style dataset.\n\n    Args:\n        data: Config for the dataset.\n        batch_size: Batch size.\n        n_workers: Number of dataloader workers.\n        img_tr: Image transform to be applied to each image.\n        seg_tr: Segmentation transform to be applied to masks.\n        sample_tr: Transform to be applied to sample dicts.\n\n    Returns:\n        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches.\n    \"\"\"\n    dataset = datasets.get_dataset(\n        data, img_transform=img_tr, seg_transform=seg_tr, sample_transform=sample_tr\n    )\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        drop_last=False,\n        num_workers=n_workers,\n        persistent_workers=n_workers &gt; 0,\n        shuffle=False,\n        pin_memory=False,\n    )\n    return dataloader\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.pixel_to_patch_labels","title":"<code>pixel_to_patch_labels(seg, n_patches, patch_size, pixel_agg='majority', bg_label=0, max_classes=256)</code>","text":"<p>Convert pixel-level segmentation to patch-level labels using vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>seg</code> <code>Image</code> <p>Pixel-level segmentation mask as PIL Image</p> required <code>n_patches</code> <code>int</code> <p>Total number of patches expected</p> required <code>patch_size</code> <code>int</code> <p>Size of each patch in pixels</p> required <code>pixel_agg</code> <code>Literal['majority', 'prefer-fg']</code> <p>How to aggregate pixel labels into patch labels</p> <code>'majority'</code> <code>bg_label</code> <code>int</code> <p>Background label index</p> <code>0</code> <code>max_classes</code> <code>int</code> <p>Maximum number of classes (for bincount)</p> <code>256</code> <p>Returns:</p> Type Description <code>UInt8[Tensor, ' n_patches']</code> <p>Patch labels as uint8 tensor of shape (n_patches,)</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef pixel_to_patch_labels(\n    seg: Image.Image,\n    n_patches: int,\n    patch_size: int,\n    pixel_agg: tp.Literal[\"majority\", \"prefer-fg\"] = \"majority\",\n    bg_label: int = 0,\n    max_classes: int = 256,\n) -&gt; UInt8[Tensor, \" n_patches\"]:\n    \"\"\"\n    Convert pixel-level segmentation to patch-level labels using vectorized operations.\n\n    Args:\n        seg: Pixel-level segmentation mask as PIL Image\n        n_patches: Total number of patches expected\n        patch_size: Size of each patch in pixels\n        pixel_agg: How to aggregate pixel labels into patch labels\n        bg_label: Background label index\n        max_classes: Maximum number of classes (for bincount)\n\n    Returns:\n        Patch labels as uint8 tensor of shape (n_patches,)\n    \"\"\"\n    # Convert to torch tensor for vectorized operations\n    seg_tensor = torch.from_numpy(np.array(seg, dtype=np.uint8))\n    assert seg_tensor.ndim == 2\n\n    h, w = seg_tensor.shape\n\n    # Calculate patch grid dimensions\n    patch_grid_h = h // patch_size\n    patch_grid_w = w // patch_size\n    assert patch_grid_w * patch_grid_h == n_patches, (\n        f\"Image size {w}x{h} with patch_size {patch_size} gives {patch_grid_w}x{patch_grid_h} = {patch_grid_w * patch_grid_h} patches, expected {n_patches}\"\n    )\n\n    # Reshape into patches using einops: (n_patches, patch_size * patch_size)\n    patches = einops.rearrange(\n        seg_tensor,\n        \"(h p1) (w p2) -&gt; (h w) (p1 p2)\",\n        p1=patch_size,\n        p2=patch_size,\n        h=patch_grid_h,\n        w=patch_grid_w,\n    )\n\n    # Use vectorized bincount approach to get class counts for all patches at once\n    # counts[i, c] = number of times class c appears in patch i\n    offsets = torch.arange(n_patches, device=patches.device).unsqueeze(1) * max_classes\n    flat = (patches + offsets).reshape(-1)\n    counts = torch.bincount(flat, minlength=n_patches * max_classes).reshape(\n        n_patches, max_classes\n    )\n\n    if pixel_agg == \"majority\":\n        # Take the most common label in each patch\n        patch_labels = counts.argmax(dim=1)\n    elif pixel_agg == \"prefer-fg\":\n        # Take the most common non-background label, or background if all background\n        nonbg = counts.clone()\n        nonbg[:, bg_label] = 0\n        has_nonbg = nonbg.sum(dim=1) &gt; 0\n        nonbg_arg = nonbg.argmax(dim=1)\n        bg_tensor = torch.full_like(nonbg_arg, bg_label)\n        patch_labels = torch.where(has_nonbg, nonbg_arg, bg_tensor)\n    else:\n        tp.assert_never(pixel_agg)\n\n    return patch_labels.to(torch.uint8)\n</code></pre>"},{"location":"api/data/shards/#saev.data.shards.worker_fn","title":"<code>worker_fn(*, family, ckpt, content_tokens_per_example, cls_token, d_model, layers, data, batch_size, n_workers, max_tokens_per_shard, shards_root, device, pixel_agg=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>family</code> <code>str</code> <p>Transformer family (dinov2, dinov3, clip, etc).</p> required <code>ckpt</code> <code>str</code> <p>Transformer ckpt (hf-hub:imageomics/bioclip2, etc).</p> required <code>content_tokens_per_example</code> <code>int</code> <p>Number of content tokens per example.</p> required <code>cls_token</code> <code>bool</code> <p>Whether the transformer has a [CLS] token.</p> required <code>d_model</code> <code>int</code> <p>Hidden dimension of transformer.</p> required <code>layers</code> <code>list[int]</code> <p>The layers to record activations for.</p> required <code>data</code> <code>Config</code> <p>Config for the particular (image) dataset to load.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the dataset.</p> required <code>n_workers</code> <code>int</code> <p>Number of workers for loading examples fromm the dataset.</p> required <code>max_tokens_per_shard</code> <code>int</code> <p>Maximum number of tokens per disk shard.</p> required <code>pixel_agg</code> <code>Literal['majority', 'prefer-fg', None]</code> <p>Optional method for aggregating segmentation label pixels.</p> <code>None</code> <code>shards_root</code> <code>Path</code> <p>Where to save shards. Should end with 'shards'. See disk-layout.md; this is $SAEV_SCRATCH/saev/shards.</p> required <code>device</code> <code>str</code> <p>Device for doing the computation.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the shards directory.</p> Source code in <code>src/saev/data/shards.py</code> <pre><code>@beartype.beartype\ndef worker_fn(\n    *,\n    family: str,\n    ckpt: str,\n    content_tokens_per_example: int,\n    cls_token: bool,\n    d_model: int,\n    layers: list[int],\n    data: datasets.Config,\n    batch_size: int,\n    n_workers: int,\n    max_tokens_per_shard: int,\n    shards_root: pathlib.Path,\n    device: str,\n    pixel_agg: tp.Literal[\"majority\", \"prefer-fg\", None] = None,\n) -&gt; pathlib.Path:\n    \"\"\"\n    Args:\n        family: Transformer family (dinov2, dinov3, clip, etc).\n        ckpt: Transformer ckpt (hf-hub:imageomics/bioclip2, etc).\n        content_tokens_per_example: Number of content tokens per example.\n        cls_token: Whether the transformer has a [CLS] token.\n        d_model: Hidden dimension of transformer.\n        layers: The layers to record activations for.\n        data: Config for the particular (image) dataset to load.\n        batch_size: Batch size for the dataset.\n        n_workers: Number of workers for loading examples fromm the dataset.\n        max_tokens_per_shard: Maximum number of tokens per disk shard.\n        pixel_agg: Optional method for aggregating segmentation label pixels.\n        shards_root: Where to save shards. Should end with 'shards'. See [disk-layout.md](../../developers/disk-layout.md); this is $SAEV_SCRATCH/saev/shards.\n        device: Device for doing the computation.\n\n    Returns:\n        Path to the shards directory.\n    \"\"\"\n    from saev import helpers\n    from saev.data import models\n\n    if torch.cuda.is_available():\n        # This enables tf32 on Ampere GPUs which is only 8% slower than\n        # float16 and almost as accurate as float32\n        # This was a default in pytorch until 1.12\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n\n    log_format = \"[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=log_format)\n    logger = logging.getLogger(\"worker_fn\")\n\n    if device == \"cuda\" and not torch.cuda.is_available():\n        logger.warning(\"No CUDA device available, using CPU.\")\n        device = \"cpu\"\n\n    assert shards_root.name == \"shards\"\n\n    # Convert every pathlib.Path to a str.\n    md_data = {**dataclasses.asdict(data), \"__class__\": data.__class__.__name__}\n    md_data = {\n        key: str(value) if isinstance(value, pathlib.Path) else value\n        for key, value in md_data.items()\n    }\n\n    md = Metadata(\n        family=family,\n        ckpt=ckpt,\n        layers=tuple(layers),\n        content_tokens_per_example=content_tokens_per_example,\n        cls_token=cls_token,\n        d_model=d_model,\n        n_examples=data.n_examples,\n        max_tokens_per_shard=max_tokens_per_shard,\n        data=md_data,\n        dataset=data.root,\n        pixel_agg=pixel_agg if datasets.is_img_seg_dataset(data) else None,\n    )\n    model_cls = models.load_model_cls(family)\n    model_instance = model_cls(ckpt).to(device)\n    model = RecordedTransformer(\n        model_instance, content_tokens_per_example, cls_token, layers\n    )\n\n    img_tr, sample_tr = model_cls.make_transforms(ckpt, content_tokens_per_example)\n\n    seg_tr = None\n    if datasets.is_img_seg_dataset(data):\n        # For image segmentation datasets, create a transform that converts pixels to patches\n        # Use make_resize with NEAREST interpolation for segmentation masks\n        seg_resize_tr = model_cls.make_resize(\n            ckpt, content_tokens_per_example, scale=1.0, resample=Image.NEAREST\n        )\n\n        def seg_to_patches(seg):\n            \"\"\"Transform that resizes segmentation and converts to patch labels.\"\"\"\n\n            # Convert to patch labels\n            return pixel_to_patch_labels(\n                seg_resize_tr(seg),\n                content_tokens_per_example,\n                patch_size=model_instance.patch_size,\n                pixel_agg=pixel_agg,\n                bg_label=data.bg_label,\n            )\n\n        seg_tr = seg_to_patches\n\n    dataloader = get_dataloader(\n        data,\n        batch_size=batch_size,\n        n_workers=n_workers,\n        img_tr=img_tr,\n        seg_tr=seg_tr,\n        sample_tr=sample_tr,\n    )\n\n    n_batches = math.ceil(data.n_examples / batch_size)\n    logger.info(\"Dumping %d batches of %d examples.\", n_batches, batch_size)\n\n    model = model.to(device)\n\n    md.dump(shards_root)\n\n    # Use context manager for proper cleanup\n    with ShardWriter(shards_root, md) as writer:\n        i = 0\n        # Calculate and write transformer activations.\n        with torch.inference_mode():\n            for batch in helpers.progress(dataloader, total=n_batches):\n                imgs = batch.get(\"image\").to(device)\n                grid = batch.get(\"grid\")\n                if grid is not None:\n                    grid = grid.to(device)\n                    out, cache = model(imgs, grid=grid)\n                else:\n                    out, cache = model(imgs)\n                # cache has shape [batch size, n layers, n patches + 1, d model]\n                del out\n\n                # Write activations and labels (if present) in one call\n                patch_labels = batch.get(\"patch_labels\")\n                if patch_labels is not None:\n                    logger.debug(\n                        \"Found patch_labels in batch: shape=%s\",\n                        patch_labels.shape\n                        if hasattr(patch_labels, \"shape\")\n                        else \"unknown\",\n                    )\n                    # Ensure correct shape\n                    assert patch_labels.shape == (\n                        len(cache),\n                        content_tokens_per_example,\n                    )\n                else:\n                    logger.debug(f\"No patch_labels in batch. Keys: {batch.keys()}\")\n\n                writer.write_batch(cache, i, patch_labels=patch_labels)\n\n                i += len(cache)\n\n    return shards_root / md.hash\n</code></pre>"},{"location":"api/data/shuffled/","title":"saev.data.shuffled","text":""},{"location":"api/data/shuffled/#saev.data.shuffled.Config","title":"<code>Config(shards=pathlib.Path('$SAEV_SCRATCH/saev/shards/abcdefg'), tokens='content', layer=-2, batch_size=1024 * 16, drop_last=False, scale_norm=False, ignore_labels=list(), n_threads=4, buffer_size=64, batch_timeout_s=30.0, seed=17, debug=False, log_every_s=30.0)</code>  <code>dataclass</code>","text":"<p>Configuration for loading shuffled activation data from disk.</p> <p>Attributes:</p> Name Type Description <code>shards</code> <code>Path</code> <p>Directory with .bin shards and a metadata.json file.</p> <code>tokens</code> <code>Literal['special', 'content', 'all']</code> <p>Which subset of tokens to use. 'special' indicates the special tokens (if any). 'content' indicates it will return content tokens. 'all' returns all tokens.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_size","title":"<code>batch_size = 1024 * 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.batch_timeout_s","title":"<code>batch_timeout_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How long to wait for at least one batch.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.buffer_size","title":"<code>buffer_size = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.debug","title":"<code>debug = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the dataloader process should log debug messages.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.drop_last","title":"<code>drop_last = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to drop the last batch if it's smaller than the others.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.ignore_labels","title":"<code>ignore_labels = dataclasses.field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If provided, exclude tokens with these label values. None means no filtering. Common use: ignore_labels=[0] to exclude background.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.layer","title":"<code>layer = -2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Which transformer layer(s) to read from disk. <code>-2</code> selects the second-to-last layer. <code>\"all\"</code> enumerates every recorded layer.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.log_every_s","title":"<code>log_every_s = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How frequently the dataloader process should log (debug) performance messages.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.n_threads","title":"<code>n_threads = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of dataloading threads.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.scale_norm","title":"<code>scale_norm = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to scale norms to sqrt(D).</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.Config.seed","title":"<code>seed = 17</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader","title":"<code>DataLoader(cfg)</code>","text":"<p>High-throughput streaming loader that deterministically shuffles data from disk shards.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __init__(self, cfg: Config):\n    self.cfg = cfg\n\n    self.manager_proc = None\n    self.reservoir = None\n    self.stop_event = None\n\n    self.logger = logging.getLogger(\"shuffled.DataLoader\")\n    self.ctx = mp.get_context()\n\n    if not os.path.isdir(self.cfg.shards):\n        raise RuntimeError(f\"Activations are not saved at '{self.cfg.shards}'.\")\n\n    if self.cfg.scale_norm:\n        raise NotImplementedError(\"scale_norm not implemented.\")\n\n    self.metadata = shards.Metadata.load(self.cfg.shards)\n\n    # Validate shard files exist\n    shard_info = shards.ShardInfo.load(self.cfg.shards)\n    for shard in shard_info:\n        shard_path = os.path.join(self.cfg.shards, shard.name)\n        if not os.path.exists(shard_path):\n            raise FileNotFoundError(f\"Shard file not found: {shard_path}\")\n\n    self._n_samples = self._calculate_n_samples()\n\n    # Check if labels.bin exists for filtering\n    self.labels_mmap = None\n    if self.cfg.ignore_labels:\n        labels_path = os.path.join(self.cfg.shards, \"labels.bin\")\n        if not os.path.exists(labels_path):\n            raise FileNotFoundError(\n                f\"ignore_labels filtering requested but labels.bin not found at {labels_path}\"\n            )\n</code></pre>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.ExampleBatch","title":"<code>ExampleBatch</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Individual example.</p>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Yields batches.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __iter__(self) -&gt; collections.abc.Iterator[ExampleBatch]:\n    \"\"\"Yields batches.\"\"\"\n    self._start_manager()\n    n, b = 0, 0\n\n    try:\n        while n &lt; self.n_samples:\n            need = min(self.cfg.batch_size, self.n_samples - n)\n            if not self.err_queue.empty():\n                who, tb = self.err_queue.get_nowait()\n                raise RuntimeError(f\"{who} crashed:\\n{tb}\")\n\n            try:\n                act, meta = self.reservoir.get(\n                    need, timeout=self.cfg.batch_timeout_s\n                )\n                n += need\n                b += 1\n                example_idx, token_idx = meta.T\n                yield self.ExampleBatch(\n                    act=act, example_idx=example_idx, token_idx=token_idx\n                )\n                continue\n            except TimeoutError:\n                if self.cfg.ignore_labels:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds. This can happen when filtering out many labels.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n                else:\n                    self.logger.info(\n                        \"Did not get a batch from %d worker threads in %.1fs seconds.\",\n                        self.cfg.n_threads,\n                        self.cfg.batch_timeout_s,\n                    )\n\n            # If we don't continue, then we should check on the manager process.\n            if not self.manager_proc.is_alive():\n                raise RuntimeError(\n                    f\"Manager process died unexpectedly after {b}/{len(self)} batches.\"\n                )\n\n    finally:\n        self.shutdown()\n</code></pre>"},{"location":"api/data/shuffled/#saev.data.shuffled.DataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of batches in an epoch.</p> Source code in <code>src/saev/data/shuffled.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of batches in an epoch.\"\"\"\n    return math.ceil(self.n_samples / self.cfg.batch_size)\n</code></pre>"},{"location":"api/data/siglip/","title":"saev.data.siglip","text":""},{"location":"api/data/siglip/#saev.data.siglip.Vit","title":"<code>Vit(ckpt)</code>","text":"<p>               Bases: <code>Module</code>, <code>VisionTransformer</code></p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>def __init__(self, ckpt: str):\n    super().__init__()\n\n    if ckpt.startswith(\"hf-hub:\"):\n        clip, _ = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        clip, _ = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    self._ckpt = ckpt\n\n    model = clip.visual\n    model.proj = None\n    model.output_tokens = True  # type: ignore\n    self.model = model\n\n    assert isinstance(self.model, open_clip.timm_model.TimmModel)\n</code></pre>"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_resize","title":"<code>make_resize(ckpt, n_patches_per_img=-1, *, scale=1.0, resample=Image.LANCZOS)</code>  <code>staticmethod</code>","text":"<p>Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.</p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>@staticmethod\ndef make_resize(\n    ckpt: str,\n    n_patches_per_img: int = -1,\n    *,\n    scale: float = 1.0,\n    resample: Image.Resampling = Image.LANCZOS,\n) -&gt; Callable[[Image.Image], Image.Image]:\n    \"\"\"Create resize transform for visualization. Use resample=Image.NEAREST for segmentation masks.\"\"\"\n    from PIL import Image\n\n    def resize(img: Image.Image) -&gt; Image.Image:\n        # SigLIP typically uses 224x224 or 384x384 images\n        # We'll assume 224x224 for simplicity\n        resize_size_px = (int(224 * scale), int(224 * scale))\n        return img.resize(resize_size_px, resample=resample)\n\n    return resize\n</code></pre>"},{"location":"api/data/siglip/#saev.data.siglip.Vit.make_transforms","title":"<code>make_transforms(ckpt, n_patches_per_img)</code>  <code>staticmethod</code>","text":"<p>Create transforms for preprocessing: (img_transform, sample_transform | None).</p> Source code in <code>src/saev/data/siglip.py</code> <pre><code>@staticmethod\ndef make_transforms(\n    ckpt: str, n_patches_per_img: int\n) -&gt; tuple[Callable, Callable | None]:\n    \"\"\"Create transforms for preprocessing: (img_transform, sample_transform | None).\"\"\"\n    if ckpt.startswith(\"hf-hub:\"):\n        _, img_transform = open_clip.create_model_from_pretrained(\n            ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    else:\n        arch, ckpt = ckpt.split(\"/\")\n        _, img_transform = open_clip.create_model_from_pretrained(\n            arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()\n        )\n    return img_transform, None\n</code></pre>"},{"location":"api/data/transforms/","title":"saev.data.transforms","text":""},{"location":"api/data/transforms/#saev.data.transforms.conv2d_to_tokens","title":"<code>conv2d_to_tokens(x_bchw, conv)</code>","text":"<p>Conv2d then flatten spatial to L, return (B, L, D).</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef conv2d_to_tokens(\n    x_bchw: Float[Tensor, \"b c h w\"], conv: nn.Conv2d\n) -&gt; Float[Tensor, \"b n d\"]:\n    \"\"\"Conv2d then flatten spatial to L, return (B, L, D).\"\"\"\n    y_bdhw = conv(x_bchw)\n    return einops.rearrange(y_bdhw, \"b d h w -&gt; b (h w) d\")\n</code></pre>"},{"location":"api/data/transforms/#saev.data.transforms.resize_to_patch_grid","title":"<code>resize_to_patch_grid(img, *, p, n, resample=Image.LANCZOS)</code>","text":"<p>Resize image to (w, h) so that:   - w % p == 0, h % p == 0   - (h/p) * (w/p) == N   - Minimizes change in aspect ratio.</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@beartype.beartype\ndef resize_to_patch_grid(\n    img: Image.Image,\n    *,\n    p: int,\n    n: int,\n    resample: Image.Resampling | int = Image.LANCZOS,\n) -&gt; Image.Image:\n    \"\"\"\n    Resize image to (w, h) so that:\n      - w % p == 0, h % p == 0\n      - (h/p) * (w/p) == N\n      - Minimizes change in aspect ratio.\n    \"\"\"\n    if p &lt;= 0 or n &lt;= 0:\n        raise ValueError(\"p and n must be positive integers\")\n\n    w0, h0 = img.size\n    a0 = w0 / h0\n\n    # Find the aspect ratio closest to a0\n    best_c = 0\n    best_dist = float(\"inf\")\n    for i in range(1, int(math.sqrt(n) + 1)):\n        if n % i != 0:\n            continue\n\n        for d in (i, n // i):\n            c, r = d, n // d\n            aspect = c / r\n            dist = abs(aspect - a0)\n\n            if dist &lt; best_dist:\n                best_c = d\n                best_dist = dist\n\n    c = best_c\n    r = n // c\n    w, h = c * p, r * p\n    return img.resize((w, h), resample=resample)\n</code></pre>"},{"location":"api/data/transforms/#saev.data.transforms.unfolded_conv2d","title":"<code>unfolded_conv2d(x_bchw, conv)</code>","text":"<p>Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels. Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1.</p> Source code in <code>src/saev/data/transforms.py</code> <pre><code>@jaxtyped(typechecker=beartype.beartype)\ndef unfolded_conv2d(\n    x_bchw: Float[Tensor, \"b c h w\"], conv: nn.Conv2d\n) -&gt; Float[Tensor, \"b n d\"]:\n    \"\"\"\n    Returns tokens shaped (B, L, D), where L = (H/k)*(W/k), D = conv.out_channels.\n    Requires: stride == kernel_size, padding == 0, groups == 1, dilation == 1.\n    \"\"\"\n    k = conv.kernel_size[0]\n\n    assert conv.kernel_size == (k, k)\n    assert conv.stride == (k, k)\n    assert conv.padding == (0, 0)\n    assert conv.groups == 1\n    assert conv.dilation == (1, 1)\n\n    *b, c, h, w = x_bchw.shape\n\n    assert h % k == 0 and w % k == 0\n\n    tokens_bnd = einops.rearrange(\n        x_bchw, \"b c (hp p1) (wp p2) -&gt; b (hp wp) (c p1 p2)\", p1=k, p2=k\n    ).contiguous()\n    w_dp = conv.weight.reshape(conv.out_channels, c * k * k)\n    tokens_bnd = tokens_bnd @ w_dp.T\n    if conv.bias is not None:\n        tokens_bnd = tokens_bnd + conv.bias[None, None, :]\n    return tokens_bnd\n</code></pre>"},{"location":"api/nn/modeling/","title":"saev.nn.modeling","text":"<p>Neural network architectures for sparse autoencoders.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK","title":"<code>BatchTopK(top_k=32)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopK.top_k","title":"<code>top_k = 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many values are allowed to be non-zero per sample in the batch.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation","title":"<code>BatchTopKActivation(cfg=BatchTopK())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Batch Top-K activation function. For use as activation function of sparse encoder. Applies top-k selection per sample in the batch.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: BatchTopK = BatchTopK()):\n    super().__init__()\n    self.cfg = cfg\n    self.k = cfg.top_k\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.BatchTopKActivation.forward","title":"<code>forward(x)</code>","text":"<p>Apply top-k activation to each sample in the batch.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(self, x: Float[Tensor, \"batch d_sae\"]) -&gt; Float[Tensor, \"batch d_sae\"]:\n    \"\"\"\n    Apply top-k activation to each sample in the batch.\n    \"\"\"\n    if self.k &lt;= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # Handle case where k exceeds number of elements per sample\n    k = min(self.k, x.shape[-1])\n\n    # Apply top-k per sample (along the last dimension)\n    k_vals, k_inds = torch.topk(x, k, dim=-1, sorted=False)\n    mask = torch.zeros_like(x).scatter_(\n        dim=-1, index=k_inds, src=torch.ones_like(x)\n    )\n\n    return torch.mul(mask, x)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.Relu","title":"<code>Relu()</code>  <code>dataclass</code>","text":"<p>Vanilla ReLU</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder","title":"<code>SparseAutoencoder(cfg)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse auto-encoder (SAE) using L1 sparsity penalty.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: SparseAutoencoderConfig):\n    super().__init__()\n\n    self.cfg = cfg\n    self.logger = logging.getLogger(f\"sae(seed={cfg.seed})\")\n\n    self.W_enc = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_model, cfg.d_sae))\n    )\n    self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))\n\n    self.W_dec = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_model))\n    )\n    self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_model))\n\n    self.normalize_w_dec()\n\n    self.activation = get_activation(cfg.activation)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.decode","title":"<code>decode(f_x, *, prefixes=None)</code>","text":"<p>Decode latent features to reconstructions.</p> <p>Parameters:</p> Name Type Description Default <code>f_x</code> <code>Float[Tensor, 'batch d_sae']</code> <p>Latent features of shape (batch, d_sae)</p> required <code>prefixes</code> <code>Int64[Tensor, ' n_prefixes'] | None</code> <p>Optional tensor of prefix lengths for Matryoshka decoding.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_prefixes d_model']</code> <p>Matryoshka reconstructions (batch, n_prefixes, d_model).</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def decode(\n    self,\n    f_x: Float[Tensor, \"batch d_sae\"],\n    *,\n    prefixes: Int64[Tensor, \" n_prefixes\"] | None = None,\n) -&gt; Float[Tensor, \"batch n_prefixes d_model\"]:\n    \"\"\"\n    Decode latent features to reconstructions.\n\n    Args:\n        f_x: Latent features of shape (batch, d_sae)\n        prefixes: Optional tensor of prefix lengths for Matryoshka decoding.\n\n    Returns:\n        Matryoshka reconstructions (batch, n_prefixes, d_model).\n    \"\"\"\n    b, d_sae = f_x.shape\n\n    # Matryoshka cumulative decode\n    device = f_x.device\n    if prefixes is None:\n        prefixes = torch.tensor([d_sae], dtype=torch.int64)\n    assert torch.all(prefixes[1:] &gt; prefixes[:-1])\n    assert 1 &lt;= int(prefixes[0]) and int(prefixes[-1]) == d_sae\n    prefixes = prefixes.to(device)\n\n    # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ...\n    block_indices = torch.cat([\n        torch.tensor([0], dtype=prefixes.dtype, device=device),\n        prefixes,\n    ])\n    blocks = list(zip(block_indices[:-1], block_indices[1:]))\n\n    # Compute block outputs\n    block_outputs = []\n    for i, (start, end) in enumerate(blocks):\n        # Each block uses its portion of f_x and W_dec\n        block_f_x = f_x[:, start:end]\n        block_W_dec = self.W_dec[start:end, :]\n\n        # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_model) -&gt; (batch, d_model)\n        # Note: W_dec is (d_sae, d_model), so block_W_dec is (block_size, d_model)\n        block_output = einops.einsum(\n            block_f_x,\n            block_W_dec,\n            \"... d_sae_block, d_sae_block d_model -&gt; ... d_model\",\n        )\n\n        # Add bias only to the first block\n        if i == 0:\n            block_output = block_output + self.b_dec\n\n        block_outputs.append(block_output)\n\n    # Cumulative sum to get prefix reconstructions\n    x_hats = torch.cumsum(torch.stack(block_outputs, dim=-2), dim=-2)\n\n    return x_hats\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Given x, calculates the reconstructed x_hat and the intermediate activations f_x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model']</code> <p>a batch of transformer activations.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, \"batch d_model\"]\n) -&gt; tuple[Float[Tensor, \"batch d_model\"], Float[Tensor, \"batch d_sae\"]]:\n    \"\"\"\n    Given x, calculates the reconstructed x_hat and the intermediate activations f_x.\n\n    Arguments:\n        x: a batch of transformer activations.\n    \"\"\"\n    f_x = self.encode(x)\n    x_hat = self.decode(f_x)\n\n    return x_hat, f_x\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.normalize_w_dec","title":"<code>normalize_w_dec()</code>","text":"<p>Set W_dec to unit-norm columns.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef normalize_w_dec(self):\n    \"\"\"\n    Set W_dec to unit-norm columns.\n    \"\"\"\n    if self.cfg.normalize_w_dec:\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoder.remove_parallel_grads","title":"<code>remove_parallel_grads()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_model) shape</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef remove_parallel_grads(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_model) shape\n    \"\"\"\n    if not self.cfg.remove_parallel_grads:\n        return\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_model, d_sae d_model -&gt; d_sae\",\n    )\n\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_model -&gt; d_sae d_model\",\n    )\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig(d_model=1024, exp_factor=16, n_reinit_samples=1024 * 16 * 32, remove_parallel_grads=True, normalize_w_dec=True, seed=0, activation=Relu())</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.exp_factor","title":"<code>exp_factor = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion factor for SAE.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.n_reinit_samples","title":"<code>n_reinit_samples = 1024 * 16 * 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.normalize_w_dec","title":"<code>normalize_w_dec = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.remove_parallel_grads","title":"<code>remove_parallel_grads = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.SparseAutoencoderConfig.seed","title":"<code>seed = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopK","title":"<code>TopK(top_k=32)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/modeling/#saev.nn.modeling.TopK.top_k","title":"<code>top_k = 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many values are allowed to be non-zero.</p>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation","title":"<code>TopKActivation(cfg=TopK())</code>","text":"<p>               Bases: <code>Module</code></p> <p>Top-K activation function. For use as activation function of sparse encoder.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: TopK = TopK()):\n    super().__init__()\n    self.cfg = cfg\n    self.k = cfg.top_k\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.TopKActivation.forward","title":"<code>forward(x)</code>","text":"<p>Apply top-k activation to the input tensor.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(self, x: Float[Tensor, \"batch d_sae\"]) -&gt; Float[Tensor, \"batch d_sae\"]:\n    \"\"\"\n    Apply top-k activation to the input tensor.\n    \"\"\"\n    if self.k &lt;= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    k_vals, k_inds = torch.topk(x, self.k, dim=-1, sorted=False)\n    mask = torch.zeros_like(x).scatter_(\n        dim=-1, index=k_inds, src=torch.ones_like(x)\n    )\n\n    return torch.mul(mask, x)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.dump","title":"<code>dump(fpath, sae)</code>","text":"<p>Save an SAE checkpoint to disk along with configuration, using the trick from equinox.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>Path | str</code> <p>filepath to save checkpoint to.</p> required <code>sae</code> <code>SparseAutoencoder</code> <p>sparse autoencoder checkpoint to save.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef dump(fpath: pathlib.Path | str, sae: SparseAutoencoder):\n    \"\"\"\n    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).\n\n    Arguments:\n        fpath: filepath to save checkpoint to.\n        sae: sparse autoencoder checkpoint to save.\n    \"\"\"\n    # Custom serialization to handle activation object\n    cfg_dict = dataclasses.asdict(sae.cfg)\n    # Replace activation dict with custom format\n    activation = sae.cfg.activation\n    cfg_dict[\"activation\"] = {\n        \"cls\": activation.__class__.__name__,\n        \"params\": dataclasses.asdict(activation),\n    }\n\n    header = {\n        \"schema\": 3,\n        \"cfg\": cfg_dict,\n        \"commit\": helpers.current_git_commit() or \"unknown\",\n        \"lib\": __version__,\n    }\n\n    fpath = pathlib.Path(fpath)\n    fpath.parent.mkdir(exist_ok=True, parents=True)\n    with open(fpath, \"wb\") as fd:\n        helpers.dump(header, fd, option=orjson.OPT_APPEND_NEWLINE)\n        torch.save(sae.state_dict(), fd)\n</code></pre>"},{"location":"api/nn/modeling/#saev.nn.modeling.load","title":"<code>load(fpath, *, device='cpu')</code>","text":"<p>Loads a sparse autoencoder from disk.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef load(fpath: pathlib.Path | str, *, device=\"cpu\") -&gt; SparseAutoencoder:\n    \"\"\"\n    Loads a sparse autoencoder from disk.\n    \"\"\"\n    with open(fpath, \"rb\") as fd:\n        header = json.loads(fd.readline())\n        buffer = io.BytesIO(fd.read())\n\n    if \"schema\" not in header:\n        # Original, pre-schema format: just raw config parameters\n        # Remove old parameters that no longer exist\n        for keyword in (\"sparsity_coeff\", \"ghost_grads\", \"l1_coeff\", \"use_ghost_grads\"):\n            header.pop(keyword, None)\n        # Legacy format - create SparseAutoencoderConfig with Relu activation\n        header[\"d_model\"] = header.pop(\"d_vit\")\n        cfg = SparseAutoencoderConfig(**header, activation=Relu())\n    elif header[\"schema\"] == 1:\n        # Schema version 1: A cautionary tale of poor version management\n        #\n        # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent!\n        #\n        # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.)\n        # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict\n        #\n        # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format.\n        #\n        # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number!\n\n        cls_name = header.get(\"cls\", \"SparseAutoencoderConfig\")\n        cfg_dict = header[\"cfg\"]\n\n        if cls_name in [\"Relu\", \"TopK\", \"BatchTopK\"]:\n            # Format 1A: Old format where cls indicates the activation type\n            activation_cls = globals()[cls_name]\n            if cls_name in [\"TopK\", \"BatchTopK\"]:\n                activation = activation_cls(top_k=cfg_dict.get(\"top_k\", 32))\n            else:\n                activation = activation_cls()\n            cfg = SparseAutoencoderConfig(**cfg_dict, activation=activation)\n        else:\n            # Format 1B: Newer format with activation as dict\n            if \"activation\" in cfg_dict:\n                activation_info = cfg_dict[\"activation\"]\n                activation_cls = globals()[activation_info[\"cls\"]]\n                activation = activation_cls(**activation_info[\"params\"])\n                cfg_dict[\"activation\"] = activation\n            cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 2:\n        # Schema version 2: cleaner format with activation serialization\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 3:\n        # Schema version 3: Uses d_model\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    else:\n        raise ValueError(f\"Unknown schema version: {header['schema']}\")\n\n    model = SparseAutoencoder(cfg)\n    model.load_state_dict(torch.load(buffer, weights_only=True, map_location=device))\n    return model\n</code></pre>"},{"location":"api/nn/objectives/","title":"saev.nn.objectives","text":""},{"location":"api/nn/objectives/#saev.nn.objectives.Loss","title":"<code>Loss()</code>  <code>dataclass</code>","text":"<p>The loss term for an autoencoder training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Loss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka","title":"<code>Matryoshka(sparsity_coeff=0.0004, n_prefixes=10)</code>  <code>dataclass</code>","text":"<p>Config for the Matryoshka loss for another arbitrary SAE class.</p> <p>Reference code is here: https://github.com/noanabeshima/matryoshka-saes and the original reading is https://sparselatents.com/matryoshka.html and https://arxiv.org/pdf/2503.17547</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.n_prefixes","title":"<code>n_prefixes = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of random length prefixes to use for loss calculation.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.Matryoshka.sparsity_coeff","title":"<code>sparsity_coeff = 0.0004</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How much to weight sparsity loss term (if not using TopK/BatchTopK).</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss","title":"<code>MatryoshkaLoss(mse, sparsity, l0, l1)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Loss</code></p> <p>The composite loss terms for an training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l0","title":"<code>l0</code>  <code>instance-attribute</code>","text":"<p>Sum of L0 magnitudes of hidden activations for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.l1","title":"<code>l1</code>  <code>instance-attribute</code>","text":"<p>Sum of L1 magnitudes of hidden activations for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.mse","title":"<code>mse</code>  <code>instance-attribute</code>","text":"<p>Average of reconstruction loss (mean squared error) for all prefix lengths.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaLoss.sparsity","title":"<code>sparsity</code>  <code>instance-attribute</code>","text":"<p>Sparsity loss, typically lambda * L1.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.MatryoshkaObjective","title":"<code>MatryoshkaObjective(cfg)</code>","text":"<p>               Bases: <code>Objective</code></p> <p>Torch module for calculating the matryoshka loss for an SAE.</p> Source code in <code>src/saev/nn/objectives.py</code> <pre><code>def __init__(self, cfg: Matryoshka):\n    super().__init__()\n    self.cfg = cfg\n    # Keep sparsity_coeff as mutable attribute for scheduler compatibility\n    self.sparsity_coeff = cfg.sparsity_coeff\n</code></pre>"},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla","title":"<code>Vanilla(sparsity_coeff=0.0004)</code>  <code>dataclass</code>","text":""},{"location":"api/nn/objectives/#saev.nn.objectives.Vanilla.sparsity_coeff","title":"<code>sparsity_coeff = 0.0004</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How much to weight sparsity loss term.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss","title":"<code>VanillaLoss(mse, sparsity, l0, l1)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Loss</code></p> <p>The vanilla loss terms for an training batch.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l0","title":"<code>l0</code>  <code>instance-attribute</code>","text":"<p>L0 magnitude of hidden activations.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.l1","title":"<code>l1</code>  <code>instance-attribute</code>","text":"<p>L1 magnitude of hidden activations.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Total loss.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.mse","title":"<code>mse</code>  <code>instance-attribute</code>","text":"<p>Reconstruction loss (mean squared error).</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.VanillaLoss.sparsity","title":"<code>sparsity</code>  <code>instance-attribute</code>","text":"<p>Sparsity loss, typically lambda * L1.</p>"},{"location":"api/nn/objectives/#saev.nn.objectives.sample_prefixes","title":"<code>sample_prefixes(d_sae, n_prefixes, min_prefix_length=1, pareto_power=0.5)</code>","text":"<p>Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547)</p> <p>Parameters:</p> Name Type Description Default <code>d_sae</code> <code>int</code> <p>Total number of latent dimensions</p> required <code>n_prefixes</code> <code>int</code> <p>Number of prefixes to sample</p> required <code>min_prefix_length</code> <code>int</code> <p>Minimum length of any prefix</p> <code>1</code> <code>pareto_power</code> <code>float</code> <p>Power parameter for Pareto distribution (lower = more uniform)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Int64[Tensor, ' n_prefixes']</code> <p>torch.Tensor: Sorted prefix lengths</p> Source code in <code>src/saev/nn/objectives.py</code> <pre><code>@torch.no_grad()\n@jaxtyped(typechecker=beartype.beartype)\ndef sample_prefixes(\n    d_sae: int, n_prefixes: int, min_prefix_length: int = 1, pareto_power: float = 0.5\n) -&gt; Int64[Tensor, \" n_prefixes\"]:\n    \"\"\"\n    Samples prefix lengths using a Pareto distribution. Derived from \"Learning Multi-Level Features with\n    Matryoshka Sparse Autoencoders\" (https://doi.org/10.48550/arXiv.2503.17547)\n\n    Args:\n        d_sae: Total number of latent dimensions\n        n_prefixes: Number of prefixes to sample\n        min_prefix_length: Minimum length of any prefix\n        pareto_power: Power parameter for Pareto distribution (lower = more uniform)\n\n    Returns:\n        torch.Tensor: Sorted prefix lengths\n    \"\"\"\n    if n_prefixes &lt;= 1:\n        return torch.tensor([d_sae], dtype=torch.int64)\n\n    assert n_prefixes &lt;= d_sae\n\n    # Calculate probability distribution favoring shorter prefixes\n    lengths = torch.arange(1, d_sae)\n    pareto_cdf = 1 - ((min_prefix_length / lengths.float()) ** pareto_power)\n    pareto_pdf = torch.cat([pareto_cdf[:1], pareto_cdf[1:] - pareto_cdf[:-1]])\n    probability_dist = pareto_pdf / pareto_pdf.sum()\n\n    # Sample and sort prefix lengths\n    sampled_indices = torch.multinomial(\n        probability_dist, num_samples=n_prefixes - 1, replacement=False\n    )\n\n    # Convert indices to actual prefix lengths\n    prefixes = lengths[sampled_indices]\n\n    # Add n_latents as the final prefix\n    prefixes = torch.cat((prefixes.detach().clone(), torch.tensor([d_sae])))\n\n    prefixes, _ = torch.sort(prefixes, descending=False)\n\n    return prefixes.to(torch.int64)\n</code></pre>"},{"location":"api/nn/saev.nn/","title":"saev.nn","text":""},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder","title":"<code>SparseAutoencoder(cfg)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse auto-encoder (SAE) using L1 sparsity penalty.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def __init__(self, cfg: SparseAutoencoderConfig):\n    super().__init__()\n\n    self.cfg = cfg\n    self.logger = logging.getLogger(f\"sae(seed={cfg.seed})\")\n\n    self.W_enc = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_model, cfg.d_sae))\n    )\n    self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))\n\n    self.W_dec = torch.nn.Parameter(\n        torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_model))\n    )\n    self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_model))\n\n    self.normalize_w_dec()\n\n    self.activation = get_activation(cfg.activation)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.decode","title":"<code>decode(f_x, *, prefixes=None)</code>","text":"<p>Decode latent features to reconstructions.</p> <p>Parameters:</p> Name Type Description Default <code>f_x</code> <code>Float[Tensor, 'batch d_sae']</code> <p>Latent features of shape (batch, d_sae)</p> required <code>prefixes</code> <code>Int64[Tensor, ' n_prefixes'] | None</code> <p>Optional tensor of prefix lengths for Matryoshka decoding.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'batch n_prefixes d_model']</code> <p>Matryoshka reconstructions (batch, n_prefixes, d_model).</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def decode(\n    self,\n    f_x: Float[Tensor, \"batch d_sae\"],\n    *,\n    prefixes: Int64[Tensor, \" n_prefixes\"] | None = None,\n) -&gt; Float[Tensor, \"batch n_prefixes d_model\"]:\n    \"\"\"\n    Decode latent features to reconstructions.\n\n    Args:\n        f_x: Latent features of shape (batch, d_sae)\n        prefixes: Optional tensor of prefix lengths for Matryoshka decoding.\n\n    Returns:\n        Matryoshka reconstructions (batch, n_prefixes, d_model).\n    \"\"\"\n    b, d_sae = f_x.shape\n\n    # Matryoshka cumulative decode\n    device = f_x.device\n    if prefixes is None:\n        prefixes = torch.tensor([d_sae], dtype=torch.int64)\n    assert torch.all(prefixes[1:] &gt; prefixes[:-1])\n    assert 1 &lt;= int(prefixes[0]) and int(prefixes[-1]) == d_sae\n    prefixes = prefixes.to(device)\n\n    # Build blocks from prefix cuts: [0, cut1), [cut1, cut2), ...\n    block_indices = torch.cat([\n        torch.tensor([0], dtype=prefixes.dtype, device=device),\n        prefixes,\n    ])\n    blocks = list(zip(block_indices[:-1], block_indices[1:]))\n\n    # Compute block outputs\n    block_outputs = []\n    for i, (start, end) in enumerate(blocks):\n        # Each block uses its portion of f_x and W_dec\n        block_f_x = f_x[:, start:end]\n        block_W_dec = self.W_dec[start:end, :]\n\n        # Compute block output: (batch, d_sae_block) @ (d_sae_block, d_model) -&gt; (batch, d_model)\n        # Note: W_dec is (d_sae, d_model), so block_W_dec is (block_size, d_model)\n        block_output = einops.einsum(\n            block_f_x,\n            block_W_dec,\n            \"... d_sae_block, d_sae_block d_model -&gt; ... d_model\",\n        )\n\n        # Add bias only to the first block\n        if i == 0:\n            block_output = block_output + self.b_dec\n\n        block_outputs.append(block_output)\n\n    # Cumulative sum to get prefix reconstructions\n    x_hats = torch.cumsum(torch.stack(block_outputs, dim=-2), dim=-2)\n\n    return x_hats\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Given x, calculates the reconstructed x_hat and the intermediate activations f_x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch d_model']</code> <p>a batch of transformer activations.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>def forward(\n    self, x: Float[Tensor, \"batch d_model\"]\n) -&gt; tuple[Float[Tensor, \"batch d_model\"], Float[Tensor, \"batch d_sae\"]]:\n    \"\"\"\n    Given x, calculates the reconstructed x_hat and the intermediate activations f_x.\n\n    Arguments:\n        x: a batch of transformer activations.\n    \"\"\"\n    f_x = self.encode(x)\n    x_hat = self.decode(f_x)\n\n    return x_hat, f_x\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.normalize_w_dec","title":"<code>normalize_w_dec()</code>","text":"<p>Set W_dec to unit-norm columns.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef normalize_w_dec(self):\n    \"\"\"\n    Set W_dec to unit-norm columns.\n    \"\"\"\n    if self.cfg.normalize_w_dec:\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoder.remove_parallel_grads","title":"<code>remove_parallel_grads()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_model) shape</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@torch.no_grad()\ndef remove_parallel_grads(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_model) shape\n    \"\"\"\n    if not self.cfg.remove_parallel_grads:\n        return\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_model, d_sae d_model -&gt; d_sae\",\n    )\n\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_model -&gt; d_sae d_model\",\n    )\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig","title":"<code>SparseAutoencoderConfig(d_model=1024, exp_factor=16, n_reinit_samples=1024 * 16 * 32, remove_parallel_grads=True, normalize_w_dec=True, seed=0, activation=Relu())</code>  <code>dataclass</code>","text":""},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.exp_factor","title":"<code>exp_factor = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Expansion factor for SAE.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.n_reinit_samples","title":"<code>n_reinit_samples = 1024 * 16 * 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.normalize_w_dec","title":"<code>normalize_w_dec = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.remove_parallel_grads","title":"<code>remove_parallel_grads = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic.</p>"},{"location":"api/nn/saev.nn/#saev.nn.SparseAutoencoderConfig.seed","title":"<code>seed = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed.</p>"},{"location":"api/nn/saev.nn/#saev.nn.dump","title":"<code>dump(fpath, sae)</code>","text":"<p>Save an SAE checkpoint to disk along with configuration, using the trick from equinox.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>Path | str</code> <p>filepath to save checkpoint to.</p> required <code>sae</code> <code>SparseAutoencoder</code> <p>sparse autoencoder checkpoint to save.</p> required Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef dump(fpath: pathlib.Path | str, sae: SparseAutoencoder):\n    \"\"\"\n    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).\n\n    Arguments:\n        fpath: filepath to save checkpoint to.\n        sae: sparse autoencoder checkpoint to save.\n    \"\"\"\n    # Custom serialization to handle activation object\n    cfg_dict = dataclasses.asdict(sae.cfg)\n    # Replace activation dict with custom format\n    activation = sae.cfg.activation\n    cfg_dict[\"activation\"] = {\n        \"cls\": activation.__class__.__name__,\n        \"params\": dataclasses.asdict(activation),\n    }\n\n    header = {\n        \"schema\": 3,\n        \"cfg\": cfg_dict,\n        \"commit\": helpers.current_git_commit() or \"unknown\",\n        \"lib\": __version__,\n    }\n\n    fpath = pathlib.Path(fpath)\n    fpath.parent.mkdir(exist_ok=True, parents=True)\n    with open(fpath, \"wb\") as fd:\n        helpers.dump(header, fd, option=orjson.OPT_APPEND_NEWLINE)\n        torch.save(sae.state_dict(), fd)\n</code></pre>"},{"location":"api/nn/saev.nn/#saev.nn.load","title":"<code>load(fpath, *, device='cpu')</code>","text":"<p>Loads a sparse autoencoder from disk.</p> Source code in <code>src/saev/nn/modeling.py</code> <pre><code>@beartype.beartype\ndef load(fpath: pathlib.Path | str, *, device=\"cpu\") -&gt; SparseAutoencoder:\n    \"\"\"\n    Loads a sparse autoencoder from disk.\n    \"\"\"\n    with open(fpath, \"rb\") as fd:\n        header = json.loads(fd.readline())\n        buffer = io.BytesIO(fd.read())\n\n    if \"schema\" not in header:\n        # Original, pre-schema format: just raw config parameters\n        # Remove old parameters that no longer exist\n        for keyword in (\"sparsity_coeff\", \"ghost_grads\", \"l1_coeff\", \"use_ghost_grads\"):\n            header.pop(keyword, None)\n        # Legacy format - create SparseAutoencoderConfig with Relu activation\n        header[\"d_model\"] = header.pop(\"d_vit\")\n        cfg = SparseAutoencoderConfig(**header, activation=Relu())\n    elif header[\"schema\"] == 1:\n        # Schema version 1: A cautionary tale of poor version management\n        #\n        # This schema version unfortunately has TWO incompatible formats because we made breaking changes without incrementing the schema version. This is exactly what schema versioning is supposed to prevent!\n        #\n        # Format 1A (original): cls field contains activation type (\"Relu\", \"TopK\", etc.)\n        # Format 1B (later): cls field is \"SparseAutoencoderConfig\" and activation is a dict\n        #\n        # The complex logic below exists to handle both formats. This should have been avoided by incrementing to schema version 2 when we changed the format.\n        #\n        # Apologies from Sam for this mess - proper schema versioning discipline would have prevented this confusing situation. Every breaking change should increment the version number!\n\n        cls_name = header.get(\"cls\", \"SparseAutoencoderConfig\")\n        cfg_dict = header[\"cfg\"]\n\n        if cls_name in [\"Relu\", \"TopK\", \"BatchTopK\"]:\n            # Format 1A: Old format where cls indicates the activation type\n            activation_cls = globals()[cls_name]\n            if cls_name in [\"TopK\", \"BatchTopK\"]:\n                activation = activation_cls(top_k=cfg_dict.get(\"top_k\", 32))\n            else:\n                activation = activation_cls()\n            cfg = SparseAutoencoderConfig(**cfg_dict, activation=activation)\n        else:\n            # Format 1B: Newer format with activation as dict\n            if \"activation\" in cfg_dict:\n                activation_info = cfg_dict[\"activation\"]\n                activation_cls = globals()[activation_info[\"cls\"]]\n                activation = activation_cls(**activation_info[\"params\"])\n                cfg_dict[\"activation\"] = activation\n            cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 2:\n        # Schema version 2: cleaner format with activation serialization\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    elif header[\"schema\"] == 3:\n        # Schema version 3: Uses d_model\n        cfg_dict = header[\"cfg\"]\n        activation_info = cfg_dict[\"activation\"]\n        activation_cls = globals()[activation_info[\"cls\"]]\n        activation = activation_cls(**activation_info[\"params\"])\n        cfg_dict[\"activation\"] = activation\n        cfg = SparseAutoencoderConfig(**cfg_dict)\n    else:\n        raise ValueError(f\"Unknown schema version: {header['schema']}\")\n\n    model = SparseAutoencoder(cfg)\n    model.load_state_dict(torch.load(buffer, weights_only=True, map_location=device))\n    return model\n</code></pre>"},{"location":"api/utils/saev.utils/","title":"saev.utils","text":""},{"location":"api/utils/scheduling/","title":"saev.utils.scheduling","text":""},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter","title":"<code>BatchLimiter(dataloader, n_samples)</code>","text":"<p>Limits the number of batches to only return <code>n_samples</code> total samples.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(self, dataloader: DataLoaderLike, n_samples: int):\n    self.dataloader = dataloader\n    self.n_samples = n_samples\n    self.batch_size = dataloader.batch_size\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.BatchLimiter.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Pass through attribute access to the wrapped dataloader.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Pass through attribute access to the wrapped dataloader.\"\"\"\n    # __getattr__ is only called when the attribute wasn't found on self\n    # So we delegate to the wrapped dataloader\n    try:\n        return getattr(self.dataloader, name)\n    except AttributeError:\n        # Re-raise with more context about where the attribute was not found\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object and its wrapped dataloader have no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.Warmup","title":"<code>Warmup(init, final, n_steps)</code>","text":"<p>               Bases: <code>Scheduler</code></p> <p>Linearly increases from <code>init</code> to <code>final</code> over <code>n_warmup_steps</code> steps.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(self, init: float, final: float, n_steps: int):\n    self.final = final\n    self.init = init\n    self.n_steps = n_steps\n    self._step = 0\n</code></pre>"},{"location":"api/utils/scheduling/#saev.utils.scheduling.WarmupCosine","title":"<code>WarmupCosine(init, n_warmup, peak, n_steps, final)</code>","text":"<p>               Bases: <code>Scheduler</code></p> <p>Linearly increases from <code>init</code> to <code>peak</code> over <code>n_warmup</code> steps, then decrease down to final using cosine decay over n_steps - n_warmup.</p> Source code in <code>src/saev/utils/scheduling.py</code> <pre><code>def __init__(\n    self, init: float, n_warmup: int, peak: float, n_steps: int, final: float\n):\n    self.init = init\n    self.peak = peak\n    self.final = final\n    self.n_warmup = n_warmup\n    self.n_steps = n_steps\n    self._step = 0\n</code></pre>"},{"location":"api/utils/statistics/","title":"saev.utils.statistics","text":""},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator","title":"<code>PercentileEstimator(percentile, total, lr=0.001, shape=())</code>","text":"Source code in <code>src/saev/utils/statistics.py</code> <pre><code>def __init__(\n    self,\n    percentile: float | int,\n    total: int,\n    lr: float = 1e-3,\n    shape: tuple[int, ...] = (),\n):\n    self.percentile = percentile\n    self.total = total\n    self.lr = lr\n\n    self._estimate = torch.zeros(shape)\n    self._step = 0\n</code></pre>"},{"location":"api/utils/statistics/#saev.utils.statistics.PercentileEstimator.update","title":"<code>update(x)</code>","text":"<p>Update the estimator with a new value.</p> <p>This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>The new value to incorporate into the estimation</p> required Source code in <code>src/saev/utils/statistics.py</code> <pre><code>def update(self, x):\n    \"\"\"\n    Update the estimator with a new value.\n\n    This method maintains the marker positions using the P2 algorithm rules. When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.\n\n    Arguments:\n        x: The new value to incorporate into the estimation\n    \"\"\"\n    self._step += 1\n\n    step_size = self.lr * (self.total - self._step) / self.total\n\n    # Is a no-op if it's already on the same device.\n    if isinstance(x, Tensor):\n        self._estimate = self._estimate.to(x.device)\n\n    self._estimate += step_size * (\n        torch.sign(x - self._estimate) + 2 * self.percentile / 100 - 1.0\n    )\n</code></pre>"},{"location":"api/utils/wandb/","title":"saev.utils.wandb","text":""},{"location":"api/utils/wandb/#saev.utils.wandb.ParallelWandbRun","title":"<code>ParallelWandbRun(project, cfgs, mode, tags, dir='.wandb')</code>","text":"<p>Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387</p> Source code in <code>src/saev/utils/wandb.py</code> <pre><code>def __init__(\n    self,\n    project: str,\n    cfgs: list[dict[str, object]],\n    mode: str,\n    tags: list[str],\n    dir: str = \".wandb\",\n):\n    cfg, *cfgs = cfgs\n    self.project = project\n    self.cfgs = cfgs\n    self.mode = mode\n    self.tags = tags\n    self.dir = dir\n\n    self.live_run = wandb.init(\n        project=project, config=cfg, mode=mode, tags=tags, dir=dir\n    )\n\n    self.metric_queues: list[MetricQueue] = [[] for _ in self.cfgs]\n</code></pre>"},{"location":"developers/contributing/","title":"Contributing","text":""},{"location":"developers/contributing/#project-layout","title":"Project layout","text":"<pre><code>docs/\n    mkdocs.yml    # The configuration file.\n    src/\n        index.md  # The documentation homepage.\n        ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"developers/disk-layout/","title":"Storage &amp; Run Manifest Spec (v1)","text":"<p>There are two main locations:</p> <ol> <li><code>$SAEV_SCRATCH/saev/shards</code>: where we store transformer activations (referred to as <code>shards_root</code> in the codebase).</li> <li><code>$SAEV_NFS/saev/runs</code>: where we store checkpoints and other computed intermediate stuff like example images, probe1d results, etc. (referred to as <code>runs_root</code> in the codebase).</li> </ol> <p>Visually, these are:</p> <pre><code>$SAEV_SCRATCH/saev/\n  shards/\n    &lt;shard_hash&gt;/\n      metadata.json\n      shards.json\n      acts000000.bin\n      acts000001.bin\n      ...\n      labels.bin\n</code></pre> <p>and</p> <pre><code>$SAEV_NFS/saev/\n  runs/\n    &lt;run_id&gt;/\n      checkpoint/           # output of train.py on &lt;shard_hash&gt;\n        sae.pt\n        config.json\n      links/                # Symlinks\n        train-shards        # $SCRATCH/saev/shards/&lt;shard_hash&gt;\n        train-dataset       # Whatever the original image dataset was\n        val-shards          # $SCRATCH/saev/shards/&lt;shard_hash&gt;\n        val-dataset         # Whatever the original image dataset was\n      inference/            # outputs from dump.py\n        &lt;shard_hash&gt;/\n          config.json\n          patch_acts.npz\n          visuals/          # output of visuals.py\n</code></pre> <p>Each <code>$SAEV_SCRATCH/shards/&lt;shard_hash&gt;/</code> MUST include:</p> <ul> <li><code>metadata.json</code> (UTF-8, canonical spec; see <code>protocol.md</code>)</li> <li><code>shards.json</code> (UTF-8, shard index and sizes; see <code>protocol.md</code>)</li> <li><code>acts*.bin</code> (binary shards; format in <code>protocol.md</code>)</li> <li><code>labels.bin</code> (binary patch labels aligned to shards; format in <code>protocol.md</code>)</li> </ul> <p>Note</p> <p>Immutability: Files under <code>saev/shards/&lt;shard_hash&gt;/</code> MUST be treated as read-only after publication. Any change yields a new <code>shard_hash</code>.</p> <p>All CLI entrypoints should accept a single <code>--run &lt;path&gt;</code> argument. Every other path MUST be resolved from the run root:</p> <ul> <li>ViT activations: <code>links/shards</code> \u2192 <code>saev/shards/&lt;shard_hash&gt;</code></li> <li>Dataset: <code>links/dataset</code> \u2192 Dataset root, wherever it is on disk.</li> <li>SAE checkpoint: <code>checkpoint/sae.pt</code></li> </ul> <p>Example resolution:</p> <pre><code>run = pathlib.Path(cfg.run)\nshards_root = (run / \"links\" / \"shards\").resolve()\ndataset_root = (run / \"links\" / \"dataset\").resolve()\nckpt = run / \"checkpoint\" / \"sae.pt\"\nlabels = vit_root / \"labels.bin\"\n</code></pre> <ul> <li><code>$SAEV_SCRATCH</code> and <code>$SAEV_NFS</code> should be set for all users/processes running saev tools.</li> </ul>"},{"location":"developers/disk-layout/#faqs","title":"FAQs","text":"<ul> <li> <p>Where do patch labels live? Next to <code>acts*.bin</code> in <code>$SAEV_SCRATCH/shards/&lt;shard_hash&gt;/labels.bin</code>. Scripts discover them via <code>links/shards/labels.bin</code>.</p> </li> <li> <p>Can I put datasets directly in <code>$SAEV_SCRATCH</code>? Sure, but not in <code>$SAEV_SCRATCH/shards</code>.</p> </li> </ul>"},{"location":"developers/naming/","title":"Variable Naming","text":""},{"location":"developers/protocol/","title":"saev Sharded Activation File Protocol","text":"<p>saev caches activations to disk rather than run ViT or LLM inference when training SAEs. Gemma Scope makes this decision as well (see Section 3.3.2 of https://arxiv.org/pdf/2408.05147). <code>saev.data</code> has a specific protocol to support this in on OSC, a super computer center, and take advantage of OSC's specific disk performance. </p> <p>Goal: loss-lessly persist very large Transformer (ViT or LLM) activations in a form that is:</p> <ul> <li>mem-mappable</li> <li>Parameterized solely by the experiment configuration (<code>scripts/shards.py:Config</code>)</li> <li>Referenced by a content-hash, so identical configs collide, divergent ones never do</li> <li>Can be read quickly in a random order for training, and can be read (slowly) with random-access for visuals.</li> </ul> <p>This document is the single normative source. Any divergence in code is a bug.</p>"},{"location":"developers/protocol/#1-directory-layout","title":"1. Directory layout","text":"<pre><code>&lt;dump_to&gt;/&lt;HASH&gt;/\n    metadata.json    # UTF-8 JSON, human-readable, describes data-generating config\n    shards.json      # UTF-8 JSON, human-readable, describes shards.\n    acts000000.bin   # shard 0\n    acts000001.bin   # shard 1\n    ...\n    actsNNNNNN.bin   # shard NNNNNN  (zero-padded width=6)\n    labels.bin       # patch labels (optional)\n</code></pre> <p><code>HASH</code> = <code>sha256(json.dumps(metadata, sort_keys=True, separators=(',', ':')).encode('utf-8'))</code> Guards against silent config drift.</p>"},{"location":"developers/protocol/#2-json-file-schemas","title":"2. JSON file schemas","text":""},{"location":"developers/protocol/#21-metadatajson","title":"2.1. <code>metadata.json</code>","text":"field type semantic <code>family</code> string <code>\"clip\" \\| \"siglip\" \\| \"dinov2\"</code> <code>ckpt</code> string model identifier (OpenCLIP, HF, etc.) <code>layers</code> int[] ViT residual\u2010block indices recorded <code>patches_per_ex</code> int example patches only (excludes CLS) <code>cls_token</code> bool <code>true</code> -&gt; patch 0 is CLS, else no CLS <code>d_model</code> int activation dimensionality <code>n_examples</code> int total examples in dataset <code>patches_per_shard</code> int logical activations per shard (see #3) <code>data</code> object opaque dataset description <code>dataset</code> string absolute path to original dataset root <code>dtype</code> string numpy dtype. Fixed <code>\"float32\"</code> for now. <code>protocol</code> string <code>\"2.0\"</code> (shards after big refactor) <p>The <code>data</code> object is <code>dataclasses.asdict(cfg.data)</code>, with an additional <code>__class__</code> field with <code>cfg.data.__class__.__name__</code> as the value.</p> <p>The <code>dataset</code> field stores the absolute path to the root directory of the original image dataset, allowing runs to create symlinks back to the source images for visualization and analysis.</p>"},{"location":"developers/protocol/#22-shardsjson","title":"2.2. <code>shards.json</code>","text":"<p>A single array of <code>shard</code> objects, each of which has the following fields:</p> field type semantic name string shard filename (<code>acts000000.bin</code>). n_examples int the number of examples in the shard."},{"location":"developers/protocol/#3-shard-sizing-maths","title":"3. Shard sizing maths","text":"<pre><code>tokens_per_ex = patches_per_ex + (1 if cls_token else 0)\n\nexamples_per_shard = floor(patches_per_shard / (tokens_per_ex * len(layers)))\n\nshape_per_shard = (\n    examples_per_shard, len(layers), tokens_per_ex, d_model,\n)\n</code></pre> <p><code>patches_per_shard</code> is a budget (default ~2.4 M) chosen so a shard is approximately 10 GiB for Float32 @ <code>d_model = 1024</code>.</p> <p>The last shard will have a smaller value for <code>examples_per_shard</code>; this value is documented in <code>n_examples</code> in <code>shards.json</code></p>"},{"location":"developers/protocol/#4-data-layout-and-global-indexing","title":"4. Data Layout and Global Indexing","text":"<p>The entire dataset of activations is treated as a single logical 4D tensor with the shape <code>(n_examples, len(layers), tokens_per_ex, d_model)</code>. This logical tensor is C-contiguous with axes ordered <code>[Example, Layer, Token, Dimension]</code>.</p> <p>Physically, this tensor is split along the first axis (<code>Example</code>) into multiple shards, where each shard is a single binary file. The number of examples in each shard is constant, except for the final shard, which may be smaller.</p> <p>To locate an arbitrary activation vector, a reader must convert a logical coordinate (<code>global_ex_idx</code>, <code>layer_value</code>, <code>token_idx</code>) into a file path and an offset within that file.</p>"},{"location":"developers/protocol/#41-definitions","title":"4.1 Definitions","text":"<p>Let the parameters from <code>metadata.json</code> be:</p> <ul> <li>L = <code>len(layers)</code></li> <li>P = <code>patches_per_ex</code></li> <li>T = <code>P + (1 if cls_token else 0)</code> (Total tokens per example)</li> <li>D = <code>d_model</code></li> <li>S = <code>n_examples</code> from <code>shards.json</code> or <code>examples_per_shard</code> from Section 3 (shard sizing).</li> </ul>"},{"location":"developers/protocol/#42-coordinate-transformations","title":"4.2 Coordinate Transformations","text":"<p>Given a logical coordinate:</p> <ul> <li><code>global_ex_idx</code>: integer, with <code>0 &lt;= global_ex_idx &lt; n_examples</code></li> <li><code>layer</code>: integer, must be an element of <code>layers</code></li> <li><code>token_idx</code>: integer, <code>0 &lt;= token_idx &lt; T</code></li> </ul> <p>The physical location is found as follows:</p> <ol> <li> <p>Identify Shard:</p> <ul> <li><code>shard_idx = global_ex_idx // S</code></li> <li><code>ex_in_shard = global_ex_idx % S</code> The target file is <code>acts{shard_idx:06d}.bin</code>.</li> </ul> </li> <li> <p>Identify Layer Index: The stored data contains a subset of the ViT's layers. The logical <code>layer_value</code> must be mapped to its index in the stored <code>layers</code> array.</p> <ul> <li><code>layer_idx = layers.index(layer)</code> A reader must raise an error if <code>layer</code> is not in <code>layers</code>.</li> </ul> </li> <li> <p>Calculate Offset: The data within a shard is a 4D tensor of shape <code>(S, L, T, D)</code>. The offset to the first byte of the desired activation vector <code>[ex_in_shard, layer_idx , token_idx]</code> is:</p> <ul> <li><code>offset_in_vectors = (ex_in_shard * L * T) + (layer_idx * T) + token_idx</code></li> <li><code>offset_in_bytes = offset_in_vectors * D * 4</code> (assuming 4 bytes for <code>float32</code>)</li> </ul> </li> </ol> <p>A reader can then seek to <code>offset_in_bytes</code> and read $D \\times 4$ bytes to retrieve the vector.</p> <p>Alternatively, rather than calculate the offset, readers can memmap the shard, then use Numpy indexing to get the activation vector.</p>"},{"location":"developers/protocol/#43-token-axis-layout","title":"4.3 Token Axis Layout","text":"<p>The <code>token</code> axis of length $T$ is ordered as follows: * If <code>cls_token</code> is <code>true</code>:     * Index <code>0</code>: [CLS] token activation     * Indices <code>1</code> to $P$: Patch token activations * If <code>cls_token</code> is <code>false</code>:     * Indices <code>0</code> to $P-1$: Patch token activations</p> <p>The relative order of patch tokens is preserved exactly as produced by the upstream Vision Transformer.</p>"},{"location":"developers/protocol/#5-versioning-compatibility","title":"5 Versioning &amp; compatibility","text":"<ul> <li>Major changes (shape reorder, dtype switch, new required JSON keys) increment the major protocol version number at the top of this document and must emit a breaking warning in loader code.</li> <li>Minor, backward-compatible additions (new optional JSON key) merely update this doc and the minor protocol version number.</li> </ul> <p>That's it. Anything else you find in code that contradicts this document, fix the code or update the spec.</p>"},{"location":"users/glossary/","title":"Glossary","text":"<p>Definitions for words used in the code and documentation.</p> <ul> <li>example: one dataset item (image, sentence, audio clip, point cloud, graph instance).</li> <li>token: one model position in the encoder\u2019s residual stream (the thing with hidden size <code>d_model</code>). Always \"token\" inside the model.</li> <li>content token: tokens derived from the raw input (image patches, wordpieces, audio windows, nodes, etc.).</li> <li>special token: tokens not directly derived from the raw input (class/summary token, [SEP], [MASK], [PAD], register tokens, etc.).</li> <li>sequence length L: total tokens per example (content + special). If variable, call it \u201cragged\u201d.</li> <li>layer: an integer index into the encoder\u2019s stack.</li> <li>activation kind (optional but useful): which stream you saved (e.g., resid_pre, resid_post, mlp_out, attn_out, qkv, head_out).</li> </ul> <p>Modality-specific vocab:</p> <ul> <li>patch (vision): a 2D content token. Often laid out on a grid with shape (H_patches, W_patches).</li> <li>frame/token or tube (video): content token in time \u00d7 space; often (T, H, W).</li> <li>wordpiece / subword (text): content token from a tokenizer.</li> <li>window / frame (audio): time\u2013frequency window.</li> <li>node (graph), point (point cloud).</li> </ul>"},{"location":"users/guide/","title":"Guide","text":"<p>This guide explains how to transition from the ADE20K demo to using <code>saev</code> with your own custom datasets.</p> <p>Here are the steps:</p> <ol> <li>Record ViT activations and save them to disk.</li> <li>Train SAEs on the activations.</li> <li>Evaluate the SAE checkpoints.</li> <li>Visualize the learned features from the trained SAEs.</li> </ol> <p>Note</p> <p><code>saev</code> assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with <code>CUDA_VISIBLE_DEVICES=X</code> to run on GPU X.</p>"},{"location":"users/guide/#record-vit-activations-to-disk","title":"Record ViT Activations to Disk","text":"<p>To save activations to disk, we need to specify:</p> <ol> <li>Which model we would like to use</li> <li>Which layers we would like to save.</li> <li>Where on disk and how we would like to save activations.</li> <li>Which images we want to save activations for.</li> </ol> <p>The <code>scripts/shards.py</code> script does all of this for us.</p> <p>Run <code>uv run scripts/launch.py shards --help</code> to see all the configuration.</p> <p>In practice, you might run:</p> <pre><code>uv run scripts/launch.py shards \\\n  --family siglip \\\n  --ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \\\n  --d-model 1024 \\\n  --n-patches-per-img 256 \\\n  --no-cls-token \\\n  --layers 13 15 17 19 21 23 \\\n  --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \\\n  --max-patches-per-shard 500_000 \\\n  --slurm-acct PAS2136 \\\n  --n-hours 48 \\\n  --slurm-partition nextgen \\\n  data:image-folder \\\n  --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/\n</code></pre> <p>Let's break down these arguments.</p> <p>This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49). It will save the second-to-last layer (<code>--layer -2</code>). It will write 2.4M patches per shard, and save shards to a new directory <code>/local/scratch/$USER/cache/saev</code>.</p> <p>.. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a total of 723GB for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.</p> <p>This script will also save a <code>metadata.json</code> file that will record the relevant metadata for these activations, which will be read by future steps. The activations will be in <code>.bin</code> files, numbered starting from 000000.</p> <p>To add your own models, see the guide to extending in <code>saev.activations</code>.</p>"},{"location":"users/guide/#train-saes-on-activations","title":"Train SAEs on Activations","text":"<p>To train an SAE, we need to specify:</p> <ol> <li>Which activations to use as input.</li> <li>SAE architectural stuff.</li> <li>Optimization-related stuff.</li> </ol> <p>The <code>saev.training</code> module handles this.</p> <p>Run <code>uv run python -m saev train --help</code> to see all the configuration.</p> <p>Continuing on from our example before, you might want to run something like:</p> <pre><code>uv run python -m saev train \\\n  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\\n  --data.layer -2 \\\n  --data.patches patches \\\n  --data.no-scale-mean \\\n  --data.no-scale-norm \\\n  --sae.d-model 768 \\\n  --lr 5e-4\n</code></pre> <pre><code>uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-model 1024\n</code></pre> <p><code>--data.*</code> flags describe which activations to use.</p> <p><code>--data.shard-root</code> should point to a directory with <code>*.bin</code> files and the <code>metadata.json</code> file. <code>--data.layer</code> specifies the layer, and <code>--data.patches</code> says that want to train on individual patch activations, rather than the [CLS] token activation. <code>--data.no-scale-mean</code> and <code>--data.no-scale-norm</code> mean not to scale the activation mean or L2 norm. Anthropic's and OpenAI's papers suggest normalizing these factors, but <code>saev</code> still has a bug with this, so I suggest not scaling these factors.</p> <p><code>--sae.*</code> flags are about the SAE itself.</p> <p><code>--sae.d-model</code> is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.</p> <p>Finally, choose a slightly larger learning rate than the default with <code>--lr 5e-4</code>.</p> <p>This will train one (1) sparse autoencoder on the data. See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.</p>"},{"location":"users/guide/#visualize-the-learned-features","title":"Visualize the Learned Features","text":"<p>Now that you've trained an SAE, you probably want to look at its learned features. One way to visualize an individual learned feature (f) is by picking out images that maximize the activation of feature (f). Since we train SAEs on patch-level activations, we try to find the top patches for each feature (f). Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.</p> <p>.. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in <code>saev</code> unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to <code>contrib/</code> or another location.</p> <p><code>saev.visuals</code> records these maximally activating images for us. You can see all the options with <code>uv run python -m saev visuals --help</code>.</p> <p>The most important configuration options:</p> <ol> <li>The SAE checkpoint that you want to use (<code>--ckpt</code>).</li> <li>The ViT activations that you want to use (<code>--data.*</code> options, should be roughly the same as the options you used to train your SAE, like the same layer, same <code>--data.patches</code>).</li> <li>The images that produced the ViT activations that you want to use (<code>images</code> and <code>--images.*</code> options, should be the same as what you used to generate your ViT activtions).</li> <li>Some filtering options on which SAE latents to include (<code>--log-freq-range</code>, <code>--log-value-range</code>, <code>--include-latents</code>, <code>--n-latents</code>).</li> </ol> <p>Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on.</p> <p>.. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of (f)) are equally interesting. Some latents are dead, some are dense, some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents (<code>--include-latents</code>). I recommend using <code>saev.interactive.metrics</code> to figure out good thresholds.</p> <p>So you might run:</p> <pre><code>uv run python -m saev visuals \\\n  --ckpt checkpoints/abcdefg/sae.pt \\\n  --dump-to /nfs/$USER/saev/webapp/abcdefg \\\n  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \\\n  --data.layer -2 \\\n  --data.patches patches \\\n  images:imagenet-dataset\n</code></pre> <p>This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE. It will cache these best activations to disk, then start saving images to visualize later on.</p> <p><code>saev.interactive.features</code> is a small web application based on marimo to interactively look at these images.</p> <p>You can run it with <code>uv run marimo edit saev/interactive/features.py</code>.</p>"},{"location":"users/guide/#sweeps","title":"Sweeps","text":"<p>tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU.</p>"},{"location":"users/guide/#why-parallel-sweeps","title":"Why Parallel Sweeps","text":"<p>SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation. When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself.</p> <p>A single set of ImageNet activations for a vision transformer can require terabytes of storage. Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient.</p>"},{"location":"users/guide/#parallelized-training-architecture","title":"Parallelized Training Architecture","text":"<p>To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch:</p> <pre>\nflowchart TD\n    A[Pre-computed ViT Activations] --&gt;|Slow I/O| B[Memory Buffer]\n    B --&gt;|Shared Batch| C[SAE Model 1]\n    B --&gt;|Shared Batch| D[SAE Model 2]\n    B --&gt;|Shared Batch| E[SAE Model 3]\n    B --&gt;|Shared Batch| F[...]\n</pre> <p>This approach:</p> <ul> <li>Loads each batch of activations once from disk</li> <li>Uses that same batch for multiple SAE models with different hyperparameters</li> <li>Amortizes the slow I/O cost across all models in the sweep</li> </ul>"},{"location":"users/guide/#running-a-sweep","title":"Running a Sweep","text":"<p>The <code>train</code> command accepts a <code>--sweep</code> parameter that points to a TOML file defining the hyperparameter grid:</p> <pre><code>uv run python -m saev train --sweep configs/my_sweep.toml\n</code></pre> <p>Here's an example sweep configuration file:</p> <pre><code>[sae]\nsparsity_coeff = [1e-4, 2e-4, 3e-4]\nd_vit = 768\nexp_factor = [8, 16]\n\n[data]\nscale_mean = true\n</code></pre> <p>This would train 6 models (3 sparsity coefficients \u00d7 2 expansion factors), each sharing the same data loading operation.</p>"},{"location":"users/guide/#limitations","title":"Limitations","text":"<p>Not all parameters can be swept in parallel. Parameters that affect data loading (like <code>batch_size</code> or dataset configuration) will cause the sweep to split into separate parallel groups. The system automatically handles this division to maximize efficiency.</p>"},{"location":"users/guide/#training-metrics-and-visualizations","title":"Training Metrics and Visualizations","text":"<p>When you train a sweep of SAEs, you probably want to understand which checkpoint is best. <code>saev</code> provides some tools to help with that.</p> <p>First, we offer a tool to look at some basic summary statistics of all your trained checkpoints.</p> <p><code>saev.interactive.metrics</code> is a marimo notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB.</p> <p>However, there are some pieces of code that need to be changed for you to use it.</p> <p>.. todo:: Explain how to use the <code>saev.interactive.metrics</code> notebook.</p> <ul> <li>Need to change your wandb username from samuelstevens to USERNAME from wandb</li> <li>Tag filter</li> <li>Need to run the notebook on the same machine as the original ViT shards and the shards need to be there.</li> <li>Think of better ways to do model and data keys</li> <li>Look at examples</li> <li>run visuals before features</li> </ul> <p>How to run visuals faster?</p> <p>explain how these features are visualized</p>"},{"location":"users/inference/","title":"Inference","text":"<p>Briefly, you need to:</p> <ol> <li>Download a checkpoint.</li> <li>Get the code.</li> <li>Load the checkpoint.</li> <li>Get activations.</li> </ol> <p>Details are below.</p>"},{"location":"users/inference/#download-a-checkpoint","title":"Download a Checkpoint","text":"<p>First, download an SAE checkpoint from the Huggingface collection.</p> <p>For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations here.</p> <p>You can use <code>wget</code> if you want:</p> <pre><code>wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt\n</code></pre>"},{"location":"users/inference/#get-the-code","title":"Get the Code","text":"<p>The easiest way to do this is to clone the code:</p> <pre><code>git clone https://github.com/OSU-NLP-Group/saev\n</code></pre> <p>You can also install the package from git if you use uv (not sure about pip or cuda):</p> <pre><code>uv add git+https://github.com/OSU-NLP-Group/saev\n</code></pre> <p>Or clone it and install it as an editable with pip, lik <code>pip install -e .</code> in your virtual environment.</p> <p>Then you can do things like <code>from saev import ...</code>.</p> <p>.. note::   If you struggle to get <code>saev</code> installed, open an issue on GitHub and I will figure out how to make it easier.</p>"},{"location":"users/inference/#load-the-checkpoint","title":"Load the Checkpoint","text":"<pre><code>import saev.nn\n\nsae = saev.nn.load(\"PATH_TO_YOUR_SAE_CKPT.pt\")\n</code></pre> <p>Now you have a pretrained SAE.</p>"},{"location":"users/inference/#get-activations","title":"Get Activations","text":"<p>This is the hardest part. We need to:</p> <ol> <li>Pass an image into a ViT</li> <li>Record the dense ViT activations at the same layer that the SAE was trained on.</li> <li>Pass the activations into the SAE to get sparse activations.</li> <li>Do something interesting with the sparse SAE activations.</li> </ol> <p>There are examples of this in the demo code: for classification and semantic segmentation. If the permalinks change, you are looking for the <code>get_sae_latents()</code> functions in both files.</p> <p>Below is example code to do it using the <code>saev</code> package.</p> <pre><code>import saev.nn\nimport saev.activations\n\nimg_transform = saev.activations.make_img_transform(\"clip\", \"ViT-B-16/openai\")\n\nvit = saev.activations.make_vit(\"clip\", \"ViT-B-16/openai\")\nrecorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10])\n\nimg = Image.open(\"example.jpg\")\n\nx = img_transform(img)\n# Add a batch dimension\nx = x[None, ...]\n_, vit_acts = recorded_vit(x)\n# Select the only layer in the batch and ignore the CLS token.\nvit_acts = vit_acts[:, 0, 1:, :]\n\nx_hat, f_x, loss = sae(vit_acts)\n</code></pre> <p>Now you have the reconstructed x (<code>x_hat</code>) and the sparse representation of all patches in the image (<code>f_x</code>).</p> <p>You might select the dimensions with maximal values for each patch and see what other images are maximimally activating.</p> <p>.. todo::   Provide documentation for how get maximally activating images.</p>"},{"location":"users/sweeps/","title":"Sweeps","text":"<p>Hyperparameter sweeps in <code>saev</code> train multiple SAE configurations in parallel on a single GPU, amortizing the cost of loading activation data from disk across all models. Furthermore, sweeps make it easy to train multiple SAEs with one command across multiple GPUs using Slurm.</p>"},{"location":"users/sweeps/#quick-start","title":"Quick Start","text":"<p>Create a Python file defining your sweep:</p> <pre><code># sweeps/my_sweep.py\n\ndef make_cfgs() -&gt; list[dict]:\n    cfgs = []\n\n    # Grid search over learning rate and sparsity\n    for lr in [3e-4, 1e-3, 3e-3]:\n        for sparsity in [4e-4, 8e-4, 1.6e-3]:\n            cfg = {\n                \"lr\": lr,\n                \"objective\": {\"sparsity_coeff\": sparsity},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>Run the sweep:</p> <pre><code>uv run train.py --sweep sweeps/my_sweep.py \\\n  --train-data.layer 23 \\\n  --val-data.layer 23\n</code></pre> <p>This trains 9 SAEs (3 learning rates x 3 sparsity coefficients) in parallel.</p>"},{"location":"users/sweeps/#why-parallel-sweeps","title":"Why Parallel Sweeps?","text":"<p>SAE training is bottlenecked by disk I/O, not GPU computation. Loading terabytes of pre-computed ViT activations from disk is the slowest part. By training multiple SAE configurations on the same batch simultaneously, we amortize the I/O cost:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ViT Activations (disk) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502 (slow I/O, once per batch)\n            \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  Batch   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u25bc         \u25bc         \u25bc         \u25bc\n         SAE #1    SAE #2    SAE #3     ...\n        (lr=3e-4) (lr=1e-3) (lr=3e-3)\n</code></pre>"},{"location":"users/sweeps/#sweep-configuration","title":"Sweep Configuration","text":""},{"location":"users/sweeps/#python-based-sweeps","title":"Python-Based Sweeps","text":"<p>Python sweeps give you full control over config generation. Your sweep file must define a <code>make_cfgs()</code> function that returns a list of dicts.</p> <p>Grid search example:</p> <pre><code>def make_cfgs():\n    cfgs = []\n\n    for lr in [1e-4, 3e-4, 1e-3]:\n        for exp_factor in [8, 16, 32]:\n            cfg = {\n                \"lr\": lr,\n                \"sae\": {\"exp_factor\": exp_factor},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>Paired parameters (not a grid):</p> <pre><code>def make_cfgs():\n    cfgs = []\n\n    # Grid over lr x sparsity\n    for lr in [3e-4, 1e-3, 3e-3]:\n        for sparsity in [4e-4, 8e-4, 1.6e-3]:\n            # Paired layers (train and val use same layer)\n            for layer in [6, 7, 8, 9, 10, 11]:\n                cfg = {\n                    \"lr\": lr,\n                    \"objective\": {\"sparsity_coeff\": sparsity},\n                    \"train_data\": {\"layer\": layer},\n                    \"val_data\": {\"layer\": layer},\n                }\n                cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>This generates 54 configs (3 x 3 x 6) where each train/val pair uses the same layer, avoiding the 162 configs you'd get from a full grid (3 x 3 x 6 x 6).</p> <p>Conditional sweeps:</p> <pre><code>def make_cfgs():\n    cfgs = []\n\n    for exp_factor in [8, 16, 32]:\n        # Use different LR for different expansion factors\n        lrs = [1e-3, 3e-3] if exp_factor &lt;= 16 else [3e-4, 1e-3]\n\n        for lr in lrs:\n            cfg = {\n                \"lr\": lr,\n                \"sae\": {\"exp_factor\": exp_factor},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre>"},{"location":"users/sweeps/#command-line-overrides","title":"Command-Line Overrides","text":"<p>Command-line arguments override sweep parameters with deep merging. The precedence order is: CLI &gt; Sweep &gt; Default.</p> <pre><code>uv run train.py --sweep sweeps/my_sweep.py \\\n  --lr 5e-4  # Overrides all LRs in the sweep\n</code></pre> <p>Override nested config fields with dotted notation:</p> <pre><code>uv run train.py --sweep sweeps/my_sweep.py \\\n  --train-data.layer 23 \\\n  --val-data.layer 23 \\\n  --sae.exp-factor 16\n</code></pre> <p>Deep merging means that when you override a nested field, only that specific field is replaced\u2014other fields in the nested config are preserved from the sweep or default values.</p>"},{"location":"users/sweeps/#parallel-groups","title":"Parallel Groups","text":"<p>Not all parameters can vary within a parallel sweep. Parameters that affect data loading (like <code>train_data</code>, <code>n_train</code>, <code>device</code>) must be identical across all configs in a parallel group.</p> <p>When configs differ in these parameters, they're automatically split into separate Slurm jobs:</p> <pre><code>def make_cfgs():\n    cfgs = []\n\n    # These will run in 2 separate jobs\n    for layer in [6, 12]:  # Different data loading\n        for lr in [1e-4, 3e-4]:  # Can parallelize\n            cfg = {\n                \"lr\": lr,\n                \"train_data\": {\"layer\": layer},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>This creates 2 parallel groups: - Job 1: layer=6, lr=[1e-4, 3e-4] - Job 2: layer=12, lr=[1e-4, 3e-4]</p> <p>Implementation detail</p> <p>See <code>CANNOT_PARALLELIZE</code> in <code>train.py</code> for the full list of parameters that split parallel groups. The <code>split_cfgs()</code> function handles grouping automatically.</p>"},{"location":"users/sweeps/#module-loading","title":"Module Loading","text":"<p>Your sweep file is executed as a Python module, so you can use imports and helper functions:</p> <pre><code>def make_cfgs():\n    cfgs = []\n\n    # You can use helper functions\n    base_layers = list(range(6, 24, 2))\n\n    for layer in base_layers:\n        for lr in [1e-4, 3e-4]:\n            cfg = {\n                \"lr\": lr,\n                \"train_data\": {\"layer\": layer, \"n_threads\": 8},\n                \"val_data\": {\"layer\": layer, \"n_threads\": 8},\n                \"sae\": {\"exp_factor\": 16, \"d_vit\": 1024},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>Import mechanics</p> <p>The sweep file is loaded with <code>importlib.import_module()</code>, so it must be importable as a Python module. Place sweep files in a location where Python can find them (typically the project root or a <code>sweeps/</code> subdirectory).</p>"},{"location":"users/sweeps/#slurm-integration","title":"Slurm Integration","text":"<p>When running with <code>--slurm-acct</code>, each parallel group becomes a separate Slurm job:</p> <pre><code>uv run train.py --sweep sweeps/large.py \\\n  --slurm-acct PAS2136 \\\n  --slurm-partition nextgen \\\n  --n-hours 24\n</code></pre> <p>The system automatically: - Groups configs that can parallelize - Submits one Slurm job per group - Waits for all jobs to complete - Reports results</p>"},{"location":"users/sweeps/#seed-management","title":"Seed Management","text":"<p>Seeds are automatically incremented for each config to ensure reproducibility:</p> <pre><code># Base config has seed=42\n# Sweep generates 9 configs with seeds: 42, 43, 44, ..., 50\n</code></pre> <p>Override the base seed on the command line:</p> <pre><code>uv run train.py --sweep sweeps/my_sweep.py --seed 100\n</code></pre>"},{"location":"users/sweeps/#examples","title":"Examples","text":"<p>Simple grid:</p> <pre><code># sweeps/simple.py\ndef make_cfgs():\n    return [\n        {\"lr\": lr, \"objective\": {\"sparsity_coeff\": sp}}\n        for lr in [1e-4, 3e-4, 1e-3]\n        for sp in [4e-4, 8e-4, 1.6e-3]\n    ]\n</code></pre> <p>Layer sweep with paired train/val:</p> <pre><code># sweeps/layers.py\ndef make_cfgs():\n    cfgs = []\n\n    for layer in range(6, 24, 2):  # Layers 6, 8, 10, ..., 22\n        for lr in [3e-4, 1e-3]:\n            cfg = {\n                \"lr\": lr,\n                \"train_data\": {\"layer\": layer},\n                \"val_data\": {\"layer\": layer},\n            }\n            cfgs.append(cfg)\n\n    return cfgs\n</code></pre> <p>Architecture sweep:</p> <pre><code># sweeps/architecture.py\ndef make_cfgs():\n    cfgs = []\n\n    architectures = [\n        (\"small\", 8, 1e-3),\n        (\"medium\", 16, 5e-4),\n        (\"large\", 32, 3e-4),\n    ]\n\n    for name, exp_factor, lr in architectures:\n        cfg = {\n            \"lr\": lr,\n            \"sae\": {\"exp_factor\": exp_factor},\n            \"tag\": name,\n        }\n        cfgs.append(cfg)\n\n    return cfgs\n</code></pre>"}]}