<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Guide - saev</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Guide";
        var mkdocs_page_input_path = "users/guide.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> saev
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Users</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Guide</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#record-vit-activations-to-disk">Record ViT Activations to Disk</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#train-saes-on-activations">Train SAEs on Activations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#visualize-the-learned-features">Visualize the Learned Features</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sweeps">Sweeps</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#why-parallel-sweeps">Why Parallel Sweeps</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#parallelized-training-architecture">Parallelized Training Architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-a-sweep">Running a Sweep</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#training-metrics-and-visualizations">Training Metrics and Visualizations</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../inference/">Inference</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Developers</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../developers/contributing/">Contributing</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/saev/">saev</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/colors/">saev.colors</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/saev.data/">saev.data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/__main__/">saev.data.main</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/buffers/">saev.data.buffers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/clip/">saev.data.clip</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/datasets/">saev.data.datasets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/dinov2/">saev.data.dinov2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/dinov3/">saev.data.dinov3</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/fake_clip/">saev.data.fake_clip</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/indexed/">saev.data.indexed</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/models/">saev.data.models</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/ordered/">saev.data.ordered</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/shuffled/">saev.data.shuffled</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/siglip/">saev.data.siglip</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/transforms/">saev.data.transforms</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/data/writers/">saev.data.writers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/helpers/">saev.helpers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/nn/saev.nn/">saev.nn</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/nn/modeling/">saev.nn.modeling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/nn/objectives/">saev.nn.objectives</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/utils/saev.utils/">saev.utils</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/utils/scheduling/">saev.utils.scheduling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/utils/statistics/">saev.utils.statistics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/utils/wandb/">saev.utils.wandb</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/viz/">saev.viz</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">saev</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Users</li>
      <li class="breadcrumb-item active">Guide</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="guide">Guide</h1>
<ol>
<li>Record ViT activations and save them to disk.</li>
<li>Train SAEs on the activations.</li>
<li>Visualize the learned features from the trained SAEs.</li>
<li>(your job) Propose trends and patterns in the visualized features.</li>
<li>(your job, supported by code) Construct datasets to test your hypothesized trends.</li>
<li>Confirm/reject hypotheses using <code>probing</code> package.</li>
</ol>
<p><code>saev</code> helps with steps 1, 2 and 3.</p>
<pre><code>`saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X.
</code></pre>
<h2 id="record-vit-activations-to-disk">Record ViT Activations to Disk</h2>
<p>To save activations to disk, we need to specify:</p>
<ol>
<li>Which model we would like to use</li>
<li>Which layers we would like to save.</li>
<li>Where on disk and how we would like to save activations.</li>
<li>Which images we want to save activations for.</li>
</ol>
<p>The <code>saev.activations</code> module does all of this for us.</p>
<p>Run <code>uv run python -m saev activations --help</code> to see all the configuration.</p>
<p>In practice, you might run:</p>
<pre><code class="language-sh">uv run python -m saev.data \
  --vit-family siglip \
  --vit-ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \
  --d-vit 1024 \
  --n-patches-per-img 256 \
  --no-cls-token \
  --vit-layers 13 15 17 19 21 23 \
  --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \
  --max-patches-per-shard 500_000 \
  --slurm-acct PAS2136 \
  --n-hours 48 \
  --slurm-partition nextgen \
  data:image-folder \
  --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/
</code></pre>
<p>Let's break down these arguments.</p>
<p>This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49).
It will save the second-to-last layer (<code>--layer -2</code>).
It will write 2.4M patches per shard, and save shards to a new directory <code>/local/scratch/$USER/cache/saev</code>.</p>
<p>.. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a <strong>total of 723GB</strong> for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.</p>
<p>This script will also save a <code>metadata.json</code> file that will record the relevant metadata for these activations, which will be read by future steps.
The activations will be in <code>.bin</code> files, numbered starting from 000000.</p>
<p>To add your own models, see the guide to extending in <code>saev.activations</code>.</p>
<h2 id="train-saes-on-activations">Train SAEs on Activations</h2>
<p>To train an SAE, we need to specify:</p>
<ol>
<li>Which activations to use as input.</li>
<li>SAE architectural stuff.</li>
<li>Optimization-related stuff.</li>
</ol>
<p>The <code>saev.training</code> module handles this.</p>
<p>Run <code>uv run python -m saev train --help</code> to see all the configuration.</p>
<p>Continuing on from our example before, you might want to run something like:</p>
<pre><code class="language-sh">uv run python -m saev train \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  --data.no-scale-mean \
  --data.no-scale-norm \
  --sae.d-vit 768 \
  --lr 5e-4
</code></pre>
<pre><code class="language-sh">uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-vit 1024
</code></pre>
<p><code>--data.*</code> flags describe which activations to use.</p>
<p><code>--data.shard-root</code> should point to a directory with <code>*.bin</code> files and the <code>metadata.json</code> file.
<code>--data.layer</code> specifies the layer, and <code>--data.patches</code> says that want to train on individual patch activations, rather than the [CLS] token activation.
<code>--data.no-scale-mean</code> and <code>--data.no-scale-norm</code> mean not to scale the activation mean or L2 norm.
Anthropic's and OpenAI's papers suggest normalizing these factors, but <code>saev</code> still has a bug with this, so I suggest not scaling these factors.</p>
<p><code>--sae.*</code> flags are about the SAE itself.</p>
<p><code>--sae.d-vit</code> is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.</p>
<p>Finally, choose a slightly larger learning rate than the default with <code>--lr 5e-4</code>.</p>
<p>This will train one (1) sparse autoencoder on the data.
See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.</p>
<h2 id="visualize-the-learned-features">Visualize the Learned Features</h2>
<p>Now that you've trained an SAE, you probably want to look at its learned features.
One way to visualize an individual learned feature (f) is by picking out images that maximize the activation of feature (f).
Since we train SAEs on patch-level activations, we try to find the top <em>patches</em> for each feature (f).
Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.</p>
<p>.. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in <code>saev</code> unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to <code>contrib/</code> or another location.</p>
<p><code>saev.visuals</code> records these maximally activating images for us.
You can see all the options with <code>uv run python -m saev visuals --help</code>.</p>
<p>The most important configuration options:</p>
<ol>
<li>The SAE checkpoint that you want to use (<code>--ckpt</code>).</li>
<li>The ViT activations that you want to use (<code>--data.*</code> options, should be roughly the same as the options you used to train your SAE, like the same layer, same <code>--data.patches</code>).</li>
<li>The images that produced the ViT activations that you want to use (<code>images</code> and <code>--images.*</code> options, should be the same as what you used to generate your ViT activtions).</li>
<li>Some filtering options on which SAE latents to include (<code>--log-freq-range</code>, <code>--log-value-range</code>, <code>--include-latents</code>, <code>--n-latents</code>).</li>
</ol>
<p>Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on.</p>
<p>.. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of (f)) are equally interesting. Some latents are dead, some are <em>dense</em>, some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents (<code>--include-latents</code>). <strong>I recommend using <code>saev.interactive.metrics</code> to figure out good thresholds.</strong></p>
<p>So you might run:</p>
<pre><code class="language-sh">uv run python -m saev visuals \
  --ckpt checkpoints/abcdefg/sae.pt \
  --dump-to /nfs/$USER/saev/webapp/abcdefg \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  images:imagenet-dataset
</code></pre>
<p>This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE.
It will cache these best activations to disk, then start saving images to visualize later on.</p>
<p><code>saev.interactive.features</code> is a small web application based on <a href="https://marimo.io/">marimo</a> to interactively look at these images.</p>
<p>You can run it with <code>uv run marimo edit saev/interactive/features.py</code>.</p>
<h2 id="sweeps">Sweeps</h2>
<blockquote>
<p>tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU.</p>
</blockquote>
<h3 id="why-parallel-sweeps">Why Parallel Sweeps</h3>
<p>SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation.
When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself.</p>
<p>A single set of ImageNet activations for a vision transformer can require terabytes of storage.
Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient.</p>
<h3 id="parallelized-training-architecture">Parallelized Training Architecture</h3>
<p>To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch:</p>
<pre class="mermaid">
flowchart TD
    A[Pre-computed ViT Activations] -->|Slow I/O| B[Memory Buffer]
    B -->|Shared Batch| C[SAE Model 1]
    B -->|Shared Batch| D[SAE Model 2]
    B -->|Shared Batch| E[SAE Model 3]
    B -->|Shared Batch| F[...]
</pre>
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
</script>

<p>This approach:</p>
<ul>
<li>Loads each batch of activations <strong>once</strong> from disk</li>
<li>Uses that same batch for multiple SAE models with different hyperparameters</li>
<li>Amortizes the slow I/O cost across all models in the sweep</li>
</ul>
<h3 id="running-a-sweep">Running a Sweep</h3>
<p>The <code>train</code> command accepts a <code>--sweep</code> parameter that points to a TOML file defining the hyperparameter grid:</p>
<pre><code class="language-bash">uv run python -m saev train --sweep configs/my_sweep.toml
</code></pre>
<p>Here's an example sweep configuration file:</p>
<pre><code class="language-toml">[sae]
sparsity_coeff = [1e-4, 2e-4, 3e-4]
d_vit = 768
exp_factor = [8, 16]

[data]
scale_mean = true
</code></pre>
<p>This would train 6 models (3 sparsity coefficients Ã— 2 expansion factors), each sharing the same data loading operation.</p>
<h3 id="limitations">Limitations</h3>
<p>Not all parameters can be swept in parallel.
Parameters that affect data loading (like <code>batch_size</code> or dataset configuration) will cause the sweep to split into separate parallel groups.
The system automatically handles this division to maximize efficiency.</p>
<h2 id="training-metrics-and-visualizations">Training Metrics and Visualizations</h2>
<p>When you train a sweep of SAEs, you probably want to understand which checkpoint is best.
<code>saev</code> provides some tools to help with that.</p>
<p>First, we offer a tool to look at some basic summary statistics of all your trained checkpoints.</p>
<p><code>saev.interactive.metrics</code> is a <a href="https://marimo.io/">marimo</a> notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB.</p>
<p>However, there are some pieces of code that need to be changed for you to use it.</p>
<p>.. todo:: Explain how to use the <code>saev.interactive.metrics</code> notebook.</p>
<ul>
<li>Need to change your wandb username from samuelstevens to USERNAME from wandb</li>
<li>Tag filter</li>
<li>Need to run the notebook on the same machine as the original ViT shards and the shards need to be there.</li>
<li>Think of better ways to do model and data keys</li>
<li>Look at examples</li>
<li>run visuals before features</li>
</ul>
<p>How to run visuals faster?</p>
<p>explain how these features are visualized</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../.." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../inference/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(false);
        });
    </script>

</body>
</html>
