<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>saev.data.writers API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>saev.data.writers</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="saev.data.writers.get_acts_dir"><code class="name flex">
<span>def <span class="ident">get_acts_dir</span></span>(<span>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Return the activations directory based on the relevant values of a config.
Also saves a metadata.json file to that directory for human reference.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong></dt>
<dd>Config for experiment.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Directory to where activations should be dumped/loaded from.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def get_acts_dir(cfg: Config) -&gt; str:
    &#34;&#34;&#34;
    Return the activations directory based on the relevant values of a config.
    Also saves a metadata.json file to that directory for human reference.

    Args:
        cfg: Config for experiment.

    Returns:
        Directory to where activations should be dumped/loaded from.
    &#34;&#34;&#34;
    metadata = Metadata.from_cfg(cfg)

    acts_dir = os.path.join(cfg.dump_to, metadata.hash)
    os.makedirs(acts_dir, exist_ok=True)

    metadata.dump(acts_dir)

    return acts_dir</code></pre>
</details>
</dd>
<dt id="saev.data.writers.get_dataloader"><code class="name flex">
<span>def <span class="ident">get_dataloader</span></span>(<span>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>,<br>*,<br>img_tr: collections.abc.Callable | None = None,<br>seg_tr: collections.abc.Callable | None = None,<br>sample_tr: collections.abc.Callable | None = None) ‑> torch.utils.data.dataloader.DataLoader</span>
</code></dt>
<dd>
<div class="desc"><p>Get a dataloader for a default map-style dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong></dt>
<dd>Config.</dd>
<dt><strong><code>img_tr</code></strong></dt>
<dd>Image transform to be applied to each image.</dd>
<dt><strong><code>seg_tr</code></strong></dt>
<dd>Segmentation transform to be applied to masks.</dd>
<dt><strong><code>sample_tr</code></strong></dt>
<dd>Transform to be applied to sample dicts.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A PyTorch Dataloader that yields dictionaries with <code>'image'</code> keys containing image batches, <code>'index'</code> keys containing original dataset indices and <code>'label'</code> keys containing label batches.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def get_dataloader(
    cfg: Config,
    *,
    img_tr: Callable | None = None,
    seg_tr: Callable | None = None,
    sample_tr: Callable | None = None,
) -&gt; torch.utils.data.DataLoader:
    &#34;&#34;&#34;
    Get a dataloader for a default map-style dataset.

    Args:
        cfg: Config.
        img_tr: Image transform to be applied to each image.
        seg_tr: Segmentation transform to be applied to masks.
        sample_tr: Transform to be applied to sample dicts.

    Returns:
        A PyTorch Dataloader that yields dictionaries with `&#39;image&#39;` keys containing image batches, `&#39;index&#39;` keys containing original dataset indices and `&#39;label&#39;` keys containing label batches.
    &#34;&#34;&#34;
    dataset = datasets.get_dataset(
        cfg.data, img_transform=img_tr, seg_transform=seg_tr, sample_transform=sample_tr
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.vit_batch_size,
        drop_last=False,
        num_workers=cfg.n_workers,
        persistent_workers=cfg.n_workers &gt; 0,
        shuffle=False,
        pin_memory=False,
    )
    return dataloader</code></pre>
</details>
</dd>
<dt id="saev.data.writers.pixel_to_patch_labels"><code class="name flex">
<span>def <span class="ident">pixel_to_patch_labels</span></span>(<span>seg: PIL.Image.Image,<br>n_patches: int,<br>patch_size: int,<br>pixel_agg: Literal['majority', 'prefer-fg'] = 'majority',<br>bg_label: int = 0,<br>max_classes: int = 256) ‑> jaxtyping.UInt8[Tensor, 'n_patches']</span>
</code></dt>
<dd>
<div class="desc"><p>Convert pixel-level segmentation to patch-level labels using vectorized operations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>seg</code></strong></dt>
<dd>Pixel-level segmentation mask as PIL Image</dd>
<dt><strong><code>n_patches</code></strong></dt>
<dd>Total number of patches expected</dd>
<dt><strong><code>patch_size</code></strong></dt>
<dd>Size of each patch in pixels</dd>
<dt><strong><code>pixel_agg</code></strong></dt>
<dd>How to aggregate pixel labels into patch labels</dd>
<dt><strong><code>bg_label</code></strong></dt>
<dd>Background label index</dd>
<dt><strong><code>max_classes</code></strong></dt>
<dd>Maximum number of classes (for bincount)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Patch labels as uint8 tensor of shape (n_patches,)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
def pixel_to_patch_labels(
    seg: Image.Image,
    n_patches: int,
    patch_size: int,
    pixel_agg: tp.Literal[&#34;majority&#34;, &#34;prefer-fg&#34;] = &#34;majority&#34;,
    bg_label: int = 0,
    max_classes: int = 256,
) -&gt; UInt8[Tensor, &#34; n_patches&#34;]:
    &#34;&#34;&#34;
    Convert pixel-level segmentation to patch-level labels using vectorized operations.

    Args:
        seg: Pixel-level segmentation mask as PIL Image
        n_patches: Total number of patches expected
        patch_size: Size of each patch in pixels
        pixel_agg: How to aggregate pixel labels into patch labels
        bg_label: Background label index
        max_classes: Maximum number of classes (for bincount)

    Returns:
        Patch labels as uint8 tensor of shape (n_patches,)
    &#34;&#34;&#34;
    # Convert to torch tensor for vectorized operations
    seg_tensor = torch.from_numpy(np.array(seg, dtype=np.uint8))
    assert seg_tensor.ndim == 2

    h, w = seg_tensor.shape

    # Calculate patch grid dimensions
    patch_grid_h = h // patch_size
    patch_grid_w = w // patch_size
    assert patch_grid_w * patch_grid_h == n_patches, (
        f&#34;Image size {w}x{h} with patch_size {patch_size} gives {patch_grid_w}x{patch_grid_h} = {patch_grid_w * patch_grid_h} patches, expected {n_patches}&#34;
    )

    # Reshape into patches using einops: (n_patches, patch_size * patch_size)
    patches = einops.rearrange(
        seg_tensor,
        &#34;(h p1) (w p2) -&gt; (h w) (p1 p2)&#34;,
        p1=patch_size,
        p2=patch_size,
        h=patch_grid_h,
        w=patch_grid_w,
    )

    # Use vectorized bincount approach to get class counts for all patches at once
    # counts[i, c] = number of times class c appears in patch i
    offsets = torch.arange(n_patches, device=patches.device).unsqueeze(1) * max_classes
    flat = (patches + offsets).reshape(-1)
    counts = torch.bincount(flat, minlength=n_patches * max_classes).reshape(
        n_patches, max_classes
    )

    if pixel_agg == &#34;majority&#34;:
        # Take the most common label in each patch
        patch_labels = counts.argmax(dim=1)
    elif pixel_agg == &#34;prefer-fg&#34;:
        # Take the most common non-background label, or background if all background
        nonbg = counts.clone()
        nonbg[:, bg_label] = 0
        has_nonbg = nonbg.sum(dim=1) &gt; 0
        nonbg_arg = nonbg.argmax(dim=1)
        bg_tensor = torch.full_like(nonbg_arg, bg_label)
        patch_labels = torch.where(has_nonbg, nonbg_arg, bg_tensor)
    else:
        tp.assert_never(pixel_agg)

    return patch_labels.to(torch.uint8)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.worker_fn"><code class="name flex">
<span>def <span class="ident">worker_fn</span></span>(<span>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong></dt>
<dd>Config for activations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def worker_fn(cfg: Config):
    &#34;&#34;&#34;
    Args:
        cfg: Config for activations.
    &#34;&#34;&#34;
    from . import models

    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    log_format = &#34;[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s&#34;
    logging.basicConfig(level=logging.INFO, format=log_format)
    logger = logging.getLogger(&#34;worker_fn&#34;)

    if cfg.device == &#34;cuda&#34; and not torch.cuda.is_available():
        logger.warning(&#34;No CUDA device available, using CPU.&#34;)
        cfg = dataclasses.replace(cfg, device=&#34;cpu&#34;)

    vit_cls = models.load_vit_cls(cfg.vit_family)
    vit_instance = vit_cls(cfg.vit_ckpt).to(cfg.device)
    vit = RecordedVisionTransformer(
        vit_instance, cfg.n_patches_per_img, cfg.cls_token, cfg.vit_layers
    )

    img_tr, sample_tr = vit_cls.make_transforms(cfg.vit_ckpt, cfg.n_patches_per_img)

    seg_tr = None
    if _is_segmentation_dataset(cfg.data):
        # For segmentation datasets, create a transform that converts pixels to patches
        # Use make_resize with NEAREST interpolation for segmentation masks
        seg_resize_tr = vit_cls.make_resize(
            cfg.vit_ckpt, cfg.n_patches_per_img, scale=1.0, resample=Image.NEAREST
        )

        def seg_to_patches(seg):
            &#34;&#34;&#34;Transform that resizes segmentation and converts to patch labels.&#34;&#34;&#34;

            # Convert to patch labels
            return pixel_to_patch_labels(
                seg_resize_tr(seg),
                cfg.n_patches_per_img,
                patch_size=vit_instance.patch_size,
                pixel_agg=cfg.pixel_agg,
                bg_label=cfg.data.bg_label,
            )

        seg_tr = seg_to_patches

    dataloader = get_dataloader(cfg, img_tr=img_tr, seg_tr=seg_tr, sample_tr=sample_tr)

    n_batches = math.ceil(cfg.data.n_imgs / cfg.vit_batch_size)
    logger.info(&#34;Dumping %d batches of %d examples.&#34;, n_batches, cfg.vit_batch_size)

    vit = vit.to(cfg.device)
    # vit = torch.compile(vit)

    # Use context manager for proper cleanup
    with ShardWriter(cfg) as writer:
        i = 0
        # Calculate and write ViT activations.
        with torch.inference_mode():
            for batch in helpers.progress(dataloader, total=n_batches):
                imgs = batch.get(&#34;image&#34;).to(cfg.device)
                grid = batch.get(&#34;grid&#34;)
                if grid is not None:
                    grid = grid.to(cfg.device)
                    out, cache = vit(imgs, grid=grid)
                else:
                    out, cache = vit(imgs)
                # cache has shape [batch size, n layers, n patches + 1, d vit]
                del out

                # Write activations and labels (if present) in one call
                patch_labels = batch.get(&#34;patch_labels&#34;)
                if patch_labels is not None:
                    logger.debug(
                        f&#34;Found patch_labels in batch: shape={patch_labels.shape if hasattr(patch_labels, &#39;shape&#39;) else &#39;unknown&#39;}&#34;
                    )
                    # Ensure correct shape
                    assert patch_labels.shape == (len(cache), cfg.n_patches_per_img)
                else:
                    logger.debug(f&#34;No patch_labels in batch. Keys: {batch.keys()}&#34;)

                writer.write_batch(cache, i, patch_labels=patch_labels)

                i += len(cache)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="saev.data.writers.Config"><code class="flex name class">
<span>class <span class="ident">Config</span></span>
<span>(</span><span>data: <a title="saev.data.datasets.Imagenet" href="datasets.html#saev.data.datasets.Imagenet">Imagenet</a> | <a title="saev.data.datasets.ImageFolder" href="datasets.html#saev.data.datasets.ImageFolder">ImageFolder</a> | <a title="saev.data.datasets.SegFolder" href="datasets.html#saev.data.datasets.SegFolder">SegFolder</a> | <a title="saev.data.datasets.Fake" href="datasets.html#saev.data.datasets.Fake">Fake</a> | <a title="saev.data.datasets.FakeSeg" href="datasets.html#saev.data.datasets.FakeSeg">FakeSeg</a> = &lt;factory&gt;,<br>dump_to: str = './shards',<br>vit_family: Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip'] = 'clip',<br>vit_ckpt: str = 'ViT-L-14/openai',<br>vit_batch_size: int = 1024,<br>n_workers: int = 8,<br>d_vit: int = 1024,<br>vit_layers: list[int] = &lt;factory&gt;,<br>n_patches_per_img: int = 256,<br>cls_token: bool = True,<br>pixel_agg: Literal['majority', 'prefer-fg'] = 'majority',<br>max_patches_per_shard: int = 2400000,<br>ssl: bool = True,<br>device: str = 'cuda',<br>n_hours: float = 24.0,<br>slurm_acct: str = '',<br>slurm_partition: str = '',<br>log_to: str = './logs')</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for calculating and saving ViT activations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    &#34;&#34;&#34;Configuration for calculating and saving ViT activations.&#34;&#34;&#34;

    data: datasets.Config = dataclasses.field(default_factory=datasets.Imagenet)
    &#34;&#34;&#34;Which dataset to use.&#34;&#34;&#34;
    dump_to: str = os.path.join(&#34;.&#34;, &#34;shards&#34;)
    &#34;&#34;&#34;Where to write shards.&#34;&#34;&#34;
    vit_family: tp.Literal[&#34;clip&#34;, &#34;siglip&#34;, &#34;dinov2&#34;, &#34;dinov3&#34;, &#34;fake-clip&#34;] = &#34;clip&#34;
    &#34;&#34;&#34;Which model family.&#34;&#34;&#34;
    vit_ckpt: str = &#34;ViT-L-14/openai&#34;
    &#34;&#34;&#34;Specific model checkpoint.&#34;&#34;&#34;
    vit_batch_size: int = 1024
    &#34;&#34;&#34;Batch size for ViT inference.&#34;&#34;&#34;
    n_workers: int = 8
    &#34;&#34;&#34;Number of dataloader workers.&#34;&#34;&#34;
    d_vit: int = 1024
    &#34;&#34;&#34;Dimension of the ViT activations (depends on model).&#34;&#34;&#34;
    vit_layers: list[int] = dataclasses.field(default_factory=lambda: [-2])
    &#34;&#34;&#34;Which layers to save. By default, the second-to-last layer.&#34;&#34;&#34;
    n_patches_per_img: int = 256
    &#34;&#34;&#34;Number of ViT patches per image (depends on model).&#34;&#34;&#34;
    cls_token: bool = True
    &#34;&#34;&#34;Whether the model has a [CLS] token.&#34;&#34;&#34;
    pixel_agg: tp.Literal[&#34;majority&#34;, &#34;prefer-fg&#34;] = &#34;majority&#34;
    max_patches_per_shard: int = 2_400_000
    &#34;&#34;&#34;Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations.&#34;&#34;&#34;

    ssl: bool = True
    &#34;&#34;&#34;Whether to use SSL.&#34;&#34;&#34;

    # Hardware
    device: str = &#34;cuda&#34;
    &#34;&#34;&#34;Which device to use.&#34;&#34;&#34;
    n_hours: float = 24.0
    &#34;&#34;&#34;Slurm job length.&#34;&#34;&#34;
    slurm_acct: str = &#34;&#34;
    &#34;&#34;&#34;Slurm account string.&#34;&#34;&#34;
    slurm_partition: str = &#34;&#34;
    &#34;&#34;&#34;Slurm partition.&#34;&#34;&#34;
    log_to: str = &#34;./logs&#34;
    &#34;&#34;&#34;Where to log Slurm job stdout/stderr.&#34;&#34;&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="saev.data.writers.Config.cls_token"><code class="name">var <span class="ident">cls_token</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether the model has a [CLS] token.</p></div>
</dd>
<dt id="saev.data.writers.Config.d_vit"><code class="name">var <span class="ident">d_vit</span> : int</code></dt>
<dd>
<div class="desc"><p>Dimension of the ViT activations (depends on model).</p></div>
</dd>
<dt id="saev.data.writers.Config.data"><code class="name">var <span class="ident">data</span> : <a title="saev.data.datasets.Imagenet" href="datasets.html#saev.data.datasets.Imagenet">Imagenet</a> | <a title="saev.data.datasets.ImageFolder" href="datasets.html#saev.data.datasets.ImageFolder">ImageFolder</a> | <a title="saev.data.datasets.SegFolder" href="datasets.html#saev.data.datasets.SegFolder">SegFolder</a> | <a title="saev.data.datasets.Fake" href="datasets.html#saev.data.datasets.Fake">Fake</a> | <a title="saev.data.datasets.FakeSeg" href="datasets.html#saev.data.datasets.FakeSeg">FakeSeg</a></code></dt>
<dd>
<div class="desc"><p>Which dataset to use.</p></div>
</dd>
<dt id="saev.data.writers.Config.device"><code class="name">var <span class="ident">device</span> : str</code></dt>
<dd>
<div class="desc"><p>Which device to use.</p></div>
</dd>
<dt id="saev.data.writers.Config.dump_to"><code class="name">var <span class="ident">dump_to</span> : str</code></dt>
<dd>
<div class="desc"><p>Where to write shards.</p></div>
</dd>
<dt id="saev.data.writers.Config.log_to"><code class="name">var <span class="ident">log_to</span> : str</code></dt>
<dd>
<div class="desc"><p>Where to log Slurm job stdout/stderr.</p></div>
</dd>
<dt id="saev.data.writers.Config.max_patches_per_shard"><code class="name">var <span class="ident">max_patches_per_shard</span> : int</code></dt>
<dd>
<div class="desc"><p>Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations.</p></div>
</dd>
<dt id="saev.data.writers.Config.n_hours"><code class="name">var <span class="ident">n_hours</span> : float</code></dt>
<dd>
<div class="desc"><p>Slurm job length.</p></div>
</dd>
<dt id="saev.data.writers.Config.n_patches_per_img"><code class="name">var <span class="ident">n_patches_per_img</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of ViT patches per image (depends on model).</p></div>
</dd>
<dt id="saev.data.writers.Config.n_workers"><code class="name">var <span class="ident">n_workers</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of dataloader workers.</p></div>
</dd>
<dt id="saev.data.writers.Config.pixel_agg"><code class="name">var <span class="ident">pixel_agg</span> : Literal['majority', 'prefer-fg']</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Config.slurm_acct"><code class="name">var <span class="ident">slurm_acct</span> : str</code></dt>
<dd>
<div class="desc"><p>Slurm account string.</p></div>
</dd>
<dt id="saev.data.writers.Config.slurm_partition"><code class="name">var <span class="ident">slurm_partition</span> : str</code></dt>
<dd>
<div class="desc"><p>Slurm partition.</p></div>
</dd>
<dt id="saev.data.writers.Config.ssl"><code class="name">var <span class="ident">ssl</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether to use SSL.</p></div>
</dd>
<dt id="saev.data.writers.Config.vit_batch_size"><code class="name">var <span class="ident">vit_batch_size</span> : int</code></dt>
<dd>
<div class="desc"><p>Batch size for ViT inference.</p></div>
</dd>
<dt id="saev.data.writers.Config.vit_ckpt"><code class="name">var <span class="ident">vit_ckpt</span> : str</code></dt>
<dd>
<div class="desc"><p>Specific model checkpoint.</p></div>
</dd>
<dt id="saev.data.writers.Config.vit_family"><code class="name">var <span class="ident">vit_family</span> : Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip']</code></dt>
<dd>
<div class="desc"><p>Which model family.</p></div>
</dd>
<dt id="saev.data.writers.Config.vit_layers"><code class="name">var <span class="ident">vit_layers</span> : list[int]</code></dt>
<dd>
<div class="desc"><p>Which layers to save. By default, the second-to-last layer.</p></div>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.IndexLookup"><code class="flex name class">
<span>class <span class="ident">IndexLookup</span></span>
<span>(</span><span>metadata: <a title="saev.data.writers.Metadata" href="#saev.data.writers.Metadata">Metadata</a>,<br>patches: Literal['cls', 'image', 'all'],<br>layer: int | Literal['all'])</span>
</code></dt>
<dd>
<div class="desc"><p>Index &lt;-&gt; shard helper.</p>
<p><code>map()</code>
– turn a global dataset index into precise physical offsets.
<code>length()</code>
– dataset size for a particular (patches, layer) view.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code><a title="saev.data.writers.Metadata" href="#saev.data.writers.Metadata">Metadata</a></code></dt>
<dd>Pre-computed dataset statistics (images, patches, layers, shard size).</dd>
<dt><strong><code>patches</code></strong> :&ensp;<code>'cls' | 'image' | 'all'</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>layer</code></strong> :&ensp;<code>int | 'all'</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class IndexLookup:
    &#34;&#34;&#34;
    Index &lt;-&gt; shard helper.

    `map()`      – turn a global dataset index into precise physical offsets.
    `length()`   – dataset size for a particular (patches, layer) view.

    Parameters
    ----------
    metadata : Metadata
        Pre-computed dataset statistics (images, patches, layers, shard size).
    patches: &#39;cls&#39; | &#39;image&#39; | &#39;all&#39;
    layer: int | &#39;all&#39;
    &#34;&#34;&#34;

    def __init__(
        self,
        metadata: Metadata,
        patches: tp.Literal[&#34;cls&#34;, &#34;image&#34;, &#34;all&#34;],
        layer: int | tp.Literal[&#34;all&#34;],
    ):
        if not metadata.cls_token and patches == &#34;cls&#34;:
            raise ValueError(&#34;Cannot return [CLS] token if one isn&#39;t present.&#34;)

        self.metadata = metadata
        self.patches = patches

        if isinstance(layer, int) and layer not in metadata.layers:
            raise ValueError(f&#34;Layer {layer} not in {metadata.layers}.&#34;)
        self.layer = layer
        self.layer_to_idx = {layer: i for i, layer in enumerate(metadata.layers)}

    def map_global(self, i: int) -&gt; tuple[int, tuple[int, int, int]]:
        &#34;&#34;&#34;
        Return
        -------
        (
            shard_i,
            index in shard (img_i_in_shard, layer_i, token_i)
        )
        &#34;&#34;&#34;
        n = self.length()

        if i &lt; 0 or i &gt;= n:
            raise IndexError(f&#34;{i=} out of range [0, {n})&#34;)

        match (self.patches, self.layer):
            case (&#34;cls&#34;, &#34;all&#34;):
                # For CLS token with all layers, i represents (img_idx * n_layers + layer_idx)
                n_layers = len(self.metadata.layers)
                img_i = i // n_layers
                layer_idx = i % n_layers
                shard_i, img_i_in_shard = self.map_img(img_i)
                # CLS token is at position 0
                return shard_i, (img_i_in_shard, layer_idx, 0)
            case (&#34;cls&#34;, int()):
                # For CLS token with specific layer, i is the image index
                img_i = i
                shard_i, img_i_in_shard = self.map_img(img_i)
                # CLS token is at position 0
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], 0)
            case (&#34;image&#34;, int()):
                # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx)
                img_i = i // self.metadata.n_patches_per_img
                token_i = i % self.metadata.n_patches_per_img

                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
            case (&#34;image&#34;, &#34;all&#34;):
                # For image patches with all layers
                # Total patches per image across all layers
                total_patches_per_img = self.metadata.n_patches_per_img * len(
                    self.metadata.layers
                )

                # Calculate which image and which patch within that image across all layers
                img_i = i // total_patches_per_img
                remainder = i % total_patches_per_img

                # Calculate which layer and which patch within that layer
                layer_idx = remainder // self.metadata.n_patches_per_img
                token_i = remainder % self.metadata.n_patches_per_img

                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, layer_idx, token_i)
            case (&#34;all&#34;, int()):
                n_tokens_per_img = self.metadata.n_patches_per_img + (
                    1 if self.metadata.cls_token else 0
                )
                img_i = i // n_tokens_per_img
                token_i = i % n_tokens_per_img
                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
            case (&#34;all&#34;, &#34;all&#34;):
                # For all tokens (CLS + patches) with all layers
                # Calculate total tokens per image across all layers
                n_tokens_per_img = self.metadata.n_patches_per_img + (
                    1 if self.metadata.cls_token else 0
                )
                total_tokens_per_img = n_tokens_per_img * len(self.metadata.layers)

                # Calculate which image and which token within that image
                img_i = i // total_tokens_per_img
                remainder = i % total_tokens_per_img

                # Calculate which layer and which token within that layer
                layer_idx = remainder // n_tokens_per_img
                token_i = remainder % n_tokens_per_img

                # Map to physical location
                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, layer_idx, token_i)

            case _:
                tp.assert_never((self.patches, self.layer))

    def map_img(self, img_i: int) -&gt; tuple[int, int]:
        &#34;&#34;&#34;
        Return
        -------
        (shard_i, img_i_in_shard)
        &#34;&#34;&#34;
        if img_i &lt; 0 or img_i &gt;= self.metadata.n_imgs:
            raise IndexError(f&#34;{img_i=} out of range [0, {self.metadata.n_imgs})&#34;)

        # Calculate which shard contains this image
        shard_i = img_i // self.metadata.n_imgs_per_shard
        img_i_in_shard = img_i % self.metadata.n_imgs_per_shard

        return shard_i, img_i_in_shard

    def length(self) -&gt; int:
        match (self.patches, self.layer):
            case (&#34;cls&#34;, &#34;all&#34;):
                # Return a CLS token from a random image and random layer.
                return self.metadata.n_imgs * len(self.metadata.layers)
            case (&#34;cls&#34;, int()):
                # Return a CLS token from a random image and fixed layer.
                return self.metadata.n_imgs
            case (&#34;image&#34;, int()):
                # Return a patch from a random image, fixed layer, and random patch.
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            case (&#34;image&#34;, &#34;all&#34;):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case (&#34;all&#34;, int()):
                # Return a patch from a random image, specific layer and random patch.
                return self.metadata.n_imgs * (
                    self.metadata.n_patches_per_img + int(self.metadata.cls_token)
                )
            case (&#34;all&#34;, &#34;all&#34;):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * (self.metadata.n_patches_per_img + int(self.metadata.cls_token))
                )
            case _:
                tp.assert_never((self.patches, self.layer))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.IndexLookup.length"><code class="name flex">
<span>def <span class="ident">length</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def length(self) -&gt; int:
    match (self.patches, self.layer):
        case (&#34;cls&#34;, &#34;all&#34;):
            # Return a CLS token from a random image and random layer.
            return self.metadata.n_imgs * len(self.metadata.layers)
        case (&#34;cls&#34;, int()):
            # Return a CLS token from a random image and fixed layer.
            return self.metadata.n_imgs
        case (&#34;image&#34;, int()):
            # Return a patch from a random image, fixed layer, and random patch.
            return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
        case (&#34;image&#34;, &#34;all&#34;):
            # Return a patch from a random image, random layer and random patch.
            return (
                self.metadata.n_imgs
                * len(self.metadata.layers)
                * self.metadata.n_patches_per_img
            )
        case (&#34;all&#34;, int()):
            # Return a patch from a random image, specific layer and random patch.
            return self.metadata.n_imgs * (
                self.metadata.n_patches_per_img + int(self.metadata.cls_token)
            )
        case (&#34;all&#34;, &#34;all&#34;):
            # Return a patch from a random image, random layer and random patch.
            return (
                self.metadata.n_imgs
                * len(self.metadata.layers)
                * (self.metadata.n_patches_per_img + int(self.metadata.cls_token))
            )
        case _:
            tp.assert_never((self.patches, self.layer))</code></pre>
</details>
</dd>
<dt id="saev.data.writers.IndexLookup.map_global"><code class="name flex">
<span>def <span class="ident">map_global</span></span>(<span>self, i: int) ‑> tuple[int, tuple[int, int, int]]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="return">Return</h2>
<p>(
shard_i,
index in shard (img_i_in_shard, layer_i, token_i)
)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_global(self, i: int) -&gt; tuple[int, tuple[int, int, int]]:
    &#34;&#34;&#34;
    Return
    -------
    (
        shard_i,
        index in shard (img_i_in_shard, layer_i, token_i)
    )
    &#34;&#34;&#34;
    n = self.length()

    if i &lt; 0 or i &gt;= n:
        raise IndexError(f&#34;{i=} out of range [0, {n})&#34;)

    match (self.patches, self.layer):
        case (&#34;cls&#34;, &#34;all&#34;):
            # For CLS token with all layers, i represents (img_idx * n_layers + layer_idx)
            n_layers = len(self.metadata.layers)
            img_i = i // n_layers
            layer_idx = i % n_layers
            shard_i, img_i_in_shard = self.map_img(img_i)
            # CLS token is at position 0
            return shard_i, (img_i_in_shard, layer_idx, 0)
        case (&#34;cls&#34;, int()):
            # For CLS token with specific layer, i is the image index
            img_i = i
            shard_i, img_i_in_shard = self.map_img(img_i)
            # CLS token is at position 0
            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], 0)
        case (&#34;image&#34;, int()):
            # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx)
            img_i = i // self.metadata.n_patches_per_img
            token_i = i % self.metadata.n_patches_per_img

            shard_i, img_i_in_shard = self.map_img(img_i)
            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
        case (&#34;image&#34;, &#34;all&#34;):
            # For image patches with all layers
            # Total patches per image across all layers
            total_patches_per_img = self.metadata.n_patches_per_img * len(
                self.metadata.layers
            )

            # Calculate which image and which patch within that image across all layers
            img_i = i // total_patches_per_img
            remainder = i % total_patches_per_img

            # Calculate which layer and which patch within that layer
            layer_idx = remainder // self.metadata.n_patches_per_img
            token_i = remainder % self.metadata.n_patches_per_img

            shard_i, img_i_in_shard = self.map_img(img_i)
            return shard_i, (img_i_in_shard, layer_idx, token_i)
        case (&#34;all&#34;, int()):
            n_tokens_per_img = self.metadata.n_patches_per_img + (
                1 if self.metadata.cls_token else 0
            )
            img_i = i // n_tokens_per_img
            token_i = i % n_tokens_per_img
            shard_i, img_i_in_shard = self.map_img(img_i)
            return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
        case (&#34;all&#34;, &#34;all&#34;):
            # For all tokens (CLS + patches) with all layers
            # Calculate total tokens per image across all layers
            n_tokens_per_img = self.metadata.n_patches_per_img + (
                1 if self.metadata.cls_token else 0
            )
            total_tokens_per_img = n_tokens_per_img * len(self.metadata.layers)

            # Calculate which image and which token within that image
            img_i = i // total_tokens_per_img
            remainder = i % total_tokens_per_img

            # Calculate which layer and which token within that layer
            layer_idx = remainder // n_tokens_per_img
            token_i = remainder % n_tokens_per_img

            # Map to physical location
            shard_i, img_i_in_shard = self.map_img(img_i)
            return shard_i, (img_i_in_shard, layer_idx, token_i)

        case _:
            tp.assert_never((self.patches, self.layer))</code></pre>
</details>
</dd>
<dt id="saev.data.writers.IndexLookup.map_img"><code class="name flex">
<span>def <span class="ident">map_img</span></span>(<span>self, img_i: int) ‑> tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="return">Return</h2>
<p>(shard_i, img_i_in_shard)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_img(self, img_i: int) -&gt; tuple[int, int]:
    &#34;&#34;&#34;
    Return
    -------
    (shard_i, img_i_in_shard)
    &#34;&#34;&#34;
    if img_i &lt; 0 or img_i &gt;= self.metadata.n_imgs:
        raise IndexError(f&#34;{img_i=} out of range [0, {self.metadata.n_imgs})&#34;)

    # Calculate which shard contains this image
    shard_i = img_i // self.metadata.n_imgs_per_shard
    img_i_in_shard = img_i % self.metadata.n_imgs_per_shard

    return shard_i, img_i_in_shard</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.LabelsWriter"><code class="flex name class">
<span>class <span class="ident">LabelsWriter</span></span>
<span>(</span><span>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>LabelsWriter handles writing patch-level segmentation labels to a single binary file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@jaxtyped(typechecker=beartype.beartype)
class LabelsWriter:
    &#34;&#34;&#34;
    LabelsWriter handles writing patch-level segmentation labels to a single binary file.
    &#34;&#34;&#34;

    labels: UInt8[np.ndarray, &#34;n_imgs n_patches&#34;] | None
    labels_path: str
    n_patches_per_img: int
    n_imgs: int
    current_idx: int
    has_written: bool

    def __init__(self, cfg: Config):
        self.logger = logging.getLogger(&#34;labels-writer&#34;)
        self.root = get_acts_dir(cfg)
        self.n_patches_per_img = cfg.n_patches_per_img
        self.n_imgs = cfg.data.n_imgs
        self.has_written = False
        self.current_idx = 0

        # Always create memory-mapped file for labels
        # If nothing is written, it will be deleted in flush()
        self.labels_path = os.path.join(self.root, &#34;labels.bin&#34;)
        self.labels = np.memmap(
            self.labels_path,
            mode=&#34;w+&#34;,
            dtype=np.uint8,
            shape=(self.n_imgs, self.n_patches_per_img),
        )
        self.logger.info(&#34;Opened labels file &#39;%s&#39;.&#34;, self.labels_path)

    @beartype.beartype
    def write_batch(self, batch_labels: np.ndarray | Tensor, start_idx: int):
        &#34;&#34;&#34;
        Write a batch of labels to the memory-mapped file.

        Args:
            batch_labels: Array of shape (batch_size, n_patches_per_img) with uint8 dtype
            start_idx: Starting index in the global labels array
        &#34;&#34;&#34;
        # Convert to numpy if needed
        if isinstance(batch_labels, torch.Tensor):
            batch_labels = batch_labels.cpu().numpy()

        batch_size = len(batch_labels)
        assert start_idx + batch_size &lt;= self.n_imgs
        assert batch_labels.shape == (batch_size, self.n_patches_per_img)
        assert batch_labels.dtype == np.uint8

        self.labels[start_idx : start_idx + batch_size] = batch_labels
        self.current_idx = start_idx + batch_size
        self.has_written = True

    def flush(self) -&gt; None:
        &#34;&#34;&#34;Flush the memory-mapped file to disk if anything was written.&#34;&#34;&#34;
        if self.labels is not None and self.has_written:
            self.labels.flush()
            self.logger.info(&#34;Flushed labels to &#39;%s&#39;.&#34;, self.labels_path)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="saev.data.writers.LabelsWriter.current_idx"><code class="name">var <span class="ident">current_idx</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.LabelsWriter.has_written"><code class="name">var <span class="ident">has_written</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.LabelsWriter.labels"><code class="name">var <span class="ident">labels</span> : jaxtyping.UInt8[ndarray, 'n_imgs n_patches'] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.LabelsWriter.labels_path"><code class="name">var <span class="ident">labels_path</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.LabelsWriter.n_imgs"><code class="name">var <span class="ident">n_imgs</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.LabelsWriter.n_patches_per_img"><code class="name">var <span class="ident">n_patches_per_img</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.LabelsWriter.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Flush the memory-mapped file to disk if anything was written.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush(self) -&gt; None:
    &#34;&#34;&#34;Flush the memory-mapped file to disk if anything was written.&#34;&#34;&#34;
    if self.labels is not None and self.has_written:
        self.labels.flush()
        self.logger.info(&#34;Flushed labels to &#39;%s&#39;.&#34;, self.labels_path)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.LabelsWriter.write_batch"><code class="name flex">
<span>def <span class="ident">write_batch</span></span>(<span>self, batch_labels: numpy.ndarray | torch.Tensor, start_idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Write a batch of labels to the memory-mapped file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_labels</code></strong></dt>
<dd>Array of shape (batch_size, n_patches_per_img) with uint8 dtype</dd>
<dt><strong><code>start_idx</code></strong></dt>
<dd>Starting index in the global labels array</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
def write_batch(self, batch_labels: np.ndarray | Tensor, start_idx: int):
    &#34;&#34;&#34;
    Write a batch of labels to the memory-mapped file.

    Args:
        batch_labels: Array of shape (batch_size, n_patches_per_img) with uint8 dtype
        start_idx: Starting index in the global labels array
    &#34;&#34;&#34;
    # Convert to numpy if needed
    if isinstance(batch_labels, torch.Tensor):
        batch_labels = batch_labels.cpu().numpy()

    batch_size = len(batch_labels)
    assert start_idx + batch_size &lt;= self.n_imgs
    assert batch_labels.shape == (batch_size, self.n_patches_per_img)
    assert batch_labels.dtype == np.uint8

    self.labels[start_idx : start_idx + batch_size] = batch_labels
    self.current_idx = start_idx + batch_size
    self.has_written = True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.Metadata"><code class="flex name class">
<span>class <span class="ident">Metadata</span></span>
<span>(</span><span>vit_family: Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip'],<br>vit_ckpt: str,<br>layers: tuple[int, ...],<br>n_patches_per_img: int,<br>cls_token: bool,<br>d_vit: int,<br>n_imgs: int,<br>max_patches_per_shard: int,<br>data: dict[str, object],<br>pixel_agg: Literal['majority', 'prefer-fg', None] = None,<br>dtype: Literal['float32'] = 'float32',<br>protocol: Literal['1.0.0', '1.1'] = '1.1')</span>
</code></dt>
<dd>
<div class="desc"><p>Metadata(vit_family: Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip'], vit_ckpt: str, layers: tuple[int, &hellip;], n_patches_per_img: int, cls_token: bool, d_vit: int, n_imgs: int, max_patches_per_shard: int, data: dict[str, object], pixel_agg: Literal['majority', 'prefer-fg', None] = None, dtype: Literal['float32'] = 'float32', protocol: Literal['1.0.0', '1.1'] = '1.1')</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Metadata:
    vit_family: tp.Literal[&#34;clip&#34;, &#34;siglip&#34;, &#34;dinov2&#34;, &#34;dinov3&#34;, &#34;fake-clip&#34;]
    vit_ckpt: str
    layers: tuple[int, ...]
    n_patches_per_img: int
    cls_token: bool
    d_vit: int
    n_imgs: int
    max_patches_per_shard: int
    data: dict[str, object]
    pixel_agg: tp.Literal[&#34;majority&#34;, &#34;prefer-fg&#34;, None] = None
    dtype: tp.Literal[&#34;float32&#34;] = &#34;float32&#34;
    protocol: tp.Literal[&#34;1.0.0&#34;, &#34;1.1&#34;] = &#34;1.1&#34;

    def __post_init__(self):
        # Check that at least one image per shard can fit.
        assert self.n_imgs_per_shard &gt;= 1, (
            &#34;At least one image per shard must fit; increase max_patches_per_shard.&#34;
        )

    @classmethod
    def from_cfg(cls, cfg: Config) -&gt; &#34;Metadata&#34;:
        # Only include pixel_agg for segmentation datasets
        pixel_agg = None
        if _is_segmentation_dataset(cfg.data):
            pixel_agg = cfg.pixel_agg

        return cls(
            cfg.vit_family,
            cfg.vit_ckpt,
            tuple(cfg.vit_layers),
            cfg.n_patches_per_img,
            cfg.cls_token,
            cfg.d_vit,
            cfg.data.n_imgs,
            cfg.max_patches_per_shard,
            {**dataclasses.asdict(cfg.data), &#34;__class__&#34;: cfg.data.__class__.__name__},
            pixel_agg,
        )

    @classmethod
    def load(cls, shard_root: str) -&gt; &#34;Metadata&#34;:
        with open(os.path.join(shard_root, &#34;metadata.json&#34;)) as fd:
            dct = json.load(fd)
        dct[&#34;layers&#34;] = tuple(dct.pop(&#34;layers&#34;))
        return cls(**dct)

    def dump(self, shard_root: str):
        with open(os.path.join(shard_root, &#34;metadata.json&#34;), &#34;w&#34;) as fd:
            json.dump(dataclasses.asdict(self), fd, indent=4)

    @property
    def hash(self) -&gt; str:
        cfg_bytes = json.dumps(
            dataclasses.asdict(self), sort_keys=True, separators=(&#34;,&#34;, &#34;:&#34;)
        ).encode(&#34;utf-8&#34;)
        return hashlib.sha256(cfg_bytes).hexdigest()

    @property
    def n_tokens_per_img(self) -&gt; int:
        return self.n_patches_per_img + int(self.cls_token)

    @property
    def n_shards(self) -&gt; int:
        return math.ceil(self.n_imgs / self.n_imgs_per_shard)

    @property
    def n_imgs_per_shard(self) -&gt; int:
        &#34;&#34;&#34;
        Calculate the number of images per shard based on the protocol.

        Returns:
            Number of images that fit in a shard.
        &#34;&#34;&#34;
        n_tokens_per_img = self.n_patches_per_img + (1 if self.cls_token else 0)
        return self.max_patches_per_shard // (n_tokens_per_img * len(self.layers))

    @property
    def shard_shape(self) -&gt; tuple[int, int, int, int]:
        return (
            self.n_imgs_per_shard,
            len(self.layers),
            self.n_tokens_per_img,
            self.d_vit,
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="saev.data.writers.Metadata.from_cfg"><code class="name flex">
<span>def <span class="ident">from_cfg</span></span>(<span>cls,<br>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>) ‑> <a title="saev.data.writers.Metadata" href="#saev.data.writers.Metadata">Metadata</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_cfg(cls, cfg: Config) -&gt; &#34;Metadata&#34;:
    # Only include pixel_agg for segmentation datasets
    pixel_agg = None
    if _is_segmentation_dataset(cfg.data):
        pixel_agg = cfg.pixel_agg

    return cls(
        cfg.vit_family,
        cfg.vit_ckpt,
        tuple(cfg.vit_layers),
        cfg.n_patches_per_img,
        cfg.cls_token,
        cfg.d_vit,
        cfg.data.n_imgs,
        cfg.max_patches_per_shard,
        {**dataclasses.asdict(cfg.data), &#34;__class__&#34;: cfg.data.__class__.__name__},
        pixel_agg,
    )</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>cls, shard_root: str) ‑> <a title="saev.data.writers.Metadata" href="#saev.data.writers.Metadata">Metadata</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, shard_root: str) -&gt; &#34;Metadata&#34;:
    with open(os.path.join(shard_root, &#34;metadata.json&#34;)) as fd:
        dct = json.load(fd)
    dct[&#34;layers&#34;] = tuple(dct.pop(&#34;layers&#34;))
    return cls(**dct)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="saev.data.writers.Metadata.cls_token"><code class="name">var <span class="ident">cls_token</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.d_vit"><code class="name">var <span class="ident">d_vit</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.data"><code class="name">var <span class="ident">data</span> : dict[str, object]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.dtype"><code class="name">var <span class="ident">dtype</span> : Literal['float32']</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.hash"><code class="name">prop <span class="ident">hash</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def hash(self) -&gt; str:
    cfg_bytes = json.dumps(
        dataclasses.asdict(self), sort_keys=True, separators=(&#34;,&#34;, &#34;:&#34;)
    ).encode(&#34;utf-8&#34;)
    return hashlib.sha256(cfg_bytes).hexdigest()</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.layers"><code class="name">var <span class="ident">layers</span> : tuple[int, ...]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.max_patches_per_shard"><code class="name">var <span class="ident">max_patches_per_shard</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.n_imgs"><code class="name">var <span class="ident">n_imgs</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.n_imgs_per_shard"><code class="name">prop <span class="ident">n_imgs_per_shard</span> : int</code></dt>
<dd>
<div class="desc"><p>Calculate the number of images per shard based on the protocol.</p>
<h2 id="returns">Returns</h2>
<p>Number of images that fit in a shard.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_imgs_per_shard(self) -&gt; int:
    &#34;&#34;&#34;
    Calculate the number of images per shard based on the protocol.

    Returns:
        Number of images that fit in a shard.
    &#34;&#34;&#34;
    n_tokens_per_img = self.n_patches_per_img + (1 if self.cls_token else 0)
    return self.max_patches_per_shard // (n_tokens_per_img * len(self.layers))</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.n_patches_per_img"><code class="name">var <span class="ident">n_patches_per_img</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.n_shards"><code class="name">prop <span class="ident">n_shards</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_shards(self) -&gt; int:
    return math.ceil(self.n_imgs / self.n_imgs_per_shard)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.n_tokens_per_img"><code class="name">prop <span class="ident">n_tokens_per_img</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_tokens_per_img(self) -&gt; int:
    return self.n_patches_per_img + int(self.cls_token)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.pixel_agg"><code class="name">var <span class="ident">pixel_agg</span> : Literal['majority', 'prefer-fg', None]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.protocol"><code class="name">var <span class="ident">protocol</span> : Literal['1.0.0', '1.1']</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.shard_shape"><code class="name">prop <span class="ident">shard_shape</span> : tuple[int, int, int, int]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shard_shape(self) -&gt; tuple[int, int, int, int]:
    return (
        self.n_imgs_per_shard,
        len(self.layers),
        self.n_tokens_per_img,
        self.d_vit,
    )</code></pre>
</details>
</dd>
<dt id="saev.data.writers.Metadata.vit_ckpt"><code class="name">var <span class="ident">vit_ckpt</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Metadata.vit_family"><code class="name">var <span class="ident">vit_family</span> : Literal['clip', 'siglip', 'dinov2', 'dinov3', 'fake-clip']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.Metadata.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self, shard_root: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self, shard_root: str):
    with open(os.path.join(shard_root, &#34;metadata.json&#34;), &#34;w&#34;) as fd:
        json.dump(dataclasses.asdict(self), fd, indent=4)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.RecordedVisionTransformer"><code class="flex name class">
<span>class <span class="ident">RecordedVisionTransformer</span></span>
<span>(</span><span>vit: torch.nn.modules.module.Module,<br>n_patches_per_img: int,<br>cls_token: bool,<br>layers: list[int])</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class RecordedVisionTransformer(torch.nn.Module):
    _storage: Float[Tensor, &#34;batch n_layers all_patches dim&#34;] | None
    _i: int

    def __init__(
        self,
        vit: torch.nn.Module,
        n_patches_per_img: int,
        cls_token: bool,
        layers: list[int],
    ):
        super().__init__()

        self.vit = vit

        self.n_patches_per_img = n_patches_per_img
        self.cls_token = cls_token
        self.layers = layers

        self.patches = vit.get_patches(n_patches_per_img)

        self._storage = None
        self._i = 0

        self.logger = logging.getLogger(f&#34;recorder({vit.name})&#34;)

        for i in self.layers:
            self.vit.get_residuals()[i].register_forward_hook(self.hook)

    def hook(
        self, module, args: tuple, output: Float[Tensor, &#34;batch n_layers dim&#34;]
    ) -&gt; None:
        if self._storage is None:
            batch, _, dim = output.shape
            self._storage = self._empty_storage(batch, dim, output.device)

        if self._storage[:, self._i, 0, :].shape != output[:, 0, :].shape:
            batch, _, dim = output.shape

            old_batch, _, _, old_dim = self._storage.shape
            msg = &#34;Output shape does not match storage shape: (batch) %d != %d or (dim) %d != %d&#34;
            self.logger.warning(msg, old_batch, batch, old_dim, dim)

            self._storage = self._empty_storage(batch, dim, output.device)

        # Select patches based on cls_token setting
        selected_output = output[:, self.patches, :]
        if (
            not self.cls_token
            and selected_output.shape[1] == self.n_patches_per_img + 1
        ):
            # Model has CLS token but we don&#39;t want to store it - skip first token
            selected_output = selected_output[:, 1:, :]

        self._storage[:, self._i] = selected_output.detach()
        self._i += 1

    def _empty_storage(self, batch: int, dim: int, device: torch.device):
        n_patches_per_img = self.n_patches_per_img
        if self.cls_token:
            n_patches_per_img += 1

        return torch.zeros(
            (batch, len(self.layers), n_patches_per_img, dim), device=device
        )

    def reset(self):
        self._i = 0

    @property
    def activations(self) -&gt; Float[Tensor, &#34;batch n_layers all_patches dim&#34;]:
        if self._storage is None:
            raise RuntimeError(&#34;First call forward()&#34;)
        return self._storage.cpu()

    def forward(
        self, batch: Float[Tensor, &#34;batch 3 width height&#34;], **kwargs
    ) -&gt; tuple[
        Float[Tensor, &#34;batch patches dim&#34;],
        Float[Tensor, &#34;batch n_layers all_patches dim&#34;],
    ]:
        self.reset()
        result = self.vit(batch, **kwargs)
        return result, self.activations</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="saev.data.writers.RecordedVisionTransformer.activations"><code class="name">prop <span class="ident">activations</span> : jaxtyping.Float[Tensor, 'batch n_layers all_patches dim']</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def activations(self) -&gt; Float[Tensor, &#34;batch n_layers all_patches dim&#34;]:
    if self._storage is None:
        raise RuntimeError(&#34;First call forward()&#34;)
    return self._storage.cpu()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.RecordedVisionTransformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, batch: jaxtyping.Float[Tensor, 'batch 3 width height'], **kwargs) ‑> tuple[jaxtyping.Float[Tensor, 'batch patches dim'], jaxtyping.Float[Tensor, 'batch n_layers all_patches dim']]</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, batch: Float[Tensor, &#34;batch 3 width height&#34;], **kwargs
) -&gt; tuple[
    Float[Tensor, &#34;batch patches dim&#34;],
    Float[Tensor, &#34;batch n_layers all_patches dim&#34;],
]:
    self.reset()
    result = self.vit(batch, **kwargs)
    return result, self.activations</code></pre>
</details>
</dd>
<dt id="saev.data.writers.RecordedVisionTransformer.hook"><code class="name flex">
<span>def <span class="ident">hook</span></span>(<span>self, module, args: tuple, output: jaxtyping.Float[Tensor, 'batch n_layers dim']) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hook(
    self, module, args: tuple, output: Float[Tensor, &#34;batch n_layers dim&#34;]
) -&gt; None:
    if self._storage is None:
        batch, _, dim = output.shape
        self._storage = self._empty_storage(batch, dim, output.device)

    if self._storage[:, self._i, 0, :].shape != output[:, 0, :].shape:
        batch, _, dim = output.shape

        old_batch, _, _, old_dim = self._storage.shape
        msg = &#34;Output shape does not match storage shape: (batch) %d != %d or (dim) %d != %d&#34;
        self.logger.warning(msg, old_batch, batch, old_dim, dim)

        self._storage = self._empty_storage(batch, dim, output.device)

    # Select patches based on cls_token setting
    selected_output = output[:, self.patches, :]
    if (
        not self.cls_token
        and selected_output.shape[1] == self.n_patches_per_img + 1
    ):
        # Model has CLS token but we don&#39;t want to store it - skip first token
        selected_output = selected_output[:, 1:, :]

    self._storage[:, self._i] = selected_output.detach()
    self._i += 1</code></pre>
</details>
</dd>
<dt id="saev.data.writers.RecordedVisionTransformer.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self._i = 0</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.Shard"><code class="flex name class">
<span>class <span class="ident">Shard</span></span>
<span>(</span><span>name: str, n_imgs: int)</span>
</code></dt>
<dd>
<div class="desc"><p>A single shard entry in shards.json, recording the filename and number of images.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Shard:
    &#34;&#34;&#34;
    A single shard entry in shards.json, recording the filename and number of images.
    &#34;&#34;&#34;

    name: str
    n_imgs: int</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="saev.data.writers.Shard.n_imgs"><code class="name">var <span class="ident">n_imgs</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.Shard.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.ShardInfo"><code class="flex name class">
<span>class <span class="ident">ShardInfo</span></span>
<span>(</span><span>shards: list[<a title="saev.data.writers.Shard" href="#saev.data.writers.Shard">Shard</a>] = &lt;factory&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>A read-only container for shard metadata as recorded in shards.json.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ShardInfo:
    &#34;&#34;&#34;
    A read-only container for shard metadata as recorded in shards.json.
    &#34;&#34;&#34;

    shards: list[Shard] = dataclasses.field(default_factory=list)

    @classmethod
    def load(cls, shard_path: str) -&gt; &#34;ShardInfo&#34;:
        with open(os.path.join(shard_path, &#34;shards.json&#34;)) as fd:
            data = json.load(fd)
        return cls([Shard(**entry) for entry in data])

    def dump(self, fpath: str) -&gt; None:
        with open(os.path.join(fpath, &#34;shards.json&#34;), &#34;w&#34;) as fd:
            json.dump([dataclasses.asdict(s) for s in self.shards], fd, indent=2)

    def append(self, shard: Shard):
        self.shards.append(shard)

    def __len__(self) -&gt; int:
        return len(self.shards)

    def __getitem__(self, i):
        return self.shards[i]

    def __iter__(self):
        yield from self.shards</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="saev.data.writers.ShardInfo.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>cls, shard_path: str) ‑> <a title="saev.data.writers.ShardInfo" href="#saev.data.writers.ShardInfo">ShardInfo</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, shard_path: str) -&gt; &#34;ShardInfo&#34;:
    with open(os.path.join(shard_path, &#34;shards.json&#34;)) as fd:
        data = json.load(fd)
    return cls([Shard(**entry) for entry in data])</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="saev.data.writers.ShardInfo.shards"><code class="name">var <span class="ident">shards</span> : list[<a title="saev.data.writers.Shard" href="#saev.data.writers.Shard">Shard</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.ShardInfo.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self,<br>shard: <a title="saev.data.writers.Shard" href="#saev.data.writers.Shard">Shard</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, shard: Shard):
    self.shards.append(shard)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.ShardInfo.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self, fpath: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self, fpath: str) -&gt; None:
    with open(os.path.join(fpath, &#34;shards.json&#34;), &#34;w&#34;) as fd:
        json.dump([dataclasses.asdict(s) for s in self.shards], fd, indent=2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="saev.data.writers.ShardWriter"><code class="flex name class">
<span>class <span class="ident">ShardWriter</span></span>
<span>(</span><span>cfg: <a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>ShardWriter is a stateful object that handles sharded activation writing to disk.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jaxtyped(typechecker=beartype.beartype)
class ShardWriter:
    &#34;&#34;&#34;
    ShardWriter is a stateful object that handles sharded activation writing to disk.
    &#34;&#34;&#34;

    root: str
    shape: tuple[int, int, int, int]
    shard: int
    acts_path: str
    acts: Float[np.ndarray, &#34;n_imgs_per_shard n_layers all_patches d_vit&#34;] | None
    filled: int
    labels_writer: LabelsWriter

    def __init__(self, cfg: Config):
        self.logger = logging.getLogger(&#34;shard-writer&#34;)

        self.root = get_acts_dir(cfg)

        n_patches_per_img = cfg.n_patches_per_img
        if cfg.cls_token:
            n_patches_per_img += 1
        self.n_imgs_per_shard = (
            cfg.max_patches_per_shard // len(cfg.vit_layers) // n_patches_per_img
        )
        self.shape = (
            self.n_imgs_per_shard,
            len(cfg.vit_layers),
            n_patches_per_img,
            cfg.d_vit,
        )

        # builder for shard manifest
        self._shards: ShardInfo = ShardInfo()

        # Always initialize labels writer (it handles non-seg datasets internally)
        self.labels_writer = LabelsWriter(cfg)

        self.shard = -1
        self.acts = None
        self.next_shard()

    def write_batch(
        self,
        activations: Float[Tensor, &#34;batch n_layers all_patches d_vit&#34;],
        start_idx: int,
        patch_labels: UInt8[Tensor, &#34;batch n_patches&#34;] | None = None,
    ) -&gt; None:
        &#34;&#34;&#34;Write a batch of activations and optionally patch labels.

        Args:
            activations: Batch of activations to write.
            start_idx: Starting index for this batch.
            patch_labels: Optional patch labels for segmentation datasets.
        &#34;&#34;&#34;
        batch_size = len(activations)
        end_idx = start_idx + batch_size

        # Write activations (handling sharding)
        offset = self.n_imgs_per_shard * self.shard

        if end_idx &gt;= offset + self.n_imgs_per_shard:
            # We have run out of space in this mmap&#39;ed file. Let&#39;s fill it as much as we can.
            n_fit = offset + self.n_imgs_per_shard - start_idx
            self.acts[start_idx - offset : start_idx - offset + n_fit] = activations[
                :n_fit
            ]
            self.filled = start_idx - offset + n_fit

            # Write labels for the portion that fits
            if patch_labels is not None:
                # Convert to numpy uint8 if needed
                if isinstance(patch_labels, torch.Tensor):
                    labels_to_write = (
                        patch_labels[:n_fit].cpu().numpy().astype(np.uint8)
                    )
                elif not isinstance(patch_labels, np.ndarray):
                    labels_to_write = np.array(patch_labels[:n_fit], dtype=np.uint8)
                else:
                    labels_to_write = patch_labels[:n_fit]

                self.labels_writer.write_batch(labels_to_write, start_idx)

            self.next_shard()

            # Recursively call write_batch for remaining data
            if n_fit &lt; batch_size:
                self.write_batch(
                    activations[n_fit:],
                    start_idx + n_fit,
                    patch_labels[n_fit:] if patch_labels is not None else None,
                )
        else:
            msg = f&#34;0 &lt;= {start_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}&#34;
            assert 0 &lt;= start_idx - offset &lt;= offset + self.n_imgs_per_shard, msg
            msg = f&#34;0 &lt;= {end_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}&#34;
            assert 0 &lt;= end_idx - offset &lt;= offset + self.n_imgs_per_shard, msg
            self.acts[start_idx - offset : end_idx - offset] = activations
            self.filled = end_idx - offset

            # Write labels if provided
            if patch_labels is not None:
                # Convert to numpy uint8 if needed
                if isinstance(patch_labels, torch.Tensor):
                    patch_labels = patch_labels.cpu().numpy().astype(np.uint8)
                elif not isinstance(patch_labels, np.ndarray):
                    patch_labels = np.array(patch_labels, dtype=np.uint8)

                self.labels_writer.write_batch(patch_labels, start_idx)

    def flush(self) -&gt; None:
        if self.acts is not None:
            self.acts.flush()

            # record shard info
            self._shards.append(
                Shard(name=os.path.basename(self.acts_path), n_imgs=self.filled)
            )
            self._shards.dump(self.root)

        self.acts = None

        # Flush labels to disk
        self.labels_writer.flush()

    def __enter__(self):
        &#34;&#34;&#34;Context manager entry.&#34;&#34;&#34;
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        &#34;&#34;&#34;Context manager exit - handle cleanup.&#34;&#34;&#34;
        self.flush()

        # Delete empty labels file if nothing was written
        if not self.labels_writer.has_written:
            if os.path.exists(self.labels_writer.labels_path):
                os.remove(self.labels_writer.labels_path)
                self.logger.info(
                    &#34;Removed empty labels file &#39;%s&#39;.&#34;, self.labels_writer.labels_path
                )

    def next_shard(self) -&gt; None:
        self.flush()

        self.shard += 1
        self._count = 0
        self.acts_path = os.path.join(self.root, f&#34;acts{self.shard:06}.bin&#34;)
        self.acts = np.memmap(
            self.acts_path, mode=&#34;w+&#34;, dtype=np.float32, shape=self.shape
        )
        self.filled = 0

        self.logger.info(&#34;Opened shard &#39;%s&#39;.&#34;, self.acts_path)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="saev.data.writers.ShardWriter.acts"><code class="name">var <span class="ident">acts</span> : jaxtyping.Float[ndarray, 'n_imgs_per_shard n_layers all_patches d_vit'] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.acts_path"><code class="name">var <span class="ident">acts_path</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.filled"><code class="name">var <span class="ident">filled</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.labels_writer"><code class="name">var <span class="ident">labels_writer</span> : <a title="saev.data.writers.LabelsWriter" href="#saev.data.writers.LabelsWriter">LabelsWriter</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.root"><code class="name">var <span class="ident">root</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.shape"><code class="name">var <span class="ident">shape</span> : tuple[int, int, int, int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="saev.data.writers.ShardWriter.shard"><code class="name">var <span class="ident">shard</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="saev.data.writers.ShardWriter.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush(self) -&gt; None:
    if self.acts is not None:
        self.acts.flush()

        # record shard info
        self._shards.append(
            Shard(name=os.path.basename(self.acts_path), n_imgs=self.filled)
        )
        self._shards.dump(self.root)

    self.acts = None

    # Flush labels to disk
    self.labels_writer.flush()</code></pre>
</details>
</dd>
<dt id="saev.data.writers.ShardWriter.next_shard"><code class="name flex">
<span>def <span class="ident">next_shard</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next_shard(self) -&gt; None:
    self.flush()

    self.shard += 1
    self._count = 0
    self.acts_path = os.path.join(self.root, f&#34;acts{self.shard:06}.bin&#34;)
    self.acts = np.memmap(
        self.acts_path, mode=&#34;w+&#34;, dtype=np.float32, shape=self.shape
    )
    self.filled = 0

    self.logger.info(&#34;Opened shard &#39;%s&#39;.&#34;, self.acts_path)</code></pre>
</details>
</dd>
<dt id="saev.data.writers.ShardWriter.write_batch"><code class="name flex">
<span>def <span class="ident">write_batch</span></span>(<span>self,<br>activations: jaxtyping.Float[Tensor, 'batch n_layers all_patches d_vit'],<br>start_idx: int,<br>patch_labels: jaxtyping.UInt8[Tensor, 'batch n_patches'] | None = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Write a batch of activations and optionally patch labels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>activations</code></strong></dt>
<dd>Batch of activations to write.</dd>
<dt><strong><code>start_idx</code></strong></dt>
<dd>Starting index for this batch.</dd>
<dt><strong><code>patch_labels</code></strong></dt>
<dd>Optional patch labels for segmentation datasets.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_batch(
    self,
    activations: Float[Tensor, &#34;batch n_layers all_patches d_vit&#34;],
    start_idx: int,
    patch_labels: UInt8[Tensor, &#34;batch n_patches&#34;] | None = None,
) -&gt; None:
    &#34;&#34;&#34;Write a batch of activations and optionally patch labels.

    Args:
        activations: Batch of activations to write.
        start_idx: Starting index for this batch.
        patch_labels: Optional patch labels for segmentation datasets.
    &#34;&#34;&#34;
    batch_size = len(activations)
    end_idx = start_idx + batch_size

    # Write activations (handling sharding)
    offset = self.n_imgs_per_shard * self.shard

    if end_idx &gt;= offset + self.n_imgs_per_shard:
        # We have run out of space in this mmap&#39;ed file. Let&#39;s fill it as much as we can.
        n_fit = offset + self.n_imgs_per_shard - start_idx
        self.acts[start_idx - offset : start_idx - offset + n_fit] = activations[
            :n_fit
        ]
        self.filled = start_idx - offset + n_fit

        # Write labels for the portion that fits
        if patch_labels is not None:
            # Convert to numpy uint8 if needed
            if isinstance(patch_labels, torch.Tensor):
                labels_to_write = (
                    patch_labels[:n_fit].cpu().numpy().astype(np.uint8)
                )
            elif not isinstance(patch_labels, np.ndarray):
                labels_to_write = np.array(patch_labels[:n_fit], dtype=np.uint8)
            else:
                labels_to_write = patch_labels[:n_fit]

            self.labels_writer.write_batch(labels_to_write, start_idx)

        self.next_shard()

        # Recursively call write_batch for remaining data
        if n_fit &lt; batch_size:
            self.write_batch(
                activations[n_fit:],
                start_idx + n_fit,
                patch_labels[n_fit:] if patch_labels is not None else None,
            )
    else:
        msg = f&#34;0 &lt;= {start_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}&#34;
        assert 0 &lt;= start_idx - offset &lt;= offset + self.n_imgs_per_shard, msg
        msg = f&#34;0 &lt;= {end_idx} - {offset} &lt;= {offset} + {self.n_imgs_per_shard}&#34;
        assert 0 &lt;= end_idx - offset &lt;= offset + self.n_imgs_per_shard, msg
        self.acts[start_idx - offset : end_idx - offset] = activations
        self.filled = end_idx - offset

        # Write labels if provided
        if patch_labels is not None:
            # Convert to numpy uint8 if needed
            if isinstance(patch_labels, torch.Tensor):
                patch_labels = patch_labels.cpu().numpy().astype(np.uint8)
            elif not isinstance(patch_labels, np.ndarray):
                patch_labels = np.array(patch_labels, dtype=np.uint8)

            self.labels_writer.write_batch(patch_labels, start_idx)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.css" integrity="sha512-b+T2i3P45i1LZM7I00Ci5QquB9szqaxu+uuk5TUSGjZQ4w4n+qujQiIuvTv2BxE7WCGQCifNMksyKILDiHzsOg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.16.0/tingle.min.js" integrity="sha512-2B9/byNV1KKRm5nQ2RLViPFD6U4dUjDGwuW1GU+ImJh8YinPU9Zlq1GzdTMO+G2ROrB5o1qasJBy1ttYz0wCug==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="saev.data" href="index.html">saev.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="saev.data.writers.get_acts_dir" href="#saev.data.writers.get_acts_dir">get_acts_dir</a></code></li>
<li><code><a title="saev.data.writers.get_dataloader" href="#saev.data.writers.get_dataloader">get_dataloader</a></code></li>
<li><code><a title="saev.data.writers.pixel_to_patch_labels" href="#saev.data.writers.pixel_to_patch_labels">pixel_to_patch_labels</a></code></li>
<li><code><a title="saev.data.writers.worker_fn" href="#saev.data.writers.worker_fn">worker_fn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="saev.data.writers.Config" href="#saev.data.writers.Config">Config</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.Config.cls_token" href="#saev.data.writers.Config.cls_token">cls_token</a></code></li>
<li><code><a title="saev.data.writers.Config.d_vit" href="#saev.data.writers.Config.d_vit">d_vit</a></code></li>
<li><code><a title="saev.data.writers.Config.data" href="#saev.data.writers.Config.data">data</a></code></li>
<li><code><a title="saev.data.writers.Config.device" href="#saev.data.writers.Config.device">device</a></code></li>
<li><code><a title="saev.data.writers.Config.dump_to" href="#saev.data.writers.Config.dump_to">dump_to</a></code></li>
<li><code><a title="saev.data.writers.Config.log_to" href="#saev.data.writers.Config.log_to">log_to</a></code></li>
<li><code><a title="saev.data.writers.Config.max_patches_per_shard" href="#saev.data.writers.Config.max_patches_per_shard">max_patches_per_shard</a></code></li>
<li><code><a title="saev.data.writers.Config.n_hours" href="#saev.data.writers.Config.n_hours">n_hours</a></code></li>
<li><code><a title="saev.data.writers.Config.n_patches_per_img" href="#saev.data.writers.Config.n_patches_per_img">n_patches_per_img</a></code></li>
<li><code><a title="saev.data.writers.Config.n_workers" href="#saev.data.writers.Config.n_workers">n_workers</a></code></li>
<li><code><a title="saev.data.writers.Config.pixel_agg" href="#saev.data.writers.Config.pixel_agg">pixel_agg</a></code></li>
<li><code><a title="saev.data.writers.Config.slurm_acct" href="#saev.data.writers.Config.slurm_acct">slurm_acct</a></code></li>
<li><code><a title="saev.data.writers.Config.slurm_partition" href="#saev.data.writers.Config.slurm_partition">slurm_partition</a></code></li>
<li><code><a title="saev.data.writers.Config.ssl" href="#saev.data.writers.Config.ssl">ssl</a></code></li>
<li><code><a title="saev.data.writers.Config.vit_batch_size" href="#saev.data.writers.Config.vit_batch_size">vit_batch_size</a></code></li>
<li><code><a title="saev.data.writers.Config.vit_ckpt" href="#saev.data.writers.Config.vit_ckpt">vit_ckpt</a></code></li>
<li><code><a title="saev.data.writers.Config.vit_family" href="#saev.data.writers.Config.vit_family">vit_family</a></code></li>
<li><code><a title="saev.data.writers.Config.vit_layers" href="#saev.data.writers.Config.vit_layers">vit_layers</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.IndexLookup" href="#saev.data.writers.IndexLookup">IndexLookup</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.IndexLookup.length" href="#saev.data.writers.IndexLookup.length">length</a></code></li>
<li><code><a title="saev.data.writers.IndexLookup.map_global" href="#saev.data.writers.IndexLookup.map_global">map_global</a></code></li>
<li><code><a title="saev.data.writers.IndexLookup.map_img" href="#saev.data.writers.IndexLookup.map_img">map_img</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.LabelsWriter" href="#saev.data.writers.LabelsWriter">LabelsWriter</a></code></h4>
<ul class="two-column">
<li><code><a title="saev.data.writers.LabelsWriter.current_idx" href="#saev.data.writers.LabelsWriter.current_idx">current_idx</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.flush" href="#saev.data.writers.LabelsWriter.flush">flush</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.has_written" href="#saev.data.writers.LabelsWriter.has_written">has_written</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.labels" href="#saev.data.writers.LabelsWriter.labels">labels</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.labels_path" href="#saev.data.writers.LabelsWriter.labels_path">labels_path</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.n_imgs" href="#saev.data.writers.LabelsWriter.n_imgs">n_imgs</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.n_patches_per_img" href="#saev.data.writers.LabelsWriter.n_patches_per_img">n_patches_per_img</a></code></li>
<li><code><a title="saev.data.writers.LabelsWriter.write_batch" href="#saev.data.writers.LabelsWriter.write_batch">write_batch</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.Metadata" href="#saev.data.writers.Metadata">Metadata</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.Metadata.cls_token" href="#saev.data.writers.Metadata.cls_token">cls_token</a></code></li>
<li><code><a title="saev.data.writers.Metadata.d_vit" href="#saev.data.writers.Metadata.d_vit">d_vit</a></code></li>
<li><code><a title="saev.data.writers.Metadata.data" href="#saev.data.writers.Metadata.data">data</a></code></li>
<li><code><a title="saev.data.writers.Metadata.dtype" href="#saev.data.writers.Metadata.dtype">dtype</a></code></li>
<li><code><a title="saev.data.writers.Metadata.dump" href="#saev.data.writers.Metadata.dump">dump</a></code></li>
<li><code><a title="saev.data.writers.Metadata.from_cfg" href="#saev.data.writers.Metadata.from_cfg">from_cfg</a></code></li>
<li><code><a title="saev.data.writers.Metadata.hash" href="#saev.data.writers.Metadata.hash">hash</a></code></li>
<li><code><a title="saev.data.writers.Metadata.layers" href="#saev.data.writers.Metadata.layers">layers</a></code></li>
<li><code><a title="saev.data.writers.Metadata.load" href="#saev.data.writers.Metadata.load">load</a></code></li>
<li><code><a title="saev.data.writers.Metadata.max_patches_per_shard" href="#saev.data.writers.Metadata.max_patches_per_shard">max_patches_per_shard</a></code></li>
<li><code><a title="saev.data.writers.Metadata.n_imgs" href="#saev.data.writers.Metadata.n_imgs">n_imgs</a></code></li>
<li><code><a title="saev.data.writers.Metadata.n_imgs_per_shard" href="#saev.data.writers.Metadata.n_imgs_per_shard">n_imgs_per_shard</a></code></li>
<li><code><a title="saev.data.writers.Metadata.n_patches_per_img" href="#saev.data.writers.Metadata.n_patches_per_img">n_patches_per_img</a></code></li>
<li><code><a title="saev.data.writers.Metadata.n_shards" href="#saev.data.writers.Metadata.n_shards">n_shards</a></code></li>
<li><code><a title="saev.data.writers.Metadata.n_tokens_per_img" href="#saev.data.writers.Metadata.n_tokens_per_img">n_tokens_per_img</a></code></li>
<li><code><a title="saev.data.writers.Metadata.pixel_agg" href="#saev.data.writers.Metadata.pixel_agg">pixel_agg</a></code></li>
<li><code><a title="saev.data.writers.Metadata.protocol" href="#saev.data.writers.Metadata.protocol">protocol</a></code></li>
<li><code><a title="saev.data.writers.Metadata.shard_shape" href="#saev.data.writers.Metadata.shard_shape">shard_shape</a></code></li>
<li><code><a title="saev.data.writers.Metadata.vit_ckpt" href="#saev.data.writers.Metadata.vit_ckpt">vit_ckpt</a></code></li>
<li><code><a title="saev.data.writers.Metadata.vit_family" href="#saev.data.writers.Metadata.vit_family">vit_family</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.RecordedVisionTransformer" href="#saev.data.writers.RecordedVisionTransformer">RecordedVisionTransformer</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.RecordedVisionTransformer.activations" href="#saev.data.writers.RecordedVisionTransformer.activations">activations</a></code></li>
<li><code><a title="saev.data.writers.RecordedVisionTransformer.forward" href="#saev.data.writers.RecordedVisionTransformer.forward">forward</a></code></li>
<li><code><a title="saev.data.writers.RecordedVisionTransformer.hook" href="#saev.data.writers.RecordedVisionTransformer.hook">hook</a></code></li>
<li><code><a title="saev.data.writers.RecordedVisionTransformer.reset" href="#saev.data.writers.RecordedVisionTransformer.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.Shard" href="#saev.data.writers.Shard">Shard</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.Shard.n_imgs" href="#saev.data.writers.Shard.n_imgs">n_imgs</a></code></li>
<li><code><a title="saev.data.writers.Shard.name" href="#saev.data.writers.Shard.name">name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.ShardInfo" href="#saev.data.writers.ShardInfo">ShardInfo</a></code></h4>
<ul class="">
<li><code><a title="saev.data.writers.ShardInfo.append" href="#saev.data.writers.ShardInfo.append">append</a></code></li>
<li><code><a title="saev.data.writers.ShardInfo.dump" href="#saev.data.writers.ShardInfo.dump">dump</a></code></li>
<li><code><a title="saev.data.writers.ShardInfo.load" href="#saev.data.writers.ShardInfo.load">load</a></code></li>
<li><code><a title="saev.data.writers.ShardInfo.shards" href="#saev.data.writers.ShardInfo.shards">shards</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="saev.data.writers.ShardWriter" href="#saev.data.writers.ShardWriter">ShardWriter</a></code></h4>
<ul class="two-column">
<li><code><a title="saev.data.writers.ShardWriter.acts" href="#saev.data.writers.ShardWriter.acts">acts</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.acts_path" href="#saev.data.writers.ShardWriter.acts_path">acts_path</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.filled" href="#saev.data.writers.ShardWriter.filled">filled</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.flush" href="#saev.data.writers.ShardWriter.flush">flush</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.labels_writer" href="#saev.data.writers.ShardWriter.labels_writer">labels_writer</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.next_shard" href="#saev.data.writers.ShardWriter.next_shard">next_shard</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.root" href="#saev.data.writers.ShardWriter.root">root</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.shape" href="#saev.data.writers.ShardWriter.shape">shape</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.shard" href="#saev.data.writers.ShardWriter.shard">shard</a></code></li>
<li><code><a title="saev.data.writers.ShardWriter.write_batch" href="#saev.data.writers.ShardWriter.write_batch">write_batch</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
