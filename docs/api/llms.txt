>>>> AGENTS.md
# Conventions

This document outlines some programming conventions that are not caught by automated tools.

* File descriptors from `open()` are called `fd`.
* Use types where possible, including `jaxtyping` hints.
* Decorate functions with `beartype.beartype` unless they use a `jaxtyping` hint, in which case use `jaxtyped(typechecker=beartype.beartype)`.
* Variables referring to a filepath should be suffixed with `_fpath`. Directories are `_dpath`.
* Prefer `make` over `build` when naming functions that construct objects, and use `get` when constructing primitives (like string paths or config values).
* Only use `setup` for naming functions that don't return anything.

Throughout the code, variables are annotated with shape suffixes, as [recommended by Noam Shazeer](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd).

The key for these suffixes:

* B: batch size
* W: width in patches (typically 14 or 16)
* H: height in patches (typically 14 or 16)
* D: ViT activation dimension (typically 768 or 1024)
* S: SAE latent dimension (768 x 16, etc)
* L: Number of latents being manipulated at once (typically 1-5 at a time)
* C: Number of classes in ADE20K (151)

For example, an activation tensor with shape (batch, width, height d_vit) is `acts_BWHD`.

>>>> README.md
# saev - Sparse Auto-Encoders for Vision

![Coverage](docs/coverage.svg)

Sparse autoencoders (SAEs) for vision transformers (ViTs), implemented in PyTorch.

This is the codebase used for our preprint "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models"

* [arXiv preprint](https://arxiv.org/abs/2502.06755)
* [Huggingface Models](https://huggingface.co/collections/osunlp/sae-v-67ab8c4fdf179d117db28195)
* [API Docs](https://osu-nlp-group.github.io/saev/api/saev)
* [Demos](https://osu-nlp-group.github.io/saev/#demos)

## About

saev is a package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.
It also includes an interactive webapp for looking through a trained SAE's features.

Originally forked from [HugoFry](https://github.com/HugoFry/mats_sae_training_for_ViTs) who forked it from [Joseph Bloom](https://github.com/jbloomAus/SAELens).

Read [logbook.md](docs/research/logbook.md) for a detailed log of my thought process.

See [related-work.md](saev/related-work.md) for a list of works training SAEs on vision models.
Please open an issue or a PR if there is missing work.

## Installation

Installation is supported with [uv](https://docs.astral.sh/uv/).
saev will likely work with pure pip, conda, etc. but I will not formally support it.

Clone this repository, then from the root directory:

```bash
uv run python -m saev --help
```

This will create a virtual environment and display the CLI help.

## Using `saev`

See the [docs](https://osu-nlp-group.github.io/saev/api/saev) for an overview.

You can ask questions about this repo using the `llms.txt` file.

Example (macOS):

`curl https://osu-nlp-group.github.io/saev/api/llms.txt | pbcopy`, then paste into [Claude](https://claude.ai) or any LLM interface of your choice.

>>>> __init__.py
"""
saev is a Python package for training sparse autoencoders (SAEs) on vision transformers (ViTs) in PyTorch.

The main entrypoint to the package is in `__main__`; use `python -m saev --help` to see the options and documentation for the script.

.. include:: ./guide.md

.. include:: ./inference.md
"""

import importlib.metadata
import pathlib
import tomllib  # std-lib in Python ≥3.11


def _version_from_pyproject() -> str:
    """
    Parse `[project].version` out of pyproject.toml that sits two directories above this file:
        saev/__init__.py
        saev/
        pyproject.toml
    Returns "0.0.0+unknown" on any error.
    """
    try:
        pp = pathlib.Path(__file__).resolve().parents[1] / "pyproject.toml"
        with pp.open("rb") as f:
            data = tomllib.load(f)
        return data["project"]["version"]
    except Exception:  # key missing, file missing, bad TOML, ...
        return "0.0.0+unknown"


try:
    __version__ = importlib.metadata.version("saev")  # installed wheel / editable
except importlib.metadata.PackageNotFoundError:
    __version__ = _version_from_pyproject()  # running from source tree

>>>> __main__.py
import logging
import typing

import beartype
import tyro

from . import config

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)

logger = logging.getLogger("saev")


@beartype.beartype
def visuals(cfg: typing.Annotated[config.Visuals, tyro.conf.arg(name="")]):
    """
    Save maximally activating images for each SAE latent.

    Args:
        cfg: Config
    """
    from . import visuals

    visuals.main(cfg)


if __name__ == "__main__":
    tyro.extras.subcommand_cli_from_dict({"visuals": visuals})
    logger.info("Done.")

>>>> colors.py
"""Utility color palettes used across saev visualizations."""
# https://coolors.co/palette/001219-005f73-0a9396-94d2bd-e9d8a6-ee9b00-ca6702-bb3e03-ae2012-9b2226

BLACK_HEX = "001219"
BLACK_RGB = (0, 18, 25)
BLACK_RGB01 = tuple(c / 256 for c in BLACK_RGB)


BLUE_HEX = "005f73"
BLUE_RGB = (0, 95, 115)
BLUE_RGB01 = tuple(c / 256 for c in BLUE_RGB)


CYAN_HEX = "0a9396"
CYAN_RGB = (10, 147, 150)
CYAN_RGB01 = tuple(c / 256 for c in CYAN_RGB)

SEA_HEX = "94d2bd"
SEA_RGB = (148, 210, 189)
SEA_RGB01 = tuple(c / 256 for c in SEA_RGB)

CREAM_HEX = "e9d8a6"
CREAM_RGB = (233, 216, 166)
CREAM_RGB01 = tuple(c / 256 for c in CREAM_RGB)

GOLD_HEX = "ee9b00"
GOLD_RGB = (238, 155, 0)
GOLD_RGB01 = tuple(c / 256 for c in GOLD_RGB)

ORANGE_HEX = "ca6702"
ORANGE_RGB = (202, 103, 2)
ORANGE_RGB01 = tuple(c / 256 for c in ORANGE_RGB)

RUST_HEX = "bb3e03"
RUST_RGB = (187, 62, 3)
RUST_RGB01 = tuple(c / 256 for c in RUST_RGB)

SCARLET_HEX = "ae2012"
SCARLET_RGB = (174, 32, 18)
SCARLET_RGB01 = tuple(c / 256 for c in SCARLET_RGB)

RED_HEX = "9b2226"
RED_RGB = (155, 34, 38)
RED_RGB01 = tuple(c / 256 for c in RED_RGB)


ALL_HEX = [
    BLACK_HEX,
    BLUE_HEX,
    CYAN_HEX,
    SEA_HEX,
    CREAM_HEX,
    GOLD_HEX,
    ORANGE_HEX,
    RUST_HEX,
    SCARLET_HEX,
    RED_HEX,
]
ALL_RGB01 = [
    BLACK_RGB01,
    BLUE_RGB01,
    CYAN_RGB01,
    SEA_RGB01,
    CREAM_RGB01,
    GOLD_RGB01,
    ORANGE_RGB01,
    RUST_RGB01,
    SCARLET_RGB01,
    RED_RGB01,
]

>>>> data/README.md
# Data Loading Architecture

This directory contains the data loading infrastructure for SAE training and evaluation. The system is designed to efficiently handle large-scale activation datasets stored as binary shards.

## Overview

The data pipeline consists of three main stages:

1. **Activation Generation** (`writers.py`): Extracts and saves ViT activations to disk
2. **Data Storage**: Binary shards with metadata for efficient access
3. **Data Loading**: Multiple dataloader implementations for different use cases

## File Structure

```
data/
├── __init__.py          # Re-exports main classes
├── writers.py           # Activation extraction and shard writing
├── indexed.py           # Random-access dataset for training
├── ordered.py           # Sequential dataloader for evaluation  
├── iterable.py          # Legacy shuffled dataloader
├── images.py            # Image dataset configurations
├── performance.md       # Performance analysis and design decisions
└── README.md           # This file
```

## Key Concepts

### Shape Suffix Convention

Following [Noam Shazeer's recommendation](https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd), tensors are annotated with shape suffixes:

- `B`: Batch size
- `W`: Width in patches (typically 14 or 16)
- `H`: Height in patches (typically 14 or 16)
- `D`: ViT activation dimension (typically 768 or 1024)
- `S`: SAE latent dimension (768 × 16, etc)
- `L`: Number of latents being manipulated
- `C`: Number of classes (e.g., 151 for ADE20K)

Example: `acts_BWHD` has shape `(batch, width, height, d_vit)`

### Shards

Activations are stored in binary files called "shards":
- Each shard contains activations for multiple images
- Format: `acts{NNNNNN}.bin` (e.g., `acts000000.bin`)
- Accompanied by `metadata.json` and `shards.json`
- Uses numpy's binary format for efficient memory-mapped access

### Metadata Files

1. **metadata.json**: Global dataset information
   - Model architecture details (family, checkpoint, layers)
   - Dataset dimensions (n_imgs, n_patches_per_img, d_vit)
   - Storage parameters (max_patches_per_shard)

2. **shards.json**: Per-shard information
   - List of shards with actual image counts
   - Critical for handling non-uniform shard sizes

### Coordinate System

Each activation is identified by:
- `image_i`: Global image index (0 to n_imgs-1)
- `patch_i`: Patch index within the image (0 to n_patches_per_img-1)
- `layer`: ViT layer index

## Dataloader Implementations

### 1. Indexed Dataset (`indexed.py`)

**Use case**: Training with random sampling
- Provides random access to individual activations
- Supports shuffling via PyTorch's DataLoader
- Memory-efficient through memory-mapped files
- Best for: SAE training where random sampling is desired

```python
from saev.data.indexed import Config, Dataset

cfg = Config(shard_root="./shards", layer=13)
dataset = Dataset(cfg)
# Use with torch.utils.data.DataLoader for batching
```

### 2. Ordered DataLoader (`ordered.py`)

**Use case**: Sequential evaluation and debugging
- Guarantees strict sequential order
- Single-threaded design for simplicity
- Process-based with ring buffer for I/O overlap
- Best for: Reproducible evaluation, debugging, visualization

```python
from saev.data.ordered import Config, DataLoader

cfg = Config(shard_root="./shards", layer=13, batch_size=4096)
dataloader = DataLoader(cfg)
for batch in dataloader:
    # Batches arrive in exact sequential order
    pass
```

### 3. Iterable DataLoader (`iterable.py`)

**Use case**: High-throughput training (legacy)
- Multi-process shuffled loading
- Complex buffer management for performance
- Being phased out in favor of indexed dataset
- Best for: Backwards compatibility

## Common Pitfalls and Solutions

### 1. Shard Distribution

**Problem**: Assuming uniform distribution of images across shards
**Solution**: Always use `shards.json` to get actual counts

```python
# Wrong: Assumes uniform distribution
imgs_per_shard = metadata.n_imgs // n_shards

# Right: Use actual distribution
shard_info = ShardInfo.load(shard_root)
imgs_in_shard = shard_info[shard_idx].n_imgs
```

### 2. Index Calculations

**Problem**: Incorrect mapping from global to local indices
**Solution**: Use cumulative offsets

```python
# Calculate cumulative image offsets
cumulative_imgs = [0]
for shard in shard_info:
    cumulative_imgs.append(cumulative_imgs[-1] + shard.n_imgs)

# Find shard for global image index
for i in range(len(cumulative_imgs) - 1):
    if cumulative_imgs[i] <= global_img_i < cumulative_imgs[i + 1]:
        shard_i = i
        local_img_i = global_img_i - cumulative_imgs[i]
        break
```

### 3. CLS Token Handling

**Problem**: Forgetting to account for CLS token in patch indices
**Solution**: Add offset when CLS token is present

```python
# When accessing patches in storage
patch_idx_with_cls = patch_i + int(metadata.cls_token)
activation = mmap[img_i, layer_i, patch_idx_with_cls]
```

## Performance Tips

1. **Batch Size**: Larger batches improve throughput but use more memory
   - Typical sizes: 1024-16384 for training, 4096 for evaluation

2. **Buffer Size**: Controls read-ahead in ordered dataloader
   - Default: 64 batches
   - Increase for more aggressive prefetching

3. **Memory Mapping**: Files are memory-mapped, not loaded
   - OS handles caching automatically
   - Sequential access is highly optimized

4. **Process Spawning**: Use `spawn` start method for multiprocessing
   ```python
   import torch.multiprocessing as mp
   mp.set_start_method("spawn", force=True)
   ```

## Testing

Run tests with activation data:
```bash
pytest tests/test_*.py --shards /path/to/shards/
```

Key test files:
- `test_ordered_dataloader.py`: Ordered dataloader tests
- `test_indexed.py`: Indexed dataset tests
- `test_iterable_dataloader.py`: Legacy dataloader tests

## Future Improvements

1. **Async I/O**: Investigate io_uring or similar for better performance
2. **Compression**: Add optional compression for disk space savings
3. **Distributed Loading**: Support for multi-node training
4. **Streaming**: Direct loading from cloud storage

## Debugging

Enable debug logging:
```python
cfg = Config(..., debug=True)
```

Check shard contents:
```python
import numpy as np
from saev.data.writers import Metadata

metadata = Metadata.load(shard_root)
shape = metadata.shard_shape  # (n_imgs, n_layers, n_patches, d_vit)
mmap = np.memmap("acts000000.bin", mode="r", dtype=np.float32, shape=shape)
print(f"Shard shape: {shape}")
print(f"First activation: {mmap[0, 0, 0, :5]}")  # First 5 dims
```
>>>> data/__init__.py
"""
.. include:: ./protocol.md

.. include:: ./performance.md
"""

from .indexed import Config as IndexedConfig
from .indexed import Dataset
from .ordered import Config as OrderedConfig
from .ordered import DataLoader as OrderedDataLoader
from .shuffled import Config as ShuffledConfig
from .shuffled import DataLoader as ShuffledDataLoader
from .writers import Metadata

__all__ = [
    "IndexedConfig",
    "Dataset",
    "OrderedDataLoader",
    "OrderedConfig",
    "ShuffledDataLoader",
    "ShuffledConfig",
    "Metadata",
]

>>>> data/__main__.py
"""
To save lots of activations, we want to do things in parallel, with lots of slurm jobs, and save multiple files, rather than just one.

This module handles that additional complexity.

Conceptually, activations are either thought of as

1. A single [n_imgs x n_layers x (n_patches + 1), d_vit] tensor. This is a *dataset*
2. Multiple [n_imgs_per_shard, n_layers, (n_patches + 1), d_vit] tensors. This is a set of sharded activations.
"""

import logging
import typing

import beartype
import tyro

from . import images, writers

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)

logger = logging.getLogger("saev.data")


@beartype.beartype
def main(cfg: typing.Annotated[writers.Config, tyro.conf.arg(name="")]):
    """
    Save ViT activations for use later on.

    Args:
        cfg: Configuration for activations.
    """
    logger = logging.getLogger("dump")

    if not cfg.ssl:
        logger.warning("Ignoring SSL certs. Try not to do this!")
        # https://github.com/openai/whisper/discussions/734#discussioncomment-4491761
        # Ideally we don't have to disable SSL but we are only downloading weights.
        import ssl

        ssl._create_default_https_context = ssl._create_unverified_context

    # Run any setup steps.
    images.setup(cfg.data)

    # Actually record activations.
    if cfg.slurm_acct:
        import submitit

        executor = submitit.SlurmExecutor(folder=cfg.log_to)
        executor.update_parameters(
            time=int(cfg.n_hours * 60),
            partition=cfg.slurm_partition,
            gpus_per_node=1,
            ntasks_per_node=1,
            cpus_per_task=cfg.n_workers + 4,
            stderr_to_stdout=True,
            account=cfg.slurm_acct,
        )

        job = executor.submit(writers.worker_fn, cfg)
        logger.info("Running job '%s'.", job.job_id)
        job.result()

    else:
        writers.worker_fn(cfg)


if __name__ == "__main__":
    tyro.cli(main)

>>>> data/buffers.py
# src/saev/data/buffers.py
import collections.abc
import time

import beartype
import numpy as np
import torch
import torch.multiprocessing as mp
from jaxtyping import Shaped, jaxtyped
from torch import Tensor


@beartype.beartype
class RingBuffer:
    """
    Fixed-capacity, multiple-producer / multiple-consumer queue backed by a shared-memory tensor.

    Parameters
    ----------
    slots  : int           capacity in number of items (tensor rows)
    shape  : tuple[int]    shape of one item, e.g. (batch, dim)
    dtype  : torch.dtype   tensor dtype

    put(tensor)  : blocks if full
    get() -> tensor  : blocks if empty
    qsize() -> int        advisory size (approximate)
    close()               frees shared storage (call in the main process)
    """

    def __init__(self, slots: int, shape: tuple[int, ...], dtype: torch.dtype):
        assert slots > 0, "slots must be positive"
        self.slots = slots
        # 123456789 -> Should make you very worried.
        self.buf = torch.full((slots, *shape), 123456789, dtype=dtype)
        self.buf.share_memory_()

        ctx = mp.get_context()  # obeys the global start method ("spawn")

        # shared, lock-free counters
        self.head = ctx.Value("L", 0, lock=False)  # next free slot
        self.tail = ctx.Value("L", 0, lock=False)  # next occupied slot

        # semaphores for blocking semantics
        self.free = ctx.Semaphore(slots)  # initially all slots free
        self.fill = ctx.Semaphore(0)  # no filled slots yet

        # one mutex for pointer updates
        self.mutex = ctx.Lock()

    def put(self, tensor: torch.Tensor) -> None:
        """Copy `tensor` into the next free slot; blocks if the queue is full."""
        if tensor.shape != self.buf.shape[1:] or tensor.dtype != self.buf.dtype:
            raise ValueError("tensor shape / dtype mismatch")

        self.free.acquire()  # wait for a free slot
        with self.mutex:  # exclusive update of head
            idx = self.head.value % self.slots
            self.buf[idx].copy_(tensor)
            self.head.value += 1
        self.fill.release()  # signal there is data

    def get(self) -> torch.Tensor:
        """Return a view of the next item; blocks if the queue is empty."""
        self.fill.acquire()  # wait for data
        with self.mutex:  # exclusive update of tail
            idx = self.tail.value % self.slots
            out = self.buf[idx].clone()
            self.tail.value += 1
        self.free.release()  # signal one more free slot
        return out

    def qsize(self) -> int:
        """Approximate number of filled slots (race-safe enough for tests)."""
        return (self.head.value - self.tail.value) % (1 << 64)

    def fill(self) -> float:
        """Approximate proportion of filled slots (race-safe enough for tests)."""
        return self.qsize() / self.capacity

    def close(self) -> None:
        """Release the shared-memory backing store (call once in the parent)."""
        try:
            self.buf.untyped_storage()._free_shared_mem()
        except (AttributeError, FileNotFoundError):
            pass  # already freed or never allocated


@jaxtyped(typechecker=beartype.beartype)
class ReservoirBuffer:
    """
    Pool of (tensor, meta) pairs.
    Multiple producers call put(batch_x, batch_meta).
    Multiple consumers call get(batch_size) -> (x, meta).
    Random order, each sample delivered once, blocking semantics.
    """

    def __init__(
        self,
        capacity: int,
        shape: tuple[int, ...],
        *,
        dtype: torch.dtype = torch.float32,
        meta_shape: tuple[int, ...] = (2,),
        meta_dtype: torch.dtype = torch.int32,
        seed: int = 0,
        collate_fn: collections.abc.Callable | None = None,
    ):
        self.capacity = capacity
        self._empty = 123456789

        self.data = torch.full((capacity, *shape), self._empty, dtype=dtype)
        self.data.share_memory_()

        self.meta = torch.full((capacity, *meta_shape), self._empty, dtype=meta_dtype)
        self.meta.share_memory_()

        self.ctx = mp.get_context()

        self.size = self.ctx.Value("L", 0)  # current live items
        self.lock = self.ctx.Lock()  # guards size+swap
        self.free = self.ctx.Semaphore(capacity)
        self.full = self.ctx.Semaphore(0)
        # Each process has its own RNG.
        self.rng = np.random.default_rng(seed)

        self.collate_fn = collate_fn

    def put(
        self,
        xs: Shaped[Tensor, "n? ..."],
        metadata: Shaped[Tensor, "n? ..."] | None = None,
    ):
        if xs.dtype != self.data.dtype:
            raise ValueError("tensor dtype mismatch")

        if xs.shape == self.data.shape[1:]:
            # No batch dim, add one
            xs = xs[None, ...]

        elif xs.shape[1:] == self.data.shape[1:]:
            # Already has a batch dim, we're good.
            pass
        else:
            raise ValueError("tensor shape mismatch")

        n, *_ = xs.shape
        if metadata is None:
            metadata = torch.full(
                (n, *self.meta.shape[1:]), self._empty, dtype=self.meta.dtype
            )
        else:
            if len(metadata) != n:
                raise ValueError(
                    f"len(xs) = {len(xs)} != len(metadata) = {len(metadata)}"
                )

        for _ in range(n):
            self.free.acquire()  # block if full

        with self.lock:
            start = self.size.value
            end = start + n
            self.data[start:end].copy_(xs)
            self.meta[start:end].copy_(metadata)
            self.size.value = end

        for _ in range(n):
            self.full.release()

    def get(
        self, bsz: int, timeout: float | None = None
    ) -> tuple[Shaped[Tensor, "bsz ..."], Shaped[Tensor, "bsx ..."]]:
        n_acquired = 0
        deadline = None if timeout is None else time.monotonic() + timeout

        try:
            for _ in range(bsz):
                remaining = (
                    None if deadline is None else max(0.0, deadline - time.monotonic())
                )
                if not self.full.acquire(timeout=remaining):
                    raise TimeoutError  # couldn’t get the whole batch in time
                n_acquired += 1
        except Exception:
            # Roll back any tokens we already grabbed.
            for _ in range(n_acquired):
                self.full.release()
            raise

        out_x = torch.empty((bsz, *self.data.shape[1:]), dtype=self.data.dtype)
        out_m = torch.empty((bsz, *self.meta.shape[1:]), dtype=self.meta.dtype)
        with self.lock:
            for i in range(bsz):
                r = self.rng.integers(self.size.value)
                out_x[i].copy_(self.data[r])
                out_m[i].copy_(self.meta[r])

                last = self.size.value - 1  # swap-with-tail
                if r != last:
                    self.data[r].copy_(self.data[last])
                    self.meta[r].copy_(self.meta[last])
                self.size.value -= 1

        for i in range(bsz):
            self.free.release()

        return out_x, out_m

    def close(self) -> None:
        """Release the shared-memory backing store (call once in the parent)."""
        try:
            self.data.untyped_storage()._free_shared_mem()
        except (AttributeError, FileNotFoundError):
            pass  # already freed or never allocated

    def qsize(self) -> int:
        """Approximate number of filled slots (race-safe enough for tests)."""
        return self.size.value

    def fill(self) -> float:
        """Approximate proportion of filled slots (race-safe enough for tests)."""
        return self.qsize() / self.capacity

>>>> data/config.py
import dataclasses
import os.path
import typing

import beartype


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Activations:
    """
    Configuration for loading activation data from disk.
    """

    shard_root: str = os.path.join(".", "shards")
    """Directory with .bin shards and a metadata.json file."""
    patches: typing.Literal["cls", "image", "all"] = "patches"
    """Which kinds of patches to use."""
    layer: int | typing.Literal["all"] = "all"
    """Which ViT layer(s) to read from disk. ``-2`` selects the second-to-last layer. ``"all"`` enumerates every recorded layer, and ``"meanpool"`` averages activations across layers."""
    clamp: float = 1e5
    """Maximum value for activations; activations will be clamped to within [-clamp, clamp]`."""
    n_random_samples: int = 2**19
    """Number of random samples used to calculate approximate dataset means at startup."""
    scale_mean: bool | str = True
    """Whether to subtract approximate dataset means from examples. If a string, manually load from the filepath."""
    scale_norm: bool | str = True
    """Whether to scale average dataset norm to sqrt(d_vit). If a string, manually load from the filepath."""

>>>> data/filesystem.md
# File System & Performance

## Testing

Correctness:

Reproducibility

Randomness

Performance

## Notes

Read performance for `/fs/scratch` on OSC on an Ascend node:

```
[I] samuelstevens@a0001 ~/p/saev (main)> fio --name=net --filename=/fs/scratch/PAS2136/samuelstevens/cache/saev/366017a10220b85014ae0a594276b25f6ea3d756b74d1d3218da1e34ffcf32e9/acts000000.bin --rw=read --bs=1M --direct=1 --iodepth=16 --runtime=30 --time_based
net: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=psync, iodepth=16
fio-3.35
Starting 1 process
note: both iodepth >= 1 and synchronous I/O engine are selected, queue depth will be capped at 1
Jobs: 1 (f=1): [R(1)][100.0%][r=550MiB/s][r=550 IOPS][eta 00m:00s]
net: (groupid=0, jobs=1): err= 0: pid=3555835: Sun Jun  8 22:34:38 2025
  read: IOPS=796, BW=796MiB/s (835MB/s)(23.3GiB/30001msec)
    clat (usec): min=334, max=14096, avg=1255.50, stdev=747.84
     lat (usec): min=334, max=14096, avg=1255.54, stdev=747.84
    clat percentiles (usec):
     |  1.00th=[  383],  5.00th=[  412], 10.00th=[  420], 20.00th=[  676],
     | 30.00th=[  914], 40.00th=[  922], 50.00th=[  979], 60.00th=[ 1500],
     | 70.00th=[ 1614], 80.00th=[ 1680], 90.00th=[ 2057], 95.00th=[ 2638],
     | 99.00th=[ 3818], 99.50th=[ 4359], 99.90th=[ 5800], 99.95th=[ 6259],
     | 99.99th=[ 9110]
   bw (  KiB/s): min=434176, max=2119680, per=100.00%, avg=820380.20, stdev=403858.41, samples=59
   iops        : min=  424, max= 2070, avg=801.15, stdev=394.39, samples=59
  lat (usec)   : 500=16.82%, 750=8.74%, 1000=27.34%
  lat (msec)   : 2=36.48%, 4=9.84%, 10=0.77%, 20=0.01%
  cpu          : usr=0.06%, sys=2.67%, ctx=23995, majf=0, minf=266
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=23888,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=796MiB/s (835MB/s), 796MiB/s-796MiB/s (835MB/s-835MB/s), io=23.3GiB (25.0GB), run=30001-30001msec
[I] samuelstevens@a0001 ~/p/saev (main)> fio --name=net --filename=/fs/scratch/PAS2136/samuelstevens/cache/saev/366017a10220b85014ae0a594276b25f6ea3d756b74d1d3218da1e34ffcf32e9/acts000000.bin --rw=randread --bs=4K --direct=1 --iodepth=16 --runtime=30 --time_based
net: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=16
fio-3.35
Starting 1 process
note: both iodepth >= 1 and synchronous I/O engine are selected, queue depth will be capped at 1
Jobs: 1 (f=1): [r(1)][100.0%][r=21.4MiB/s][r=5477 IOPS][eta 00m:00s]
net: (groupid=0, jobs=1): err= 0: pid=3557436: Sun Jun  8 22:50:52 2025
  read: IOPS=5865, BW=22.9MiB/s (24.0MB/s)(687MiB/30001msec)
    clat (usec): min=68, max=13344, avg=170.06, stdev=206.09
     lat (usec): min=68, max=13344, avg=170.10, stdev=206.09
    clat percentiles (usec):
     |  1.00th=[   76],  5.00th=[   79], 10.00th=[   82], 20.00th=[   90],
     | 30.00th=[   97], 40.00th=[  103], 50.00th=[  108], 60.00th=[  115],
     | 70.00th=[  206], 80.00th=[  229], 90.00th=[  260], 95.00th=[  306],
     | 99.00th=[ 1139], 99.50th=[ 1598], 99.90th=[ 2442], 99.95th=[ 2769],
     | 99.99th=[ 3589]
   bw (  KiB/s): min=11240, max=34792, per=100.00%, avg=23509.15, stdev=4943.22, samples=59
   iops        : min= 2810, max= 8698, avg=5877.29, stdev=1235.81, samples=59
  lat (usec)   : 100=34.78%, 250=53.34%, 500=9.21%, 750=0.92%, 1000=0.54%
  lat (msec)   : 2=0.98%, 4=0.23%, 10=0.01%, 20=0.01%
  cpu          : usr=0.46%, sys=8.91%, ctx=176070, majf=0, minf=10
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=175978,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: bw=22.9MiB/s (24.0MB/s), 22.9MiB/s-22.9MiB/s (24.0MB/s-24.0MB/s), io=687MiB (721MB), run=30001-30001msec
[I] samuelstevens@a0001 ~/p/saev (main)>
```

>>>> data/images.py
import dataclasses
import logging
import os
import typing
from collections.abc import Callable

import beartype
import torch
import torchvision.datasets
from PIL import Image

logger = logging.getLogger(__name__)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Imagenet:
    """Configuration for HuggingFace Imagenet."""

    name: str = "ILSVRC/imagenet-1k"
    """Dataset name on HuggingFace. Don't need to change this.."""
    split: str = "train"
    """Dataset split. For the default ImageNet-1K dataset, can either be 'train', 'validation' or 'test'."""

    @property
    def n_imgs(self) -> int:
        """Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires loading the dataset. If you need to reference this number very often, cache it in a local variable."""
        import datasets

        dataset = datasets.load_dataset(
            self.name, split=self.split, trust_remote_code=True
        )
        return len(dataset)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ImageFolder:
    """Configuration for a generic image folder dataset."""

    root: str = os.path.join(".", "data", "split")
    """Where the class folders with images are stored."""

    @property
    def n_imgs(self) -> int:
        """Number of images in the dataset. Calculated on the fly, but is non-trivial to calculate because it requires walking the directory structure. If you need to reference this number very often, cache it in a local variable."""
        n = 0
        for _, _, files in os.walk(self.root):
            n += len(files)
        return n


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Ade20k:
    """ """

    root: str = os.path.join(".", "data", "ade20k")
    """Where the class folders with images are stored."""
    split: typing.Literal["training", "validation"] = "training"
    """Data split."""

    @property
    def n_imgs(self) -> int:
        if self.split == "validation":
            return 2000
        else:
            return 20210


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Fake:
    n_imgs: int = 10


Config = Imagenet | ImageFolder | Ade20k | Fake


@beartype.beartype
def setup(cfg: Config):
    """
    Run dataset-specific setup. These setup functions can assume they are the only job running, but they should be idempotent; they should be safe (and ideally cheap) to run multiple times in a row.
    """
    if isinstance(cfg, Imagenet):
        setup_imagenet(cfg)
    elif isinstance(cfg, ImageFolder):
        setup_imagefolder(cfg)
    elif isinstance(cfg, Ade20k):
        setup_ade20k(cfg)
    elif isinstance(cfg, Fake):
        pass
    else:
        typing.assert_never(cfg.data)


@beartype.beartype
def setup_imagenet(cfg: Imagenet):
    pass


@beartype.beartype
def setup_imagefolder(cfg: ImageFolder):
    logger.info("No dataset-specific setup for ImageFolder.")


@beartype.beartype
def setup_ade20k(cfg: Ade20k):
    # url = "http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip"
    pass


@beartype.beartype
def get_dataset(cfg: Config, *, img_transform):
    """
    Gets the dataset for the current experiment; delegates construction to dataset-specific functions.

    Args:
        cfg: Experiment config.
        img_transform: Image transform to be applied to each image.

    Returns:
        A dataset that has dictionaries with `'image'`, `'index'`, `'target'`, and `'label'` keys containing examples.
    """
    if isinstance(cfg, Imagenet):
        return ImagenetDataset(cfg, img_transform=img_transform)
    elif isinstance(cfg, Ade20k):
        return Ade20kDataset(cfg, img_transform=img_transform)
    elif isinstance(cfg, ImageFolder):
        return ImageFolderDataset(cfg.root, transform=img_transform)
    elif isinstance(cfg, Fake):
        return FakeDataset(cfg, img_transform=img_transform)
    else:
        typing.assert_never(cfg)


@beartype.beartype
class ImagenetDataset(torch.utils.data.Dataset):
    def __init__(self, cfg: Imagenet, *, img_transform=None):
        import datasets

        self.hf_dataset = datasets.load_dataset(
            cfg.name, split=cfg.split, trust_remote_code=True
        )

        self.img_transform = img_transform
        self.labels = self.hf_dataset.info.features["label"].names

    def __getitem__(self, i):
        example = self.hf_dataset[i]
        example["index"] = i

        example["image"] = example["image"].convert("RGB")
        if self.img_transform:
            example["image"] = self.img_transform(example["image"])
        example["target"] = example.pop("label")
        example["label"] = self.labels[example["target"]]

        return example

    def __len__(self) -> int:
        return len(self.hf_dataset)


@beartype.beartype
class ImageFolderDataset(torchvision.datasets.ImageFolder):
    def __getitem__(self, index: int) -> dict[str, object]:
        """
        Args:
            index: Index

        Returns:
            dict with keys 'image', 'index', 'target' and 'label'.
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return {
            "image": sample,
            "target": target,
            "label": self.classes[target],
            "index": index,
        }


@beartype.beartype
class Ade20kDataset(torch.utils.data.Dataset):
    @beartype.beartype
    @dataclasses.dataclass(frozen=True)
    class Sample:
        img_path: str
        seg_path: str
        label: str
        target: int

    samples: list[Sample]

    def __init__(
        self,
        cfg: Ade20k,
        *,
        img_transform: Callable | None = None,
        seg_transform: Callable | None = lambda x: None,
    ):
        self.logger = logging.getLogger("ade20k")
        self.cfg = cfg
        self.img_dir = os.path.join(cfg.root, "images")
        self.seg_dir = os.path.join(cfg.root, "annotations")
        self.img_transform = img_transform
        self.seg_transform = seg_transform

        # Check that we have the right path.
        for subdir in ("images", "annotations"):
            if not os.path.isdir(os.path.join(cfg.root, subdir)):
                # Something is missing.
                if os.path.realpath(cfg.root).endswith(subdir):
                    self.logger.warning(
                        "The ADE20K root should contain 'images/' and 'annotations/' directories."
                    )
                raise ValueError(f"Can't find path '{os.path.join(cfg.root, subdir)}'.")

        _, split_mapping = torchvision.datasets.folder.find_classes(self.img_dir)
        split_lookup: dict[int, str] = {
            value: key for key, value in split_mapping.items()
        }
        self.loader = torchvision.datasets.folder.default_loader

        assert cfg.split in set(split_lookup.values())

        # Load all the image paths.
        imgs: list[str] = [
            path
            for path, s in torchvision.datasets.folder.make_dataset(
                self.img_dir,
                split_mapping,
                extensions=torchvision.datasets.folder.IMG_EXTENSIONS,
            )
            if split_lookup[s] == cfg.split
        ]

        segs: list[str] = [
            path
            for path, s in torchvision.datasets.folder.make_dataset(
                self.seg_dir,
                split_mapping,
                extensions=torchvision.datasets.folder.IMG_EXTENSIONS,
            )
            if split_lookup[s] == cfg.split
        ]

        # Load all the targets, classes and mappings
        with open(os.path.join(cfg.root, "sceneCategories.txt")) as fd:
            img_labels: list[str] = [line.split()[1] for line in fd.readlines()]

        label_set = sorted(set(img_labels))
        label_to_idx = {label: i for i, label in enumerate(label_set)}

        self.samples = [
            self.Sample(img_path, seg_path, label, label_to_idx[label])
            for img_path, seg_path, label in zip(imgs, segs, img_labels)
        ]

    def __getitem__(self, index: int) -> dict[str, object]:
        # Convert to dict.
        sample = dataclasses.asdict(self.samples[index])

        sample["image"] = self.loader(sample.pop("img_path"))
        if self.img_transform is not None:
            image = self.img_transform(sample.pop("image"))
            if image is not None:
                sample["image"] = image

        sample["segmentation"] = Image.open(sample.pop("seg_path")).convert("L")
        if self.seg_transform is not None:
            segmentation = self.seg_transform(sample.pop("segmentation"))
            if segmentation is not None:
                sample["segmentation"] = segmentation

        sample["index"] = index

        return sample

    def __len__(self) -> int:
        return len(self.samples)


class FakeDataset(torch.utils.data.Dataset):
    def __init__(self, cfg: Fake, *, img_transform=None):
        self.n_imgs = cfg.n_imgs
        self.img_transform = img_transform

    def __len__(self):
        return self.n_imgs

    def __getitem__(self, i):
        img = Image.new("RGB", (256, 256))
        return {
            "image": self.img_transform(img),
            "index": i,
            "target": 0,
            "label": "dummy",
        }

>>>> data/indexed.py
import dataclasses
import logging
import os
import typing

import beartype
import numpy as np
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from . import writers

logger = logging.getLogger(__name__)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """Configuration for loading indexed activation data from disk."""

    shard_root: str = os.path.join(".", "shards")
    """Directory with .bin shards and a metadata.json file."""
    patches: typing.Literal["cls", "image", "all"] = "image"
    """Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches."""
    layer: int | typing.Literal["all"] = -2
    """Which ViT layer(s) to read from disk. ``-2`` selects the second-to-last layer. ``"all"`` enumerates every recorded layer."""
    seed: int = 17
    """Random seed."""
    debug: bool = False
    """Whether the dataloader process should log debug messages."""


@jaxtyped(typechecker=beartype.beartype)
class Dataset(torch.utils.data.Dataset):
    """
    Dataset of activations from disk.
    """

    class Example(typing.TypedDict):
        """Individual example."""

        act: Float[Tensor, " d_vit"]
        image_i: int
        patch_i: int

    cfg: Config
    """Configuration; set via CLI args."""
    metadata: writers.Metadata
    """Activations metadata; automatically loaded from disk."""
    layer_index: int
    """Layer index into the shards if we are choosing a specific layer."""

    def __init__(self, cfg: Config):
        self.cfg = cfg
        if not os.path.isdir(self.cfg.shard_root):
            raise RuntimeError(f"Activations are not saved at '{self.cfg.shard_root}'.")

        self.metadata = writers.Metadata.load(self.cfg.shard_root)

        # Pick a really big number so that if you accidentally use this when you shouldn't, you get an out of bounds IndexError.
        self.layer_index = 1_000_000
        if isinstance(self.cfg.layer, int):
            err_msg = f"Non-exact matches for .layer field not supported; {self.cfg.layer} not in {self.metadata.layers}."
            assert self.cfg.layer in self.metadata.layers, err_msg
            self.layer_index = self.metadata.layers.index(self.cfg.layer)

    def transform(self, act: Float[np.ndarray, " d_vit"]) -> Float[Tensor, " d_vit"]:
        act = torch.from_numpy(act.copy())
        return act

    @property
    def d_vit(self) -> int:
        """Dimension of the underlying vision transformer's embedding space."""
        return self.metadata.d_vit

    def __getitem__(self, i: int) -> Example:
        # Add bounds checking
        if i < 0 or i >= len(self):
            raise IndexError(
                f"Index {i} out of range for dataset of length {len(self)}"
            )

        match (self.cfg.patches, self.cfg.layer):
            case ("cls", int()):
                img_act = self.get_img_patches(i)
                # Select layer's cls token.
                act = img_act[self.layer_index, 0, :]
                return self.Example(act=self.transform(act), image_i=i, patch_i=-1)
            case ("image", int()):
                # Calculate which image and patch this index corresponds to
                image_i = i // self.metadata.n_patches_per_img
                patch_i = i % self.metadata.n_patches_per_img

                # Calculate shard location
                n_imgs_per_shard = (
                    self.metadata.max_patches_per_shard
                    // len(self.metadata.layers)
                    // (self.metadata.n_patches_per_img + int(self.metadata.cls_token))
                )

                shard = image_i // n_imgs_per_shard
                img_pos_in_shard = image_i % n_imgs_per_shard

                acts_fpath = os.path.join(self.cfg.shard_root, f"acts{shard:06}.bin")
                shape = (
                    n_imgs_per_shard,
                    len(self.metadata.layers),
                    self.metadata.n_patches_per_img + int(self.metadata.cls_token),
                    self.metadata.d_vit,
                )
                acts = np.memmap(acts_fpath, mode="c", dtype=np.float32, shape=shape)

                # Account for CLS token offset when accessing patches
                patch_idx_with_cls = patch_i + int(self.metadata.cls_token)

                # Get the activation
                act = acts[img_pos_in_shard, self.layer_index, patch_idx_with_cls]

                return self.Example(
                    act=self.transform(act),
                    image_i=image_i,
                    patch_i=patch_i,
                )
            case _:
                print((self.cfg.patches, self.cfg.layer))
                typing.assert_never((self.cfg.patches, self.cfg.layer))

    def get_img_patches(
        self, i: int
    ) -> Float[np.ndarray, "n_layers all_patches d_vit"]:
        n_imgs_per_shard = (
            self.metadata.max_patches_per_shard
            // len(self.metadata.layers)
            // (self.metadata.n_patches_per_img + int(self.metadata.cls_token))
        )
        shard = i // n_imgs_per_shard
        pos = i % n_imgs_per_shard
        acts_fpath = os.path.join(self.cfg.shard_root, f"acts{shard:06}.bin")
        shape = (
            n_imgs_per_shard,
            len(self.metadata.layers),
            self.metadata.n_patches_per_img + int(self.metadata.cls_token),
            self.metadata.d_vit,
        )
        acts = np.memmap(acts_fpath, mode="c", dtype=np.float32, shape=shape)
        # Note that this is not yet copied!
        return acts[pos]

    def __len__(self) -> int:
        """
        Dataset length depends on `patches` and `layer`.
        """
        match (self.cfg.patches, self.cfg.layer):
            case ("cls", "all"):
                # Return a CLS token from a random image and random layer.
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("cls", int()):
                # Return a CLS token from a random image and fixed layer.
                return self.metadata.n_imgs
            case ("image", int()):
                # Return a patch from a random image, fixed layer, and random patch.
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            case ("image", "all"):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case _:
                typing.assert_never((self.cfg.patches, self.cfg.layer))

>>>> data/models.py
import logging
import typing
from collections.abc import Callable

import beartype
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from saev import helpers

logger = logging.getLogger(__name__)


@jaxtyped(typechecker=beartype.beartype)
class DinoV2(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        self.model = torch.hub.load("facebookresearch/dinov2", vit_ckpt)
        self.name = f"dinov2/{vit_ckpt}"

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.blocks

    def get_patches(self, n_patches_per_img: int) -> slice:
        n_reg = self.model.num_register_tokens
        patches = torch.cat((
            torch.tensor([0]),  # CLS token
            torch.arange(n_reg + 1, n_reg + 1 + n_patches_per_img),  # patches
        ))
        return patches

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        dct = self.model.forward_features(batch)

        features = torch.cat(
            (dct["x_norm_clstoken"][:, None, :], dct["x_norm_patchtokens"]), axis=1
        )
        return features


@jaxtyped(typechecker=beartype.beartype)
class Clip(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            clip, _ = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            clip, _ = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        model = clip.visual
        model.proj = None
        model.output_tokens = True  # type: ignore
        self.model = model.eval()

        assert not isinstance(self.model, open_clip.timm_model.TimmModel)

        self.name = f"clip/{vit_ckpt}"

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.transformer.resblocks

    def get_patches(self, n_patches_per_img: int) -> slice:
        return slice(None, None, None)

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        return self.model(batch)


@jaxtyped(typechecker=beartype.beartype)
class Siglip(torch.nn.Module):
    def __init__(self, vit_ckpt: str):
        super().__init__()

        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            clip, _ = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            clip, _ = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )

        model = clip.visual
        model.proj = None
        model.output_tokens = True  # type: ignore
        self.model = model

        assert isinstance(self.model, open_clip.timm_model.TimmModel)

        self.name = f"siglip/{vit_ckpt}"

    def get_residuals(self) -> list[torch.nn.Module]:
        return self.model.trunk.blocks

    def get_patches(self, n_patches_per_img: int) -> slice:
        return slice(None, None, None)

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> Float[Tensor, "batch patches dim"]:
        result = self.model(batch)
        return result


@beartype.beartype
def make_vit(vit_family: str, vit_ckpt: str):
    if vit_family == "clip":
        return Clip(vit_ckpt)
    if vit_family == "siglip":
        return Siglip(vit_ckpt)
    elif vit_family == "dinov2":
        return DinoV2(vit_ckpt)
    else:
        typing.assert_never(vit_family)


@beartype.beartype
def make_img_transform(vit_family: str, vit_ckpt: str) -> Callable:
    if vit_family == "clip" or vit_family == "siglip":
        import open_clip

        if vit_ckpt.startswith("hf-hub:"):
            _, img_transform = open_clip.create_model_from_pretrained(
                vit_ckpt, cache_dir=helpers.get_cache_dir()
            )
        else:
            arch, ckpt = vit_ckpt.split("/")
            _, img_transform = open_clip.create_model_from_pretrained(
                arch, pretrained=ckpt, cache_dir=helpers.get_cache_dir()
            )
        return img_transform

    elif vit_family == "dinov2":
        from torchvision.transforms import v2

        return v2.Compose([
            v2.Resize(size=(256, 256)),
            v2.CenterCrop(size=(224, 224)),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250]),
        ])

    else:
        typing.assert_never(vit_family)

>>>> data/ordered.py
# src/saev/data/ordered.py
"""
Ordered (sequential) dataloader for activation data.

This module provides a high-throughput dataloader that reads activation data
from disk shards in sequential order, without shuffling. The implementation uses
a single-threaded manager process to ensure data is delivered in the exact order
it appears on disk.

See the design decisions in src/saev/data/performance.md.

Usage:
    >>> cfg = Config(shard_root="./shards", layer=13, batch_size=4096)
    >>> dataloader = DataLoader(cfg)
    >>> for batch in dataloader:
    ...     activations = batch["act"]  # [batch_size, d_vit]
    ...     image_indices = batch["image_i"]  # [batch_size]
    ...     patch_indices = batch["patch_i"]  # [batch_size]
"""

import collections.abc
import dataclasses
import logging
import math
import os
import queue
import traceback
import typing
from multiprocessing.queues import Queue
from multiprocessing.synchronize import Event

import beartype
import numpy as np
import torch
import torch.multiprocessing as mp
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor

from . import writers


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """Configuration for loading ordered (non-shuffled) activation data from disk."""

    shard_root: str = os.path.join(".", "shards")
    """Directory with .bin shards and a metadata.json file."""
    patches: typing.Literal["cls", "image", "all"] = "image"
    """Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches."""
    layer: int | typing.Literal["all"] = -2
    """Which ViT layer(s) to read from disk. ``-2`` selects the second-to-last layer. ``"all"`` enumerates every recorded layer."""
    batch_size: int = 1024 * 16
    """Batch size."""
    batch_timeout_s: float = 30.0
    """How long to wait for at least one batch."""
    drop_last: bool = False
    """Whether to drop the last batch if it's smaller than the others."""
    buffer_size: int = 64
    """Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls."""
    debug: bool = False
    """Whether the dataloader process should log debug messages."""


@beartype.beartype
class ImageOutOfBoundsError(Exception):
    def __init__(self, metadata: writers.Metadata, i: int):
        self.metadata = metadata
        self.i = i

    @property
    def message(self) -> str:
        return f"Metadata says there are {self.metadata.n_imgs} images, but we found image {self.i}."


@beartype.beartype
def _manager_main(
    cfg: Config,
    metadata: writers.Metadata,
    batch_queue: Queue[dict[str, torch.Tensor]],
    stop_event: Event,
    err_queue: Queue[tuple[str, str]],
):
    """
    The main function for the data loader manager process.
    Reads data sequentially and pushes batches to the queue.
    """
    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if cfg.debug else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger("ordered.manager")
    logger.info("Manager process started.")

    # 0. PRE-CONDITIONS
    if cfg.patches != "image" or not isinstance(cfg.layer, int):
        raise NotImplementedError(
            "High-throughput loader only supports `image` and fixed `layer` mode for now."
        )

    assert cfg.layer in metadata.layers, f"Layer {cfg.layer} not in {metadata.layers}"

    try:
        # Load shard info to get actual distribution
        shard_info = writers.ShardInfo.load(cfg.shard_root)
        layer_i = metadata.layers.index(cfg.layer)

        # Calculate cumulative image offsets for each shard
        cumulative_imgs = [0]
        for shard in shard_info:
            cumulative_imgs.append(cumulative_imgs[-1] + shard.n_imgs)

        # Calculate cumulative sample offsets for each shard
        cumulative_samples = [0]
        for shard in shard_info:
            cumulative_samples.append(
                cumulative_samples[-1] + shard.n_imgs * metadata.n_patches_per_img
            )

        # Calculate total number of samples
        total_samples = metadata.n_imgs * metadata.n_patches_per_img

        # Process batches in order
        current_idx = 0
        while current_idx < total_samples and not stop_event.is_set():
            batch_end_idx = min(current_idx + cfg.batch_size, total_samples)

            # Collect batch activations and metadata
            batch_acts = []
            batch_image_is = []
            batch_patch_is = []

            # Process samples in this batch range
            for idx in range(current_idx, batch_end_idx):
                # Calculate which image and patch this index corresponds to
                global_image_i = idx // metadata.n_patches_per_img
                patch_i = idx % metadata.n_patches_per_img

                # Find which shard contains this image
                shard_i = None
                for i in range(len(cumulative_imgs) - 1):
                    if cumulative_imgs[i] <= global_image_i < cumulative_imgs[i + 1]:
                        shard_i = i
                        break

                if shard_i is None:
                    continue

                # Get local image index within the shard
                local_image_i = global_image_i - cumulative_imgs[shard_i]

                if local_image_i >= shard_info[shard_i].n_imgs:
                    continue

                if global_image_i >= metadata.n_imgs:
                    err = ImageOutOfBoundsError(metadata, global_image_i)
                    logger.warning(err.message)
                    raise err

                # Load activation from the appropriate shard
                fname = f"acts{shard_i:06}.bin"
                acts_fpath = os.path.join(cfg.shard_root, fname)

                # Open mmap for this shard if needed
                mmap = np.memmap(
                    acts_fpath, mode="r", dtype=np.float32, shape=metadata.shard_shape
                )

                # Get the activation
                patch_idx_with_cls = patch_i + int(metadata.cls_token)
                act = torch.from_numpy(
                    mmap[local_image_i, layer_i, patch_idx_with_cls].copy()
                )

                batch_acts.append(act)
                batch_image_is.append(global_image_i)
                batch_patch_is.append(patch_i)

            # Send batch if we have data
            if batch_acts:
                batch = {
                    "act": torch.stack(batch_acts),
                    "image_i": torch.tensor(batch_image_is, dtype=torch.long),
                    "patch_i": torch.tensor(batch_patch_is, dtype=torch.long),
                }
                batch_queue.put(batch)
                logger.debug(f"Sent batch with {len(batch_acts)} samples")

            current_idx = batch_end_idx

    except Exception:
        logger.exception("Fatal error in manager process")
        err_queue.put(("manager", traceback.format_exc()))
    finally:
        logger.info("Manager process finished.")


@beartype.beartype
class DataLoader:
    """
    High-throughput streaming loader that reads data from disk shards in order (no shuffling).
    """

    @jaxtyped(typechecker=beartype.beartype)
    class ExampleBatch(typing.TypedDict):
        """Individual example."""

        act: Float[Tensor, "batch d_vit"]
        image_i: Int[Tensor, " batch"]
        patch_i: Int[Tensor, " batch"]

    def __init__(self, cfg: Config):
        self.cfg = cfg
        if not os.path.isdir(self.cfg.shard_root):
            raise RuntimeError(f"Activations are not saved at '{self.cfg.shard_root}'.")

        self.metadata = writers.Metadata.load(self.cfg.shard_root)

        self.logger = logging.getLogger("ordered.DataLoader")
        self.ctx = mp.get_context()
        self.manager_proc = None
        self.batch_queue = None
        self.stop_event = None
        self._n_samples = self._calculate_n_samples()

    @property
    def n_batches(self) -> int:
        return len(self)

    @property
    def n_samples(self) -> int:
        return self._n_samples

    @property
    def batch_size(self) -> int:
        return self.cfg.batch_size

    @property
    def drop_last(self) -> int:
        return self.cfg.drop_last

    def _start_manager(self):
        # Always shutdown existing manager to ensure fresh start
        if self.manager_proc and self.manager_proc.is_alive():
            self.logger.info("Shutting down existing manager process.")
            self.shutdown()

        self.logger.info("Starting manager process.")

        # Create the batch queue
        self.batch_queue = self.ctx.Queue(maxsize=self.cfg.buffer_size)
        self.stop_event = self.ctx.Event()
        self.err_queue = self.ctx.Queue(maxsize=2)  # Manager + main process

        self.manager_proc = self.ctx.Process(
            target=_manager_main,
            args=(
                self.cfg,
                self.metadata,
                self.batch_queue,
                self.stop_event,
                self.err_queue,
            ),
            daemon=True,
        )
        self.manager_proc.start()

    def __iter__(self) -> collections.abc.Iterable[ExampleBatch]:
        """Yields batches in order."""
        self._start_manager()
        n = 0

        try:
            while n < self.n_samples:
                if not self.err_queue.empty():
                    who, tb = self.err_queue.get_nowait()
                    raise RuntimeError(f"{who} crashed:\n{tb}")

                try:
                    batch = self.batch_queue.get(timeout=self.cfg.batch_timeout_s)
                    actual_batch_size = batch["act"].shape[0]

                    # Handle drop_last
                    if (
                        self.cfg.drop_last
                        and actual_batch_size < self.cfg.batch_size
                        and n + actual_batch_size >= self.n_samples
                    ):
                        break

                    n += actual_batch_size
                    yield self.ExampleBatch(**batch)
                    continue
                except queue.Empty:
                    self.logger.info(
                        "Did not get a batch from manager process in %.1fs seconds.",
                        self.cfg.batch_timeout_s,
                    )

                # If we don't continue, then we should check on the manager process.
                if not self.manager_proc.is_alive():
                    raise RuntimeError(
                        f"Manager process died unexpectedly after {n}/{self.n_samples} samples."
                    )

        finally:
            self.shutdown()

    def shutdown(self):
        if (
            hasattr(self, "stop_event")
            and self.stop_event
            and not self.stop_event.is_set()
        ):
            self.stop_event.set()

        if (
            hasattr(self, "manager_proc")
            and self.manager_proc
            and self.manager_proc.is_alive()
        ):
            self.manager_proc.join(timeout=5.0)
            if self.manager_proc.is_alive():
                self.logger.warning(
                    "Manager process did not shut down cleanly, killing."
                )
                self.manager_proc.kill()

        self.manager_proc = None
        self.batch_queue = None
        self.stop_event = None

    def __del__(self):
        self.shutdown()

    def _calculate_n_samples(self) -> int:
        """Helper to calculate total number of examples based on config."""
        match (self.cfg.patches, self.cfg.layer):
            case ("cls", "all"):
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("cls", int()):
                return self.metadata.n_imgs
            case ("image", int()):
                return self.metadata.n_imgs * self.metadata.n_patches_per_img
            case ("image", "all"):
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case _:
                typing.assert_never((self.cfg.patches, self.cfg.layer))

    def __len__(self) -> int:
        """Returns the number of batches in an epoch."""
        if self.cfg.drop_last:
            return self.n_samples // self.cfg.batch_size
        else:
            return math.ceil(self.n_samples / self.cfg.batch_size)

>>>> data/performance.md
# Performance

SAEs are mostly disk-bound.
Gemma Scope (Google SAE paper) aimed for 1 GB/s to keep their GPUS brrr'ing.
This is pretty hard even with sequential reads, much less random access.

I run all my experiments on [OSC](https://www.osc.edu/) and their scratch filesystem `/fs/scratch` has sequential read speeds of around 800 MB/s and random access speeds around 22 MB/s.

I got these numbers with:

```sh
fio --name=net --filename=/fs/scratch/PAS2136/samuelstevens/cache/saev/366017a10220b85014ae0a594276b25f6ea3d756b74d1d3218da1e34ffcf32e9/acts000000.bin --rw=read --bs=1M --direct=1 --iodepth=16 --runtime=30 --time_based
```

and

```sh
fio --name=net --filename=/fs/scratch/PAS2136/samuelstevens/cache/saev/366017a10220b85014ae0a594276b25f6ea3d756b74d1d3218da1e34ffcf32e9/acts000000.bin --rw=randread --bs=4K --direct=1 --iodepth=16 --runtime=30 --time_based
```

These two commands reported, respectively:

```
READ: bw=796MiB/s (835MB/s), 796MiB/s-796MiB/s (835MB/s-835MB/s), io=23.3GiB (25.0GB), run=30001-30001msec
```

and

```
READ: bw=22.9MiB/s (24.0MB/s), 22.9MiB/s-22.9MiB/s (24.0MB/s-24.0MB/s), io=687MiB (721MB), run=30001-30001msec
```

My naive pytorch-style dataset that uses multiple processes to feed a dataloader did purely random reads and was too slow.
It reports around 500 examples/s:

![Performance plot showing that naive random access dataloading maxes out around 500 examples/s.](assets/benchmarking/ee86c12134a89ea819b129bcce0d1abbda5143c4/plot.png)

I've implemented a dataloader that tries to do sequential reads rather than random reads in `saev/data/iterable.py`.
It's much faster (around 4.5K examples/s) on OSC.

![Performance plot showing that my first attempt at a sequential dataloader maxes out around 4500 examples/s.](assets/benchmarking/4e9b2faf065ffb21e635633a2ee485bd699b0941/plot.png)

I know that it should be even faster; the dataset of 128M examples is 2.9TB, my sequential disk read speed is 800 MB/s, so it should take ~1 hr.
For 128M examples at 4.5K examples/s, it should take 7.9 hours.
You can see this on a [wandb run here](https://wandb.ai/samuelstevens/saev/runs/okm4fv8j?nw=nwusersamuelstevens&panelDisplayName=Disk+Utilization+%28%25%29&panelSectionName=System) which reports 14.6% disk utilization.
Certainly that can be higher.

> *Not sure if this is the correct way to think about it, but: 100 / 14.6 = 6.8, close to 7.9 hours.*

## Ordered Dataloader Design

The `saev/data/ordered.py` module implements a high-throughput ordered dataloader that guarantees sequential data delivery.
This is useful for iterating through all patches in an image at once.

### Key Design Decisions

1. Single-threaded I/O in Manager Process
   
   Initially, the dataloader used multiple worker threads for parallel I/O, similar to PyTorch's DataLoader. However, this created a fundamental ordering problem: when multiple workers read batches in parallel, they complete at different times and deliver batches out of order.
   
   We switched to single-threaded I/O because:
   - Sequential reads from memory-mapped files are already highly optimized by the OS
   - The OS page cache provides excellent performance for sequential access patterns
   - Eliminating multi-threading removes all batch reordering complexity
   - The simpler design is more maintainable and debuggable

2. Process Separation with Ring Buffer
   
   The dataloader still uses a separate manager process connected via a multiprocessing Queue (acting as a ring buffer). This provides:
   - Overlap between I/O and computation
   - Configurable read-ahead via `buffer_size` parameter
   - Natural backpressure when computation is slower than I/O
   - Process isolation for better resource management

3. Shard-Aware Sequential Reading
   
   The dataloader correctly handles the actual distribution of data across shards by:
   - Reading `shards.json` to get the exact number of images per shard
   - Maintaining cumulative offsets for efficient index-to-shard mapping
   - Handling batches that span multiple shards without gaps or duplicates

### Performance Considerations

- Memory-mapped files: Using `np.memmap` allows efficient access to large files without loading them entirely into memory
- Sequential access pattern: The dataloader reads data in the exact order it's stored on disk, maximizing OS cache effectiveness
- Minimal data copying: Activations are copied only once from the memory-mapped file to PyTorch tensors
- Read-ahead buffering: The configurable buffer size allows tuning the trade-off between memory usage and I/O overlap

### Trade-offs

The single-threaded design trades potential parallel I/O throughput for:
- Guaranteed ordering
- Simplicity and maintainability  
- Elimination of synchronization overhead
- Predictable performance characteristics

In practice, the sequential read performance is sufficient for most use cases, especially when the computation (e.g., SAE forward pass) is the bottleneck rather than I/O.

>>>> data/protocol.md
# SAEV Sharded-Activation File Protocol v1 (2025-06-17)

saev caches activations to disk rather than run ViT or LLM inference when training SAEs.
Gemma Scope makes this decision as well (see Section 3.3.2 of https://arxiv.org/pdf/2408.05147).
`saev.data` has a specific protocol to support this in on [OSC](https://www.osc.edu), a super computer center, and take advantage of OSC's specific disk performance. 

Goal: loss-lessly persist very large Transformer (ViT or LLM) activations in a form that is:

* mem-mappable
* Parameterized solely by the *experiment configuration* (`writers.Config`)
* Referenced by a content-hash, so identical configs collide, divergent ones never do
* Can be read quickly in a random order for training, and can be read (slowly) with random-access for visuals.

This document is the single normative source. Any divergence in code is a **bug**.

---

## 1. Directory layout

```
<dump_to>/<HASH>/
    metadata.json              # UTF-8 JSON, human-readable, describes data-generating config
    shards.json                # UTF-8 JSON, human-readable, describes shards.
    acts000000.bin             # shard 0
    acts000001.bin             # shard 1
    ...
    actsNNNNNN.bin             # shard NNNNNN  (zero-padded width=6)
```

*`HASH` = `sha256(json.dumps(metadata, sort_keys=True, separators=(',', ':')).encode('utf-8'))`*
Guards against silent config drift.

---

## 2. JSON file schemas

### 2.1. `metadata.json`

| field                   | type   | semantic                                   |
| ----------------------- | ------ | ------------------------------------------ |
| `vit_family`            | string | `"clip" \| "siglip" \| "dinov2"`           |
| `vit_ckpt`              | string | model identifier (OpenCLIP, HF, etc.)      |
| `layers`                | int[]  | ViT residual‐block indices recorded        |
| `n_patches_per_img`     | int    | **image patches only** (excludes CLS)      |
| `cls_token`             | bool   | `true` -> patch 0 is CLS, else no CLS      |
| `d_vit`                 | int    | activation dimensionality                  |
| `n_imgs`                | int    | total images in dataset                    |
| `max_patches_per_shard` | int    | **logical** activations per shard (see #3) |
| `data`                  | object | opaque dataset description                 |
| `dtype`                 | string | numpy dtype. Fixed `"float32"` for now.    |
| `protocol`              | string | `"1.0.0"` for now.                         |

The `data` object is `dataclasses.asdict(cfg.data)`, with an additional `__class__` field with `cfg.data.__class__.__name__` as the value.

### 2.2. `shards.json`

A single array of `shard` objects, each of which has the following fields:

| field  | type   | semantic                           |
| ------ | ------ | ---------------------------------- |
| name   | string | shard filename (`acts000000.bin`). |
| n_imgs | int    | the number of images in the shard. |

---

## 3 Shard sizing maths

```python
n_tokens_per_img = n_patches_per_img + (1 if cls_token else 0)

n_imgs_per_shard = floor(max_patches_per_shard / (n_tokens_per_img * len(layers)))

shape_per_shard = (
    n_imgs_per_shard, len(layers), n_tokens_per_img, d_vit,
)
```

*`max_patches_per_shard` is a **budget** (default ~2.4 M) chosen so a shard is approximately 10 GiB for Float32 @ `d_vit = 1024`.*

*The last shard will have a smaller value for `n_imgs_per_shard`; this value is documented in `n_imgs` in `shards.json`*

---

## 4. Data Layout and Global Indexing

The entire dataset of activations is treated as a single logical 4D tensor with the shape `(n_imgs, len(layers), n_tokens_per_img, d_vit)`. This logical tensor is C-contiguous with axes ordered `[Image, Layer, Token, Dimension]`.

Physically, this tensor is split along the first axis (`Image`) into multiple shards, where each shard is a single binary file. The number of images in each shard is constant, except for the final shard, which may be smaller.

To locate an arbitrary activation vector, a reader must convert a logical coordinate (`global_img_idx`, `layer_value`, `token_idx`) into a file path and an offset within that file.

### 4.1 Definitions

Let the parameters from `metadata.json` be:

* L = `len(layers)`
* P = `n_patches_per_img`
* T = `P + (1 if cls_token else 0)` (Total tokens per image)
* D = `d_vit`
* S = `n_imgs` from `shards.json` or `n_imgs_per_shard` from Section 3 (shard sizing).

### 4.2 Coordinate Transformations

Given a logical coordinate:

* `global_img_idx`: integer, with `0 <= global_img_idx < n_imgs`
* `layer`: integer, must be an element of `layers`
* `token_idx`: integer, `0 <= token_idx < T`

The physical location is found as follows:

1.  **Identify Shard:**
    * `shard_idx = global_img_idx // S`
    * `img_in_shard = global_img_idx % S`
    The target file is `acts{shard_idx:06d}.bin`.

2.  **Identify Layer Index:** The stored data contains a subset of the ViT's layers. The logical `layer_value` must be mapped to its index in the stored `layers` array.
    * `layer_idx = layers.index(layer)`
    A reader must raise an error if `layer` is not in `layers`.

3.  **Calculate Offset:** The data within a shard is a 4D tensor of shape `(S, L, T, D)`. The offset to the first byte of the desired activation vector `[img_in_shard, layer_in_list_idx, token_idx]` is:
    * `offset_in_vectors = (img_in_shard * L * T) + (layer_in_list_idx * T) + token_idx`
    * `offset_in_bytes = offset_in_vectors * D * 4` (assuming 4 bytes for `float32`)

A reader can then seek to `offset_in_bytes` and read $D \times 4$ bytes to retrieve the vector.

*Alternatively, rather than calculate the offset, readers can memmap the shard, then use Numpy indexing to get the activation vector.*

### 4.3 Token Axis Layout

The `token` axis of length $T$ is ordered as follows:
* If `cls_token` is `true`:
    * Index `0`: [CLS] token activation
    * Indices `1` to $P$: Patch token activations
* If `cls_token` is `false`:
    * Indices `0` to $P-1$: Patch token activations

The relative order of patch tokens is preserved exactly as produced by the upstream Vision Transformer.

---

## 5 Versioning & compatibility

* **Major changes** (shape reorder, dtype switch, new required JSON keys) increment the major protocol version number at the top of this document and must emit a *breaking* warning in loader code.
* **Minor, backward-compatible additions** (new optional JSON key) merely update this doc and the minor protocol version number.

---

That's the whole deal.
No hidden invariants.
Anything else you find in code that contradicts this sheet, fix the code or update the spec.


>>>> data/shuffled.py
# src/saev/data/shuffled.py
# TODO: read https://docs.pytorch.org/tutorials/intermediate/pinmem_nonblock.html
import collections.abc
import dataclasses
import logging
import math
import os
import queue
import threading
import time
import traceback
import typing
from multiprocessing.queues import Queue
from multiprocessing.synchronize import Event

import beartype
import numpy as np
import torch
import torch.multiprocessing as mp
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor

from saev import helpers

from . import buffers, writers


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """Configuration for loading shuffled activation data from disk."""

    shard_root: str = os.path.join(".", "shards")
    """Directory with .bin shards and a metadata.json file."""
    patches: typing.Literal["cls", "image", "all"] = "image"
    """Which kinds of patches to use. 'cls' indicates just the [CLS] token (if any). 'image' indicates it will return image patches. 'all' returns all patches."""
    layer: int | typing.Literal["all"] = -2
    """Which ViT layer(s) to read from disk. ``-2`` selects the second-to-last layer. ``"all"`` enumerates every recorded layer."""
    batch_size: int = 1024 * 16
    """Batch size."""
    drop_last: bool = False
    """Whether to drop the last batch if it's smaller than the others."""
    scale_norm: bool = False
    """Whether to scale norms to sqrt(D)."""
    # Performance
    n_threads: int = 4
    """Number of dataloading threads."""
    buffer_size: int = 64
    """Number of batches to queue in the shared-memory ring buffer. Higher values add latency but improve resilience to brief stalls."""
    batch_timeout_s: float = 30.0
    """How long to wait for at least one batch."""
    # Diagnostics
    seed: int = 17
    """Random seed."""
    debug: bool = False
    """Whether the dataloader process should log debug messages."""
    log_every_s: float = 30.0
    """How frequently the dataloader process should log (debug) performance messages."""


@beartype.beartype
class ImageOutOfBoundsError(Exception):
    def __init__(self, metadata: writers.Metadata, i: int):
        self.metadata = metadata
        self.i = i

    @property
    def message(self) -> str:
        return f"Metadata says there are {self.metadata.n_imgs} images, but we found image {self.i}."


@jaxtyped(typechecker=beartype.beartype)
def _io_worker(
    worker_id: int,
    cfg: Config,
    metadata: writers.Metadata,
    work_queue: queue.Queue[int | None],
    reservoir: buffers.ReservoirBuffer,
    stop_event: threading.Event,
    err_queue: Queue[tuple[str, str]],
):
    """
    Pulls work items from the queue, loads data, and pushes it to the ready queue.
    Work item is a tuple: (shard_idx, list_of_global_indices).

    See https://github.com/beartype/beartype/issues/397 for an explanation of why we use multiprocessing.queues.Queue for the type hint.
    """
    logger = logging.getLogger(f"shuffled.worker{worker_id}")
    logger.info(f"I/O worker {worker_id} started.")

    layer_i = metadata.layers.index(cfg.layer)
    shard_info = writers.ShardInfo.load(cfg.shard_root)

    # Pre-conditions
    assert cfg.patches == "image"
    assert isinstance(cfg.layer, int)

    bytes_sent = 0
    n_reads = 0
    t_last_report = time.time()

    while not stop_event.is_set():
        try:
            shard_i = work_queue.get(timeout=0.1)
            if shard_i is None:  # Poison pill
                logger.debug("Got 'None' from work_queue; exiting.")
                break
            t1 = time.perf_counter()

            fname = f"acts{shard_i:06}.bin"
            logger.info("Opening %s.", fname)

            img_i_offset = shard_i * metadata.n_imgs_per_shard

            acts_fpath = os.path.join(cfg.shard_root, fname)
            mmap = np.memmap(
                acts_fpath, mode="r", dtype=np.float32, shape=metadata.shard_shape
            )
            t2 = time.perf_counter()

            # Only iterate over the actual number of images in this shard
            for start, end in helpers.batched_idx(shard_info[shard_i].n_imgs, 1024):
                for p in range(metadata.n_patches_per_img):
                    patch_i = p + int(metadata.cls_token)
                    t0 = time.perf_counter()
                    acts = torch.from_numpy(mmap[start:end, layer_i, patch_i])
                    t1 = time.perf_counter()

                    last_img_i = img_i_offset + (end - 1)
                    if last_img_i >= metadata.n_imgs:
                        err = ImageOutOfBoundsError(metadata, last_img_i)
                        logger.warning(err.message)
                        raise err

                    meta = torch.full((end - start, 2), p, dtype=torch.int32)
                    meta[:, 0] = img_i_offset + torch.arange(start, end)

                    fill_before = reservoir.fill()
                    reservoir.put(acts, meta)
                    t2 = time.perf_counter()
                    fill_after = reservoir.fill()

                    n_reads += 1
                    bytes_sent += (
                        acts.numel() * acts.element_size()
                        + meta.numel() * meta.element_size()
                    )

                    now = time.time()
                    if now - t_last_report >= cfg.log_every_s:
                        logger.debug(
                            "shard=%s mb_sent=%.1f read_ms=%.2f put_ms=%.2f fill-before=%.3f fill-after=%.3f",
                            shard_i,
                            bytes_sent / 1e6,
                            (t1 - t0) * 1e3,
                            (t2 - t1) * 1e3,
                            fill_before,
                            fill_after,
                        )
                        t_last_report = now
        except queue.Empty:
            # Wait 0.1 seconds for new data.
            time.sleep(0.1)
            continue
        except Exception:
            logger.exception("Error in worker.")
            err_queue.put((f"worker{worker_id}", traceback.format_exc()))
            break
    logger.info("Worker finished.")


@beartype.beartype
def _manager_main(
    cfg: Config,
    metadata: writers.Metadata,
    reservoir: buffers.ReservoirBuffer,
    stop_event: Event,
    err_queue: Queue[tuple[str, str]],
):
    """
    The main function for the data loader manager process.
    """
    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    level = logging.DEBUG if cfg.debug else logging.INFO
    logging.basicConfig(level=level, format=log_format)
    logger = logging.getLogger("shuffled.manager")
    logger.info("Manager process started.")

    # 0. PRE-CONDITIONS
    if cfg.patches != "image" or not isinstance(cfg.layer, int):
        raise NotImplementedError(
            "High-throughput loader only supports `image` and fixed `layer` mode for now."
        )

    assert cfg.layer in metadata.layers, f"Layer {cfg.layer} not in {metadata.layers}"

    # 1. GLOBAL SHUFFLE
    logger.info("Shuffling shards.")
    rng = np.random.default_rng(cfg.seed)
    work_items = rng.permutation(metadata.n_shards)
    logger.debug("First 10 shards: %s", work_items[:10])

    try:
        # 2. SETUP WORK QUEUE & I/O THREADS
        work_queue = queue.Queue()

        for shard_i in work_items:
            work_queue.put(shard_i)

        # Stop objects.
        for _ in range(cfg.n_threads):
            work_queue.put(None)

        threads = []
        thread_stop_event = threading.Event()
        for i in range(cfg.n_threads):
            args = (
                i,
                cfg,
                metadata,
                work_queue,
                reservoir,
                thread_stop_event,
                err_queue,
            )
            thread = threading.Thread(target=_io_worker, args=args, daemon=True)
            thread.start()
            threads.append(thread)
        logger.info("Launched %d I/O threads.", cfg.n_threads)

        # 4. WAIT
        while any(t.is_alive() for t in threads):
            time.sleep(1.0)

    except Exception:
        logger.exception("Fatal error in manager process")
        err_queue.put(("manager", traceback.format_exc()))
    finally:
        # 5. CLEANUP
        logger.info("Manager process shutting down...")
        thread_stop_event.set()
        while not work_queue.empty():
            work_queue.get_nowait()
        for t in threads:
            t.join(timeout=10.0)
        logger.info("Manager process finished.")


@beartype.beartype
class DataLoader:
    """
    High-throughput streaming loader that deterministically shuffles data from disk shards.
    """

    @jaxtyped(typechecker=beartype.beartype)
    class ExampleBatch(typing.TypedDict):
        """Individual example."""

        act: Float[Tensor, "batch d_vit"]
        image_i: Int[Tensor, " batch"]
        patch_i: Int[Tensor, " batch"]

    def __init__(self, cfg: Config):
        self.cfg = cfg

        self.manager_proc = None
        self.reservoir = None
        self.stop_event = None

        self.logger = logging.getLogger("shuffled.DataLoader")
        self.ctx = mp.get_context()

        if not os.path.isdir(self.cfg.shard_root):
            raise RuntimeError(f"Activations are not saved at '{self.cfg.shard_root}'.")

        if self.cfg.scale_norm:
            raise NotImplementedError("scale_norm not implemented.")

        self.metadata = writers.Metadata.load(self.cfg.shard_root)
        self._n_samples = self._calculate_n_samples()

    @property
    def n_batches(self) -> int:
        return len(self)

    @property
    def n_samples(self) -> int:
        return self._n_samples

    @property
    def batch_size(self) -> int:
        return self.cfg.batch_size

    @property
    def drop_last(self) -> int:
        return self.cfg.drop_last

    @property
    def manager_pid(self) -> int:
        if not self.manager_proc or not self.manager_proc.is_alive():
            return -1

        return self.manager_proc.pid

    def _start_manager(self):
        if self.manager_proc and self.manager_proc.is_alive():
            return

        self.logger.info("Starting manager process.")

        # Create the shared-memory buffers
        self.reservoir = buffers.ReservoirBuffer(
            self.cfg.buffer_size * self.cfg.batch_size,
            (self.metadata.d_vit,),
            dtype=torch.float32,
            meta_shape=(2,),
            meta_dtype=torch.int32,
            collate_fn=torch.utils.data.default_collate,
        )
        self.stop_event = self.ctx.Event()
        self.err_queue = self.ctx.Queue(maxsize=self.cfg.n_threads + 1)

        self.manager_proc = self.ctx.Process(
            target=_manager_main,
            args=(
                self.cfg,
                self.metadata,
                self.reservoir,
                self.stop_event,
                self.err_queue,
            ),
            daemon=True,
        )
        self.manager_proc.start()

    def __iter__(self) -> collections.abc.Iterable[ExampleBatch]:
        """Yields batches."""
        self._start_manager()
        n, b = 0, 0

        try:
            while n < self.n_samples:
                need = min(self.cfg.batch_size, self.n_samples - n)
                if not self.err_queue.empty():
                    who, tb = self.err_q.get_nowait()
                    raise RuntimeError(f"{who} crashed:\n{tb}")

                try:
                    act, meta = self.reservoir.get(
                        need, timeout=self.cfg.batch_timeout_s
                    )
                    n += need
                    b += 1
                    image_i, patch_i = meta.T
                    yield self.ExampleBatch(act=act, image_i=image_i, patch_i=patch_i)
                    continue
                except TimeoutError:
                    self.logger.info(
                        "Did not get a batch from %d worker threads in %.1fs seconds.",
                        self.cfg.n_threads,
                        self.cfg.batch_timeout_s,
                    )

                # If we don't continue, then we should check on the manager process.
                if not self.manager_proc.is_alive():
                    raise RuntimeError(
                        f"Manager process died unexpectedly after {b}/{len(self)} batches."
                    )

        finally:
            self.shutdown()

    def shutdown(self):
        if (
            hasattr(self, "stop_event")
            and self.stop_event
            and not self.stop_event.is_set()
        ):
            self.stop_event.set()

        if (
            hasattr(self, "manager_proc")
            and self.manager_proc
            and self.manager_proc.is_alive()
        ):
            self.manager_proc.join(timeout=5.0)
            if self.manager_proc.is_alive():
                self.logger.warning(
                    "Manager process did not shut down cleanly, killing."
                )
                self.manager_proc.kill()

        if hasattr(self, "reservoir") and self.reservoir:
            self.reservoir.close()

        self.manager_proc = None
        self.reservoir = None
        self.stop_event = None

    def __del__(self):
        self.shutdown()

    def _calculate_n_samples(self) -> int:
        """Helper to calculate total number of examples based on config."""
        match (self.cfg.patches, self.cfg.layer):
            case ("cls", "all"):
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("cls", int()):
                return self.metadata.n_imgs
            case ("image", int()):
                return self.metadata.n_imgs * self.metadata.n_patches_per_img
            case ("image", "all"):
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case _:
                typing.assert_never((self.cfg.patches, self.cfg.layer))

    def __len__(self) -> int:
        """Returns the number of batches in an epoch."""
        return math.ceil(self.n_samples / self.cfg.batch_size)

>>>> data/writers.py
# src/saev/data/writers.py
import dataclasses
import hashlib
import json
import logging
import math
import os
import typing
from collections.abc import Callable

import beartype
import numpy as np
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from saev import helpers

from . import images

logger = logging.getLogger(__name__)


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Config:
    """Configuration for calculating and saving ViT activations."""

    data: images.Config = dataclasses.field(default_factory=images.Imagenet)
    """Which dataset to use."""
    dump_to: str = os.path.join(".", "shards")
    """Where to write shards."""
    vit_family: typing.Literal["clip", "siglip", "dinov2"] = "clip"
    """Which model family."""
    vit_ckpt: str = "ViT-L-14/openai"
    """Specific model checkpoint."""
    vit_batch_size: int = 1024
    """Batch size for ViT inference."""
    n_workers: int = 8
    """Number of dataloader workers."""
    d_vit: int = 1024
    """Dimension of the ViT activations (depends on model)."""
    vit_layers: list[int] = dataclasses.field(default_factory=lambda: [-2])
    """Which layers to save. By default, the second-to-last layer."""
    n_patches_per_img: int = 256
    """Number of ViT patches per image (depends on model)."""
    cls_token: bool = True
    """Whether the model has a [CLS] token."""
    max_patches_per_shard: int = 2_400_000
    """Maximum number of activations per shard; 2.4M is approximately 10GB for 1024-dimensional 4-byte activations."""

    ssl: bool = True
    """Whether to use SSL."""

    # Hardware
    device: str = "cuda"
    """Which device to use."""
    n_hours: float = 24.0
    """Slurm job length."""
    slurm_acct: str = ""
    """Slurm account string."""
    slurm_partition: str = ""
    """Slurm partition."""
    log_to: str = "./logs"
    """Where to log Slurm job stdout/stderr."""


@jaxtyped(typechecker=beartype.beartype)
class RecordedVisionTransformer(torch.nn.Module):
    _storage: Float[Tensor, "batch n_layers all_patches dim"] | None
    _i: int

    def __init__(
        self,
        vit: torch.nn.Module,
        n_patches_per_img: int,
        cls_token: bool,
        layers: list[int],
    ):
        super().__init__()

        self.vit = vit

        self.n_patches_per_img = n_patches_per_img
        self.cls_token = cls_token
        self.layers = layers

        self.patches = vit.get_patches(n_patches_per_img)

        self._storage = None
        self._i = 0

        self.logger = logging.getLogger(f"recorder({vit.name})")

        for i in self.layers:
            self.vit.get_residuals()[i].register_forward_hook(self.hook)

    def hook(
        self, module, args: tuple, output: Float[Tensor, "batch n_layers dim"]
    ) -> None:
        if self._storage is None:
            batch, _, dim = output.shape
            self._storage = self._empty_storage(batch, dim, output.device)

        if self._storage[:, self._i, 0, :].shape != output[:, 0, :].shape:
            batch, _, dim = output.shape

            old_batch, _, _, old_dim = self._storage.shape
            msg = "Output shape does not match storage shape: (batch) %d != %d or (dim) %d != %d"
            self.logger.warning(msg, old_batch, batch, old_dim, dim)

            self._storage = self._empty_storage(batch, dim, output.device)

        self._storage[:, self._i] = output[:, self.patches, :].detach()
        self._i += 1

    def _empty_storage(self, batch: int, dim: int, device: torch.device):
        n_patches_per_img = self.n_patches_per_img
        if self.cls_token:
            n_patches_per_img += 1

        return torch.zeros(
            (batch, len(self.layers), n_patches_per_img, dim), device=device
        )

    def reset(self):
        self._i = 0

    @property
    def activations(self) -> Float[Tensor, "batch n_layers all_patches dim"]:
        if self._storage is None:
            raise RuntimeError("First call forward()")
        return self._storage.cpu()

    def forward(
        self, batch: Float[Tensor, "batch 3 width height"]
    ) -> tuple[
        Float[Tensor, "batch patches dim"],
        Float[Tensor, "batch n_layers all_patches dim"],
    ]:
        self.reset()
        result = self.vit(batch)
        return result, self.activations


@beartype.beartype
class ShardWriter:
    """
    ShardWriter is a stateful object that handles sharded activation writing to disk.
    """

    root: str
    shape: tuple[int, int, int, int]
    shard: int
    acts_path: str
    acts: Float[np.ndarray, "n_imgs_per_shard n_layers all_patches d_vit"] | None
    filled: int

    def __init__(self, cfg: Config):
        self.logger = logging.getLogger("shard-writer")

        self.root = get_acts_dir(cfg)

        n_patches_per_img = cfg.n_patches_per_img
        if cfg.cls_token:
            n_patches_per_img += 1
        self.n_imgs_per_shard = (
            cfg.max_patches_per_shard // len(cfg.vit_layers) // n_patches_per_img
        )
        self.shape = (
            self.n_imgs_per_shard,
            len(cfg.vit_layers),
            n_patches_per_img,
            cfg.d_vit,
        )

        # builder for shard manifest
        self._shards: ShardInfo = ShardInfo()

        self.shard = -1
        self.acts = None
        self.next_shard()

    @jaxtyped(typechecker=beartype.beartype)
    def __setitem__(
        self, i: slice, val: Float[Tensor, "_ n_layers all_patches d_vit"]
    ) -> None:
        assert i.step is None
        a, b = i.start, i.stop
        assert len(val) == b - a

        offset = self.n_imgs_per_shard * self.shard

        if b >= offset + self.n_imgs_per_shard:
            # We have run out of space in this mmap'ed file. Let's fill it as much as we can.
            n_fit = offset + self.n_imgs_per_shard - a
            self.acts[a - offset : a - offset + n_fit] = val[:n_fit]
            self.filled = a - offset + n_fit

            self.next_shard()

            # Recursively call __setitem__ in case we need *another* shard
            self[a + n_fit : b] = val[n_fit:]
        else:
            msg = f"0 <= {a} - {offset} <= {offset} + {self.n_imgs_per_shard}"
            assert 0 <= a - offset <= offset + self.n_imgs_per_shard, msg
            msg = f"0 <= {b} - {offset} <= {offset} + {self.n_imgs_per_shard}"
            assert 0 <= b - offset <= offset + self.n_imgs_per_shard, msg
            self.acts[a - offset : b - offset] = val
            self.filled = b - offset

    def flush(self) -> None:
        if self.acts is not None:
            self.acts.flush()

            # record shard info
            self._shards.append(
                Shard(name=os.path.basename(self.acts_path), n_imgs=self.filled)
            )
            self._shards.dump(self.root)

        self.acts = None

    def next_shard(self) -> None:
        self.flush()

        self.shard += 1
        self._count = 0
        self.acts_path = os.path.join(self.root, f"acts{self.shard:06}.bin")
        self.acts = np.memmap(
            self.acts_path, mode="w+", dtype=np.float32, shape=self.shape
        )
        self.filled = 0

        self.logger.info("Opened shard '%s'.", self.acts_path)


@beartype.beartype
def get_acts_dir(cfg: Config) -> str:
    """
    Return the activations directory based on the relevant values of a config.
    Also saves a metadata.json file to that directory for human reference.

    Args:
        cfg: Config for experiment.

    Returns:
        Directory to where activations should be dumped/loaded from.
    """
    metadata = Metadata.from_cfg(cfg)

    acts_dir = os.path.join(cfg.dump_to, metadata.hash)
    os.makedirs(acts_dir, exist_ok=True)

    metadata.dump(acts_dir)

    return acts_dir


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Metadata:
    vit_family: typing.Literal["clip", "siglip", "dinov2"]
    vit_ckpt: str
    layers: tuple[int, ...]
    n_patches_per_img: int
    cls_token: bool
    d_vit: int
    n_imgs: int
    max_patches_per_shard: int
    data: dict[str, object]
    dtype: typing.Literal["float32"] = "float32"
    protocol: typing.Literal["1.0.0"] = "1.0.0"

    def __post_init__(self):
        # Check that at least one image per shard can fit.
        assert self.n_imgs_per_shard >= 1, (
            "At least one image per shard must fit; increase max_patches_per_shard."
        )

    @classmethod
    def from_cfg(cls, cfg: Config) -> "Metadata":
        return cls(
            cfg.vit_family,
            cfg.vit_ckpt,
            tuple(cfg.vit_layers),
            cfg.n_patches_per_img,
            cfg.cls_token,
            cfg.d_vit,
            cfg.data.n_imgs,
            cfg.max_patches_per_shard,
            {**dataclasses.asdict(cfg.data), "__class__": cfg.data.__class__.__name__},
        )

    @classmethod
    def load(cls, shard_root: str) -> "Metadata":
        with open(os.path.join(shard_root, "metadata.json")) as fd:
            dct = json.load(fd)
        dct["layers"] = tuple(dct.pop("layers"))
        return cls(**dct)

    def dump(self, shard_root: str):
        with open(os.path.join(shard_root, "metadata.json"), "w") as fd:
            json.dump(dataclasses.asdict(self), fd, indent=4)

    @property
    def hash(self) -> str:
        cfg_bytes = json.dumps(
            dataclasses.asdict(self), sort_keys=True, separators=(",", ":")
        ).encode("utf-8")
        return hashlib.sha256(cfg_bytes).hexdigest()

    @property
    def n_tokens_per_img(self) -> int:
        return self.n_patches_per_img + int(self.cls_token)

    @property
    def n_shards(self) -> int:
        return math.ceil(self.n_imgs / self.n_imgs_per_shard)

    @property
    def n_imgs_per_shard(self) -> int:
        """
        Calculate the number of images per shard based on the protocol.

        Returns:
            Number of images that fit in a shard.
        """
        n_tokens_per_img = self.n_patches_per_img + (1 if self.cls_token else 0)
        return self.max_patches_per_shard // (n_tokens_per_img * len(self.layers))

    @property
    def shard_shape(self) -> tuple[int, int, int, int]:
        return (
            self.n_imgs_per_shard,
            len(self.layers),
            self.n_tokens_per_img,
            self.d_vit,
        )


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Shard:
    """
    A single shard entry in shards.json, recording the filename and number of images.
    """

    name: str
    n_imgs: int


@beartype.beartype
@dataclasses.dataclass(frozen=True)
class ShardInfo:
    """
    A read-only container for shard metadata as recorded in shards.json.
    """

    shards: list[Shard] = dataclasses.field(default_factory=list)

    @classmethod
    def load(cls, shard_path: str) -> "ShardInfo":
        with open(os.path.join(shard_path, "shards.json")) as fd:
            data = json.load(fd)
        return cls([Shard(**entry) for entry in data])

    def dump(self, fpath: str) -> None:
        with open(os.path.join(fpath, "shards.json"), "w") as fd:
            json.dump([dataclasses.asdict(s) for s in self.shards], fd, indent=2)

    def append(self, shard: Shard):
        self.shards.append(shard)

    def __len__(self) -> int:
        return len(self.shards)

    def __getitem__(self, i):
        return self.shards[i]

    def __iter__(self):
        yield from self.shards


@beartype.beartype
def get_dataloader(
    cfg: Config, *, img_transform: Callable | None = None
) -> torch.utils.data.DataLoader:
    """
    Get a dataloader for a default map-style dataset.

    Args:
        cfg: Config.
        img_transform: Image transform to be applied to each image.

    Returns:
        A PyTorch Dataloader that yields dictionaries with `'image'` keys containing image batches, `'index'` keys containing original dataset indices and `'label'` keys containing label batches.
    """
    dataset = images.get_dataset(cfg.data, img_transform=img_transform)

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=cfg.vit_batch_size,
        drop_last=False,
        num_workers=cfg.n_workers,
        persistent_workers=cfg.n_workers > 0,
        shuffle=False,
        pin_memory=False,
    )
    return dataloader


@beartype.beartype
def worker_fn(cfg: Config):
    """
    Args:
        cfg: Config for activations.
    """
    from . import models

    if torch.cuda.is_available():
        # This enables tf32 on Ampere GPUs which is only 8% slower than
        # float16 and almost as accurate as float32
        # This was a default in pytorch until 1.12
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

    log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    logging.basicConfig(level=logging.INFO, format=log_format)
    logger = logging.getLogger("worker_fn")

    if cfg.device == "cuda" and not torch.cuda.is_available():
        logger.warning("No CUDA device available, using CPU.")
        cfg = dataclasses.replace(cfg, device="cpu")

    vit = models.make_vit(cfg.vit_family, cfg.vit_ckpt).to(cfg.device)
    vit = RecordedVisionTransformer(
        vit, cfg.n_patches_per_img, cfg.cls_token, cfg.vit_layers
    )
    img_transform = models.make_img_transform(cfg.vit_family, cfg.vit_ckpt)
    dataloader = get_dataloader(cfg, img_transform=img_transform)

    writer = ShardWriter(cfg)

    n_batches = cfg.data.n_imgs // cfg.vit_batch_size + 1
    logger.info("Dumping %d batches of %d examples.", n_batches, cfg.vit_batch_size)

    vit = vit.to(cfg.device)
    # vit = torch.compile(vit)

    i = 0
    # Calculate and write ViT activations.
    with torch.inference_mode():
        for batch in helpers.progress(dataloader, total=n_batches):
            imgs = batch.pop("image").to(cfg.device)
            # cache has shape [batch size, n layers, n patches + 1, d vit]
            out, cache = vit(imgs)
            del out

            writer[i : i + len(cache)] = cache
            i += len(cache)

    writer.flush()


@beartype.beartype
class IndexLookup:
    """
    Index <-> shard helper.

    `map()`      – turn a global dataset index into precise physical offsets.
    `length()`   – dataset size for a particular (patches, layer) view.

    Parameters
    ----------
    metadata : Metadata
        Pre-computed dataset statistics (images, patches, layers, shard size).
    patches: 'cls' | 'image' | 'all'
    layer: int | 'all'
    """

    def __init__(
        self,
        metadata: Metadata,
        patches: typing.Literal["cls", "image", "all"],
        layer: int | typing.Literal["all"],
    ):
        if not metadata.cls_token and patches == "cls":
            raise ValueError("Cannot return [CLS] token if one isn't present.")

        self.metadata = metadata
        self.patches = patches

        if isinstance(layer, int) and layer not in metadata.layers:
            raise ValueError(f"Layer {layer} not in {metadata.layers}.")
        self.layer = layer
        self.layer_to_idx = {layer: i for i, layer in enumerate(metadata.layers)}

    def map_global(self, i: int) -> tuple[int, tuple[int, int, int]]:
        """
        Return
        -------
        (
            shard_i,
            index in shard (img_i_in_shard, layer_i, token_i)
        )
        """
        n = self.length()

        if i < 0 or i >= n:
            raise IndexError(f"{i=} out of range [0, {n})")

        match (self.patches, self.layer):
            case ("cls", int()):
                # For CLS token with specific layer, i is the image index
                img_i = i
                shard_i, img_i_in_shard = self.map_img(img_i)
                # CLS token is at position 0
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], 0)
            case ("image", int()):
                # For image patches with specific layer, i is (img_idx * n_patches_per_img + patch_idx)
                img_i = i // self.metadata.n_patches_per_img
                token_i = i % self.metadata.n_patches_per_img

                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
            case ("image", "all"):
                raise NotImplementedError()
            case ("all", int()):
                n_tokens_per_img = self.metadata.n_patches_per_img + (
                    1 if self.metadata.cls_token else 0
                )
                img_i = i // n_tokens_per_img
                token_i = i % n_tokens_per_img
                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, self.layer_to_idx[self.layer], token_i)
            case ("all", "all"):
                # For all tokens (CLS + patches) with all layers
                # Calculate total tokens per image across all layers
                n_tokens_per_img = self.metadata.n_patches_per_img + (
                    1 if self.metadata.cls_token else 0
                )
                total_tokens_per_img = n_tokens_per_img * len(self.metadata.layers)

                # Calculate which image and which token within that image
                img_i = i // total_tokens_per_img
                remainder = i % total_tokens_per_img

                # Calculate which layer and which token within that layer
                layer_idx = remainder // n_tokens_per_img
                token_i = remainder % n_tokens_per_img

                # Map to physical location
                shard_i, img_i_in_shard = self.map_img(img_i)
                return shard_i, (img_i_in_shard, layer_idx, token_i)

            case _:
                typing.assert_never((self.patches, self.layer))

    def map_img(self, img_i: int) -> tuple[int, int]:
        """
        Return
        -------
        (shard_i, img_i_in_shard)
        """
        if img_i < 0 or img_i >= self.metadata.n_imgs:
            raise IndexError(f"{img_i=} out of range [0, {self.metadata.n_imgs})")

        # Calculate which shard contains this image
        shard_i = img_i // self.metadata.n_imgs_per_shard
        img_i_in_shard = img_i % self.metadata.n_imgs_per_shard

        return shard_i, img_i_in_shard

    def length(self) -> int:
        match (self.patches, self.layer):
            case ("cls", "all"):
                # Return a CLS token from a random image and random layer.
                return self.metadata.n_imgs * len(self.metadata.layers)
            case ("cls", int()):
                # Return a CLS token from a random image and fixed layer.
                return self.metadata.n_imgs
            case ("image", int()):
                # Return a patch from a random image, fixed layer, and random patch.
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
                return self.metadata.n_imgs * (self.metadata.n_patches_per_img)
            case ("image", "all"):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * self.metadata.n_patches_per_img
                )
            case ("all", int()):
                # Return a patch from a random image, specific layer and random patch.
                return self.metadata.n_imgs * (
                    self.metadata.n_patches_per_img + int(self.metadata.cls_token)
                )
            case ("all", "all"):
                # Return a patch from a random image, random layer and random patch.
                return (
                    self.metadata.n_imgs
                    * len(self.metadata.layers)
                    * (self.metadata.n_patches_per_img + int(self.metadata.cls_token))
                )
            case _:
                typing.assert_never((self.patches, self.layer))

>>>> extending.md


>>>> guide.md
# Guide to Training SAEs on Vision Models

1. Record ViT activations and save them to disk.
2. Train SAEs on the activations.
3. Visualize the learned features from the trained SAEs.
4. (your job) Propose trends and patterns in the visualized features.
5. (your job, supported by code) Construct datasets to test your hypothesized trends.
6. Confirm/reject hypotheses using `probing` package.

`saev` helps with steps 1, 2 and 3.

.. note:: `saev` assumes you are running on NVIDIA GPUs. On a multi-GPU system, prefix your commands with `CUDA_VISIBLE_DEVICES=X` to run on GPU X.

## Record ViT Activations to Disk

To save activations to disk, we need to specify:

1. Which model we would like to use
2. Which layers we would like to save.
3. Where on disk and how we would like to save activations.
4. Which images we want to save activations for.

The `saev.activations` module does all of this for us.

Run `uv run python -m saev activations --help` to see all the configuration.

In practice, you might run:

```sh
uv run python -m saev.data \
  --vit-family siglip \
  --vit-ckpt hf-hub:timm/ViT-L-16-SigLIP2-256 \
  --d-vit 1024 \
  --n-patches-per-img 256 \
  --no-cls-token \
  --vit-layers 13 15 17 19 21 23 \
  --dump-to /fs/scratch/PAS2136/samuelstevens/cache/saev/ \
  --max-patches-per-shard 500_000 \
  --slurm-acct PAS2136 \
  --n-hours 48 \
  --slurm-partition nextgen \
  data:image-folder \
  --data.root /fs/ess/PAS2136/foundation_model/inat21/raw/train_mini/
```

Let's break down these arguments.



This will save activations for the CLIP-pretrained model ViT-B/32, which has a residual stream dimension of 768, and has 49 patches per image (224 / 32 = 7; 7 x 7 = 49).
It will save the second-to-last layer (`--layer -2`).
It will write 2.4M patches per shard, and save shards to a new directory `/local/scratch/$USER/cache/saev`.


.. note:: A note on storage space: A ViT-B/16 will save 1.2M images x 197 patches/layer/image x 1 layer = ~240M activations, each of which take up 768 floats x 4 bytes/float = 3072 bytes, for a **total of 723GB** for the entire dataset. As you scale to larger models (ViT-L has 1024 dimensions, 14x14 patches are 224 patches/layer/image), recorded activations will grow even larger.

This script will also save a `metadata.json` file that will record the relevant metadata for these activations, which will be read by future steps.
The activations will be in `.bin` files, numbered starting from 000000.

To add your own models, see the guide to extending in `saev.activations`.

## Train SAEs on Activations

To train an SAE, we need to specify:

1. Which activations to use as input.
2. SAE architectural stuff.
3. Optimization-related stuff.

The `saev.training` module handles this.

Run `uv run python -m saev train --help` to see all the configuration.

Continuing on from our example before, you might want to run something like:

```sh
uv run python -m saev train \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  --data.no-scale-mean \
  --data.no-scale-norm \
  --sae.d-vit 768 \
  --lr 5e-4
```

```sh
uv run train.py --sweep configs/preprint/baseline.toml --data.shard-root /fs/scratch/PAS2136/samuelstevens/cache/saev/f9deaa8a07786087e8071f39a695200ff6713ee02b25e7a7b4a6d5ac1ad968db --data.patches image --data.layer 23 --data.no-scale-mean --data.no-scale-norm sae:relu --sae.d-vit 1024
```

`--data.*` flags describe which activations to use.

`--data.shard-root` should point to a directory with `*.bin` files and the `metadata.json` file.
`--data.layer` specifies the layer, and `--data.patches` says that want to train on individual patch activations, rather than the [CLS] token activation.
`--data.no-scale-mean` and `--data.no-scale-norm` mean not to scale the activation mean or L2 norm.
Anthropic's and OpenAI's papers suggest normalizing these factors, but `saev` still has a bug with this, so I suggest not scaling these factors.

`--sae.*` flags are about the SAE itself.

`--sae.d-vit` is the only one you need to change; the dimension of our ViT was 768 for a ViT-B, rather than the default of 1024 for a ViT-L.

Finally, choose a slightly larger learning rate than the default with `--lr 5e-4`.

This will train one (1) sparse autoencoder on the data.
See the section on sweeps to learn how to train multiple SAEs in parallel using only a single GPU.

## Visualize the Learned Features

Now that you've trained an SAE, you probably want to look at its learned features.
One way to visualize an individual learned feature \(f\) is by picking out images that maximize the activation of feature \(f\).
Since we train SAEs on patch-level activations, we try to find the top *patches* for each feature \(f\).
Then, we pick out the images those patches correspond to and create a heatmap based on SAE activation values.

.. note:: More advanced forms of visualization are possible (and valuable!), but should not be included in `saev` unless they can be applied to every SAE/dataset combination. If you have specific visualizations, please add them to `contrib/` or another location.

`saev.visuals` records these maximally activating images for us.
You can see all the options with `uv run python -m saev visuals --help`.

The most important configuration options:

1. The SAE checkpoint that you want to use (`--ckpt`).
2. The ViT activations that you want to use (`--data.*` options, should be roughly the same as the options you used to train your SAE, like the same layer, same `--data.patches`).
3. The images that produced the ViT activations that you want to use (`images` and `--images.*` options, should be the same as what you used to generate your ViT activtions).
4. Some filtering options on which SAE latents to include (`--log-freq-range`, `--log-value-range`, `--include-latents`, `--n-latents`).

Then, the script runs SAE inference on all of the ViT activations, calculates the images with maximal activation for each SAE feature, then retrieves the images from the original image dataset and highlights them for browsing later on.

.. note:: Because of limitations in the SAE training process, not all SAE latents (dimensions of \(f\)) are equally interesting. Some latents are dead, some are *dense*, some only fire on two images, etc. Typically, you want neurons that fire very strongly (high value) and fairly infrequently (low frequency). You might be interested in particular, fixed latents (`--include-latents`). **I recommend using `saev.interactive.metrics` to figure out good thresholds.**

So you might run:

```sh
uv run python -m saev visuals \
  --ckpt checkpoints/abcdefg/sae.pt \
  --dump-to /nfs/$USER/saev/webapp/abcdefg \
  --data.shard-root /local/scratch/$USER/cache/saev/ac89246f1934b45e2f0487298aebe36ad998b6bd252d880c0c9ec5de78d793c8 \
  --data.layer -2 \
  --data.patches patches \
  images:imagenet-dataset
```

This will record the top 128 patches, and then save the unique images among those top 128 patches for each feature in the trained SAE.
It will cache these best activations to disk, then start saving images to visualize later on.

`saev.interactive.features` is a small web application based on [marimo](https://marimo.io/) to interactively look at these images.

You can run it with `uv run marimo edit saev/interactive/features.py`.


## Sweeps

> tl;dr: basically the slow part of training SAEs is loading vit activations from disk, and since SAEs are pretty small compared to other models, you can train a bunch of different SAEs in parallel on the same data using a big GPU. That way you can sweep learning rate, lambda, etc. all on one GPU.

### Why Parallel Sweeps

SAE training optimizes for a unique bottleneck compared to typical ML workflows: disk I/O rather than GPU computation.
When training on vision transformer activations, loading the pre-computed activation data from disk is often the slowest part of the process, not the SAE training itself.

A single set of ImageNet activations for a vision transformer can require terabytes of storage.
Reading this data repeatedly for each hyperparameter configuration would be extremely inefficient.

### Parallelized Training Architecture

To address this bottleneck, we implement parallel training that allows multiple SAE configurations to train simultaneously on the same data batch:

<pre class="mermaid">
flowchart TD
    A[Pre-computed ViT Activations] -->|Slow I/O| B[Memory Buffer]
    B -->|Shared Batch| C[SAE Model 1]
    B -->|Shared Batch| D[SAE Model 2]
    B -->|Shared Batch| E[SAE Model 3]
    B -->|Shared Batch| F[...]
</pre>
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
</script>

This approach:

- Loads each batch of activations **once** from disk
- Uses that same batch for multiple SAE models with different hyperparameters
- Amortizes the slow I/O cost across all models in the sweep

### Running a Sweep

The `train` command accepts a `--sweep` parameter that points to a TOML file defining the hyperparameter grid:

```bash
uv run python -m saev train --sweep configs/my_sweep.toml
```

Here's an example sweep configuration file:

```toml
[sae]
sparsity_coeff = [1e-4, 2e-4, 3e-4]
d_vit = 768
exp_factor = [8, 16]

[data]
scale_mean = true
```

This would train 6 models (3 sparsity coefficients × 2 expansion factors), each sharing the same data loading operation.

### Limitations

Not all parameters can be swept in parallel.
Parameters that affect data loading (like `batch_size` or dataset configuration) will cause the sweep to split into separate parallel groups.
The system automatically handles this division to maximize efficiency.

## Training Metrics and Visualizations

When you train a sweep of SAEs, you probably want to understand which checkpoint is best.
`saev` provides some tools to help with that.

First, we offer a tool to look at some basic summary statistics of all your trained checkpoints.

`saev.interactive.metrics` is a [marimo](https://marimo.io/) notebook (similar to Jupyter, but more interactive) for making L0 vs MSE plots by reading runs off of WandB.

However, there are some pieces of code that need to be changed for you to use it.

.. todo:: Explain how to use the `saev.interactive.metrics` notebook.

* Need to change your wandb username from samuelstevens to USERNAME from wandb
* Tag filter
* Need to run the notebook on the same machine as the original ViT shards and the shards need to be there.
* Think of better ways to do model and data keys
* Look at examples
* run visuals before features

How to run visuals faster?

explain how these features are visualized


>>>> helpers.py
# src/saev/helpers.py
import collections.abc
import dataclasses
import itertools
import logging
import os
import pathlib
import subprocess
import time
import typing

import beartype


@beartype.beartype
class RemovedFeatureError(RuntimeError):
    """Feature existed before but is no longer supported."""

    pass


@beartype.beartype
def get_cache_dir() -> str:
    """
    Get cache directory from environment variables, defaulting to the current working directory (.)

    Returns:
        A path to a cache directory (might not exist yet).
    """
    cache_dir = ""
    for var in ("SAEV_CACHE", "HF_HOME", "HF_HUB_CACHE"):
        cache_dir = cache_dir or os.environ.get(var, "")
    return cache_dir or "."


@beartype.beartype
class progress:
    def __init__(self, it, *, every: int = 10, desc: str = "progress", total: int = 0):
        """
        Wraps an iterable with a logger like tqdm but doesn't use any control codes to manipulate a progress bar, which doesn't work well when your output is redirected to a file. Instead, simple logging statements are used, but it includes quality-of-life features like iteration speed and predicted time to finish.

        Args:
            it: Iterable to wrap.
            every: How many iterations between logging progress.
            desc: What to name the logger.
            total: If non-zero, how long the iterable is.
        """
        self.it = it
        self.every = every
        self.logger = logging.getLogger(desc)
        self.total = total

    def __iter__(self):
        start = time.time()

        try:
            total = len(self)
        except TypeError:
            total = None

        for i, obj in enumerate(self.it):
            yield obj

            if (i + 1) % self.every == 0:
                now = time.time()
                duration_s = now - start
                per_min = (i + 1) / (duration_s / 60)

                if total is not None:
                    pred_min = (total - (i + 1)) / per_min
                    self.logger.info(
                        "%d/%d (%.1f%%) | %.1f it/m (expected finish in %.1fm)",
                        i + 1,
                        total,
                        (i + 1) / total * 100,
                        per_min,
                        pred_min,
                    )
                else:
                    self.logger.info("%d/? | %.1f it/m", i + 1, per_min)

    def __len__(self) -> int:
        if self.total > 0:
            return self.total

        # Will throw exception.
        return len(self.it)


###################
# FLATTENED DICTS #
###################


@beartype.beartype
def flattened(
    dct: dict[str, object], *, sep: str = "."
) -> dict[str, str | int | float | bool | None]:
    """
    Flatten a potentially nested dict to a single-level dict with `.`-separated keys.
    """
    new = {}
    for key, value in dct.items():
        if isinstance(value, dict):
            for nested_key, nested_value in flattened(value).items():
                new[key + "." + nested_key] = nested_value
            continue

        new[key] = value

    return new


@beartype.beartype
def get(dct: dict[str, object], key: str, *, sep: str = ".") -> object:
    key = key.split(sep)
    key = list(reversed(key))

    while len(key) > 1:
        popped = key.pop()
        dct = dct[popped]

    return dct[key.pop()]


@beartype.beartype
class batched_idx:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """

    def __init__(self, total_size: int, batch_size: int):
        """
        Args:
            total_size: total number of examples
            batch_size: maximum distance between the generated indices
        """
        self.total_size = total_size
        self.batch_size = batch_size

    def __iter__(self) -> collections.abc.Iterator[tuple[int, int]]:
        """Yield (start, end) index pairs for batching."""
        for start in range(0, self.total_size, self.batch_size):
            stop = min(start + self.batch_size, self.total_size)
            yield start, stop

    def __len__(self) -> int:
        """Return the number of batches."""
        return (self.total_size + self.batch_size - 1) // self.batch_size


#################
# SWEEP HELPERS #
#################


T = typing.TypeVar("T")


@beartype.beartype
def expand(config: dict[str, object]) -> collections.abc.Iterator[dict[str, object]]:
    """Expand a nested dict that may contain lists into many dicts."""

    yield from _expand_discrete(dict(config))


@beartype.beartype
def _expand_discrete(
    config: dict[str, object],
) -> collections.abc.Iterator[dict[str, object]]:
    if not config:
        yield {}
        return

    key, value = config.popitem()

    if isinstance(value, list):
        for c in _expand_discrete(config):
            for v in value:
                yield {**c, key: v}
    elif isinstance(value, dict):
        for c, v in itertools.product(
            _expand_discrete(config), _expand_discrete(value)
        ):
            yield {**c, key: v}
    else:
        for c in _expand_discrete(config):
            yield {**c, key: value}


@beartype.beartype
def grid(cfg: T, sweep_dct: dict[str, object]) -> tuple[list[T], list[str]]:
    """Generate configs from ``cfg`` according to ``sweep_dct``."""

    cfgs: list[T] = []
    errs: list[str] = []

    for d, dct in enumerate(expand(sweep_dct)):
        updates = {}
        for key, value in dct.items():
            attr = getattr(cfg, key)
            if dataclasses.is_dataclass(attr) and isinstance(value, dict):
                sub_updates = dict(value)
                if hasattr(attr, "seed"):
                    sub_updates["seed"] = (
                        sub_updates.get("seed", attr.seed) + getattr(cfg, "seed", 0) + d
                    )
                updates[key] = dataclasses.replace(attr, **sub_updates)
            else:
                updates[key] = value

        if hasattr(cfg, "seed"):
            updates.setdefault("seed", getattr(cfg, "seed") + d)

        try:
            cfgs.append(dataclasses.replace(cfg, **updates))
        except Exception as err:  # pragma: no cover - passthrough for caller
            errs.append(str(err))

    return cfgs, errs


@beartype.beartype
def current_git_commit() -> str | None:
    """
    Best-effort short SHA of the repo containing *this* file.

    Returns `None` when
    * `git` executable is missing,
    * we’re not inside a git repo (e.g. installed wheel),
    * or any git call errors out.
    """
    try:
        # Walk up until we either hit a .git dir or the FS root
        here = pathlib.Path(__file__).resolve()
        for parent in (here, *here.parents):
            if (parent / ".git").exists():
                break
        else:  # no .git found
            return None

        result = subprocess.run(
            ["git", "-C", str(parent), "rev-parse", "--short", "HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            check=True,
        )
        return result.stdout.strip() or None
    except (FileNotFoundError, subprocess.CalledProcessError):
        return None

>>>> imaging.py
import math

import beartype
import matplotlib
import numpy as np
from jaxtyping import Float, jaxtyped
from PIL import Image, ImageDraw

colormap = matplotlib.colormaps.get_cmap("plasma")


@jaxtyped(typechecker=beartype.beartype)
def add_highlights(
    img: Image.Image,
    patches: Float[np.ndarray, " n_patches"],
    *,
    upper: float | None = None,
    opacity: float = 0.9,
) -> Image.Image:
    if not len(patches):
        return img

    iw_np, ih_np = int(math.sqrt(len(patches))), int(math.sqrt(len(patches)))
    iw_px, ih_px = img.size
    pw_px, ph_px = iw_px // iw_np, ih_px // ih_np
    assert iw_np * ih_np == len(patches)

    # Create a transparent overlay
    overlay = Image.new("RGBA", img.size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(overlay)

    colors = (colormap(patches / (upper + 1e-9))[:, :3] * 256).astype(np.uint8)

    for p, (val, color) in enumerate(zip(patches, colors)):
        assert upper is not None
        val /= upper + 1e-9
        x_np, y_np = p % iw_np, p // ih_np
        draw.rectangle(
            [
                (x_np * pw_px, y_np * ph_px),
                (x_np * pw_px + pw_px, y_np * ph_px + ph_px),
            ],
            fill=(*color, int(opacity * val * 256)),
        )

    # Composite the original image and the overlay
    return Image.alpha_composite(img.convert("RGBA"), overlay)

>>>> inference.md
# Inference Instructions

Briefly, you need to:

1. Download a checkpoint.
2. Get the code.
3. Load the checkpoint.
4. Get activations.

Details are below.

## Download a Checkpoint

First, download an SAE checkpoint from the [Huggingface collection](https://huggingface.co/collections/osunlp/sae-v-67ab8c4fdf179d117db28195).

For instance, you can choose the SAE trained on OpenAI's CLIP ViT-B/16 with ImageNet-1K activations [here](https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K).

You can use `wget` if you want:

```sh
wget https://huggingface.co/osunlp/SAE_CLIP_24K_ViT-B-16_IN1K/resolve/main/sae.pt
```

## Get the Code

The easiest way to do this is to clone the code:

```
git clone https://github.com/OSU-NLP-Group/saev
```

You can also install the package from git if you use uv (not sure about pip or cuda):

```sh
uv add git+https://github.com/OSU-NLP-Group/saev
```

Or clone it and install it as an editable with pip, lik `pip install -e .` in your virtual environment.

Then you can do things like `from saev import ...`.

.. note::
  If you struggle to get `saev` installed, open an issue on [GitHub](https://github.com/OSU-NLP-Group/saev) and I will figure out how to make it easier.

## Load the Checkpoint

```py
import saev.nn

sae = saev.nn.load("PATH_TO_YOUR_SAE_CKPT.pt")
```

Now you have a pretrained SAE.

## Get Activations

This is the hardest part.
We need to:

1. Pass an image into a ViT
2. Record the dense ViT activations at the same layer that the SAE was trained on.
3. Pass the activations into the SAE to get sparse activations.
4. Do something interesting with the sparse SAE activations.

There are examples of this in the demo code: for [classification](https://huggingface.co/spaces/samuelstevens/saev-image-classification/blob/main/app.py#L318) and [semantic segmentation](https://huggingface.co/spaces/samuelstevens/saev-semantic-segmentation/blob/main/app.py#L222).
If the permalinks change, you are looking for the `get_sae_latents()` functions in both files.

Below is example code to do it using the `saev` package.

```py
import saev.nn
import saev.activations

img_transform = saev.activations.make_img_transform("clip", "ViT-B-16/openai")

vit = saev.activations.make_vit("clip", "ViT-B-16/openai")
recorded_vit = saev.activations.RecordedVisionTransformer(vit, 196, True, [10])

img = Image.open("example.jpg")

x = img_transform(img)
# Add a batch dimension
x = x[None, ...]
_, vit_acts = recorded_vit(x)
# Select the only layer in the batch and ignore the CLS token.
vit_acts = vit_acts[:, 0, 1:, :]

x_hat, f_x, loss = sae(vit_acts)
```

Now you have the reconstructed x (`x_hat`) and the sparse representation of all patches in the image (`f_x`).

You might select the dimensions with maximal values for each patch and see what other images are maximimally activating.

.. todo::
  Provide documentation for how get maximally activating images.


>>>> interactive/features.py
import marimo

__generated_with = "0.9.32"
app = marimo.App(width="full")


@app.cell
def __():
    import json
    import os
    import random

    import marimo as mo
    import matplotlib.pyplot as plt
    import numpy as np
    import polars as pl
    import torch
    import tqdm

    return json, mo, np, os, pl, plt, random, torch, tqdm


@app.cell
def __(mo, os):
    def make_ckpt_dropdown():
        try:
            choices = sorted(
                os.listdir("/research/nfs_su_809/workspace/stevens.994/saev/features")
            )

        except FileNotFoundError:
            choices = []

        return mo.ui.dropdown(choices, label="Checkpoint:")

    ckpt_dropdown = make_ckpt_dropdown()
    return ckpt_dropdown, make_ckpt_dropdown


@app.cell
def __(ckpt_dropdown, mo):
    mo.hstack([ckpt_dropdown], justify="start")
    return


@app.cell
def __(ckpt_dropdown, mo):
    mo.stop(
        ckpt_dropdown.value is None,
        mo.md(
            "Run `uv run main.py webapp --help` to fill out at least one checkpoint."
        ),
    )

    webapp_dir = f"/research/nfs_su_809/workspace/stevens.994/saev/features/{ckpt_dropdown.value}/sort_by_patch"

    get_i, set_i = mo.state(0)
    return get_i, set_i, webapp_dir


@app.cell
def __(mo):
    sort_by_freq_btn = mo.ui.run_button(label="Sort by frequency")

    sort_by_value_btn = mo.ui.run_button(label="Sort by value")

    sort_by_latent_btn = mo.ui.run_button(label="Sort by latent")
    return sort_by_freq_btn, sort_by_latent_btn, sort_by_value_btn


@app.cell
def __(mo, sort_by_freq_btn, sort_by_latent_btn, sort_by_value_btn):
    mo.hstack(
        [sort_by_freq_btn, sort_by_value_btn, sort_by_latent_btn], justify="start"
    )
    return


@app.cell
def __(
    json,
    mo,
    os,
    sort_by_freq_btn,
    sort_by_latent_btn,
    sort_by_value_btn,
    tqdm,
    webapp_dir,
):
    def get_neurons() -> list[dict]:
        rows = []
        for name in tqdm.tqdm(list(os.listdir(f"{webapp_dir}/neurons"))):
            if not name.isdigit():
                continue
            try:
                with open(f"{webapp_dir}/neurons/{name}/metadata.json") as fd:
                    rows.append(json.load(fd))
            except FileNotFoundError:
                print(f"Missing metadata.json for neuron {name}.")
                continue
            # rows.append({"neuron": int(name)})
        return rows

    neurons = get_neurons()

    if sort_by_latent_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["neuron"])
    elif sort_by_freq_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["log10_freq"])
    elif sort_by_value_btn.value:
        neurons = sorted(neurons, key=lambda dct: dct["log10_value"], reverse=True)

    mo.md(f"Found {len(neurons)} saved neurons.")
    return get_neurons, neurons


@app.cell
def __(mo, neurons, set_i):
    next_button = mo.ui.button(
        label="Next",
        on_change=lambda _: set_i(lambda v: (v + 1) % len(neurons)),
    )

    prev_button = mo.ui.button(
        label="Previous",
        on_change=lambda _: set_i(lambda v: (v - 1) % len(neurons)),
    )
    return next_button, prev_button


@app.cell
def __(get_i, mo, neurons, set_i):
    neuron_slider = mo.ui.slider(
        0,
        len(neurons),
        value=get_i(),
        on_change=lambda i: set_i(i),
        full_width=True,
    )
    return (neuron_slider,)


@app.cell
def __():
    return


@app.cell
def __(
    display_info,
    get_i,
    mo,
    neuron_slider,
    neurons,
    next_button,
    prev_button,
):
    # label = f"Neuron {neurons[get_i()]} ({get_i()}/{len(neurons)}; {get_i() / len(neurons) * 100:.2f}%)"
    # , display_info(**neurons[get_i()])
    mo.md(f"""
    {mo.hstack([prev_button, next_button, display_info(**neurons[get_i()])], justify="start")}
    {neuron_slider}
    """)
    return


@app.cell
def __():
    return


@app.cell
def __(get_i, mo, neurons):
    def display_info(log10_freq: float, log10_value: float, neuron: int):
        return mo.md(
            f"Neuron {neuron} ({get_i()}/{len(neurons)}; {get_i() / len(neurons) * 100:.1f}%) | Frequency: {10**log10_freq * 100:.3f}% of inputs | Mean Value: {10**log10_value:.3f}"
        )

    return (display_info,)


@app.cell
def __(mo, webapp_dir):
    def show_img(n: int, i: int):
        label = "No label found."
        try:
            label = open(f"{webapp_dir}/neurons/{n}/{i}.txt").read().strip()
            label = " ".join(label.split("_"))
        except FileNotFoundError:
            return mo.md(f"*Missing image {i + 1}*")

        return mo.vstack([mo.image(f"{webapp_dir}/neurons/{n}/{i}.png"), mo.md(label)])

    return (show_img,)


@app.cell
def __(get_i, mo, neurons, show_img):
    n = neurons[get_i()]["neuron"]

    mo.vstack([
        mo.hstack(
            [
                show_img(n, 0),
                show_img(n, 1),
                show_img(n, 2),
                show_img(n, 3),
                show_img(n, 4),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 5),
                show_img(n, 6),
                show_img(n, 7),
                show_img(n, 8),
                show_img(n, 9),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 10),
                show_img(n, 11),
                show_img(n, 12),
                show_img(n, 13),
                show_img(n, 14),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 15),
                show_img(n, 16),
                show_img(n, 17),
                show_img(n, 18),
                show_img(n, 19),
            ],
            widths="equal",
        ),
        mo.hstack(
            [
                show_img(n, 20),
                show_img(n, 21),
                show_img(n, 22),
                show_img(n, 23),
                show_img(n, 24),
            ],
            widths="equal",
        ),
    ])
    return (n,)


@app.cell
def __(os, torch, webapp_dir):
    sparsity_fpath = os.path.join(webapp_dir, "sparsity.pt")
    sparsity = torch.load(sparsity_fpath, weights_only=True, map_location="cpu")

    values_fpath = os.path.join(webapp_dir, "mean_values.pt")
    values = torch.load(values_fpath, weights_only=True, map_location="cpu")
    return sparsity, sparsity_fpath, values, values_fpath


@app.cell
def __(mo, np, plt, sparsity):
    def plot_hist(counts):
        fig, ax = plt.subplots()
        ax.hist(np.log10(counts.numpy() + 1e-9), bins=100)
        return fig

    mo.md(f"""
    Sparsity Log10

    {mo.as_html(plot_hist(sparsity))}
    """)
    return (plot_hist,)


@app.cell
def __(mo, plot_hist, values):
    mo.md(f"""
    Mean Value Log10

    {mo.as_html(plot_hist(values))}
    """)
    return


@app.cell
def __(np, plt, sparsity, values):
    def plot_dist(
        min_log_sparsity: float,
        max_log_sparsity: float,
        min_log_value: float,
        max_log_value: float,
    ):
        fig, ax = plt.subplots()

        log_sparsity = np.log10(sparsity.numpy() + 1e-9)
        log_values = np.log10(values.numpy() + 1e-9)

        mask = np.ones(len(log_sparsity)).astype(bool)
        mask[log_sparsity < min_log_sparsity] = False
        mask[log_sparsity > max_log_sparsity] = False
        mask[log_values < min_log_value] = False
        mask[log_values > max_log_value] = False

        n_shown = mask.sum()
        ax.scatter(
            log_sparsity[mask],
            log_values[mask],
            marker=".",
            alpha=0.1,
            color="tab:blue",
            label=f"Shown ({n_shown})",
        )
        n_filtered = (~mask).sum()
        ax.scatter(
            log_sparsity[~mask],
            log_values[~mask],
            marker=".",
            alpha=0.1,
            color="tab:red",
            label=f"Filtered ({n_filtered})",
        )

        ax.axvline(min_log_sparsity, linewidth=0.5, color="tab:red")
        ax.axvline(max_log_sparsity, linewidth=0.5, color="tab:red")
        ax.axhline(min_log_value, linewidth=0.5, color="tab:red")
        ax.axhline(max_log_value, linewidth=0.5, color="tab:red")

        ax.set_xlabel("Feature Frequency (log10)")
        ax.set_ylabel("Mean Activation Value (log10)")
        ax.legend(loc="upper right")

        return fig

    return (plot_dist,)


@app.cell
def __(mo, plot_dist, sparsity_slider, value_slider):
    mo.md(f"""
    Log Sparsity Range: {sparsity_slider}
    {sparsity_slider.value}

    Log Value Range: {value_slider}
    {value_slider.value}

    {mo.as_html(plot_dist(sparsity_slider.value[0], sparsity_slider.value[1], value_slider.value[0], value_slider.value[1]))}
    """)
    return


@app.cell
def __(mo):
    sparsity_slider = mo.ui.range_slider(start=-8, stop=0, step=0.1, value=[-6, -1])
    return (sparsity_slider,)


@app.cell
def __(mo):
    value_slider = mo.ui.range_slider(start=-3, stop=1, step=0.1, value=[-0.75, 1.0])
    return (value_slider,)


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


@app.cell
def __():
    return


if __name__ == "__main__":
    app.run()

>>>> interactive/metrics.py
import marimo

__generated_with = "0.9.32"
app = marimo.App(width="medium")


@app.cell
def __():
    import itertools
    import json
    import math
    import os

    import altair as alt
    import beartype
    import marimo as mo
    import matplotlib.pyplot as plt
    import numpy as np
    import polars as pl
    import wandb
    from adjustText import adjust_text
    from jaxtyping import Float, jaxtyped

    import saev.colors

    return (
        Float,
        adjust_text,
        alt,
        beartype,
        itertools,
        jaxtyped,
        json,
        math,
        mo,
        np,
        os,
        pl,
        plt,
        saev,
        wandb,
    )


@app.cell
def __(mo):
    mo.md(
        """
        # SAE Metrics Explorer

        This notebook helps you analyze and compare SAE training runs from WandB.

        ## Setup Instructions

        1. Edit the configuration cell at the top to set your WandB username and project
        2. Make sure you have access to the original ViT activation shards
        3. Use the filters to narrow down which models to compare

        ## Troubleshooting

        - **Missing data error**: This notebook needs access to the original ViT activation shards
        - **No runs found**: Check your WandB username, project name, and tag filter
        """
    )
    return


@app.cell
def __():
    WANDB_USERNAME = "samuelstevens"
    WANDB_PROJECT = "saev"
    return WANDB_PROJECT, WANDB_USERNAME


@app.cell
def __(mo):
    tag_input = mo.ui.text(value="classification-v1.0", label="Sweep Tag:")
    return (tag_input,)


@app.cell
def __(WANDB_PROJECT, WANDB_USERNAME, mo, tag_input):
    mo.vstack([
        mo.md(
            f"Look at [{WANDB_USERNAME}/{WANDB_PROJECT} on WandB](https://wandb.ai/{WANDB_USERNAME}/{WANDB_PROJECT}/table) to pick your tag."
        ),
        tag_input,
    ])
    return


@app.cell
def __(df, mo):
    # All unique (model, layer) pairs
    pairs = (
        df.select(["model_key", "config/data/layer"])
        .unique()
        .sort(by=["model_key", "config/data/layer"])
        .iter_rows()
    )

    pair_elems = {
        f"{model}|{layer}": mo.ui.switch(value=True, label=f"{model} / layer {layer}")
        for model, layer in pairs
    }
    pair_dict = mo.ui.dictionary(pair_elems)  # ★ reactive wrapper ★

    # Global toggle for non-frontier ("rest") points
    show_rest = mo.ui.switch(value=True, label="Show non-frontier points")
    show_ids = mo.ui.switch(value=False, label="Annotate Pareto points")

    def _make_grid(elems, ncols: int, gap: float):
        return mo.hstack(
            [
                mo.vstack(elems[i : i + ncols], gap=gap, justify="start")
                for i in range(0, len(elems), ncols)
            ],
            gap=gap,
        )

    elems = [*pair_dict.elements.values(), show_rest, show_ids]
    ui_grid = _make_grid(elems, 3, 0.5)
    ui_grid
    return elems, pair_dict, pair_elems, pairs, show_ids, show_rest, ui_grid


@app.cell
def __(
    adjust_text,
    df,
    itertools,
    mo,
    pair_dict,
    pl,
    plt,
    saev,
    show_ids,
    show_rest,
):
    def plot_layerwise(
        df: pl.DataFrame,
        show_rest: bool,
        show_ids: bool,
        l0_col: str = "summary/eval/l0",
        mse_col: str = "summary/eval/mse",
        layer_col: str = "config/data/layer",
        model_col: str = "model_key",
    ):
        """
        Plot Pareto frontiers (L0 vs MSE) for every (layer, model) pair using **polars** only.
        """

        fig, ax = plt.subplots(figsize=(7, 4), dpi=300)

        linestyles = ["-", "--", ":", "-."]
        colors = [
            saev.colors.CYAN_RGB01,
            saev.colors.SEA_RGB01,
            saev.colors.CREAM_RGB01,
            saev.colors.GOLD_RGB01,
            saev.colors.ORANGE_RGB01,
            saev.colors.RUST_RGB01,
            saev.colors.SCARLET_RGB01,
            saev.colors.RED_RGB01,
        ]
        markers = ["X", "o", "+"]

        models = sorted(df.select(model_col).unique().get_column(model_col).to_list())

        texts = []

        for model, marker in zip(models, itertools.cycle(markers)):
            model_df = df.filter(pl.col(model_col) == model)

            layers = model_df.select(layer_col).unique().get_column(layer_col).to_list()
            for layer, color, linestyle in zip(
                sorted(layers),
                itertools.cycle(colors),
                itertools.cycle(linestyles),
            ):
                group = (
                    model_df.filter(pl.col(layer_col) == layer)
                    .sort(l0_col)
                    .with_columns(pl.col(mse_col).cum_min().alias("cummin_mse"))
                )

                pareto = group.filter(pl.col(mse_col) == pl.col("cummin_mse"))
                ids = pareto.get_column("id").to_list()

                xs = pareto.get_column(l0_col).to_numpy()
                ys = pareto.get_column(mse_col).to_numpy()

                line, *_ = ax.plot(
                    xs,
                    ys,
                    color=color,
                    linestyle=linestyle,
                    marker=marker,
                    alpha=0.8,
                    label=f"{model} / layer {layer}",
                )

                if show_ids:  # <-- annotate
                    for x, y, rid in zip(xs, ys, ids):
                        texts.append(
                            ax.text(
                                x,
                                y,
                                rid,
                                fontsize=6,
                                color="black",
                                ha="left",
                                va="bottom",
                            )
                        )

                rest = group.filter(pl.col(mse_col) != pl.col("cummin_mse"))
                xs = rest.get_column(l0_col).to_numpy()
                ys = rest.get_column(mse_col).to_numpy()

                if show_rest:
                    # Scatter
                    ax.scatter(
                        xs,
                        ys,
                        color=color,
                        marker=marker,
                        s=12,
                        linewidth=0,
                        alpha=0.5,
                    )

        ax.set_xlabel("L0 sparsity (lower is better)")
        ax.set_ylabel("Reconstruction MSE (lower is better)")
        ax.grid(True, linewidth=0.3, alpha=0.7)
        ax.legend(fontsize="small", ncols=2)
        ax.spines[["right", "top"]].set_visible(False)

        adjust_text(texts)

        return fig

    selected_keys = [k for k, v in pair_dict.value.items() if v]
    if selected_keys:
        models, layers = zip(*[k.rsplit("|", 1) for k in selected_keys])
        pairs_df = pl.DataFrame({
            "model_key": list(models),
            "config/data/layer": list(map(int, layers)),
        })
        filtered_df = df.join(
            pairs_df, on=["model_key", "config/data/layer"], how="inner"
        )
    else:
        filtered_df = pl.DataFrame(schema=df.schema)

    mo.stop(filtered_df.height == 0, mo.md("No runs match the current filters."))

    fig = plot_layerwise(filtered_df, show_rest.value, show_ids.value)
    fig
    return (
        fig,
        filtered_df,
        layers,
        models,
        pairs_df,
        plot_layerwise,
        selected_keys,
    )


@app.cell
def __(alt, df, mo, pl):
    chart = mo.ui.altair_chart(
        alt.Chart(
            df.select(
                "summary/eval/l0",
                "summary/losses/mse",
                "id",
                "config/sae/sparsity_coeff",
                "config/lr",
                "config/sae/d_sae",
                "config/data/layer",
                "model_key",
            )
        )
        .mark_point()
        .encode(
            x=alt.X("summary/eval/l0"),
            y=alt.Y("summary/losses/mse"),
            tooltip=["id", "config/lr"],
            color="config/lr:Q",
            shape="config/data/layer:N",
            # shape="config/objective/sparsity_coeff:N",
            # shape="config/sae/d_sae:N",
            # shape="model_key",
        )
    )
    chart
    return (chart,)


@app.cell
def __(chart, df, mo, np, plot_dist, plt):
    mo.stop(
        len(chart.value) < 2,
        mo.md(
            "Select two or more points. Exactly one point is not supported because of a [Polars bug](https://github.com/pola-rs/polars/issues/19855)."
        ),
    )

    sub_df = (
        df.select(
            "id",
            "summary/eval/freqs",
            "summary/eval/mean_values",
            "summary/eval/l0",
        )
        .join(chart.value.select("id"), on="id", how="inner")
        .sort(by="summary/eval/l0")
        .head(4)
    )

    scatter_fig, scatter_axes = plt.subplots(
        ncols=len(sub_df), figsize=(12, 3), squeeze=False, sharey=True, sharex=True
    )

    hist_fig, hist_axes = plt.subplots(
        ncols=len(sub_df),
        nrows=2,
        figsize=(12, 6),
        squeeze=False,
        sharey=True,
        sharex=True,
    )

    # Always one row
    scatter_axes = scatter_axes.reshape(-1)
    hist_axes = hist_axes.T

    for (id, freqs, values, _), scatter_ax, (freq_hist_ax, values_hist_ax) in zip(
        sub_df.iter_rows(), scatter_axes, hist_axes
    ):
        plot_dist(
            freqs.astype(float),
            (-1.0, 1.0),
            values.astype(float),
            (-2.0, 2.0),
            scatter_ax,
        )
        # ax.scatter(freqs, values, marker=".", alpha=0.03)
        # ax.set_yscale("log")
        # ax.set_xscale("log")
        scatter_ax.set_title(id)

        # Plot feature
        bins = np.linspace(-6, 1, 100)
        freq_hist_ax.hist(np.log10(freqs.astype(float)), bins=bins)
        freq_hist_ax.set_title(f"{id} Feat. Freq. Dist.")

        values_hist_ax.hist(np.log10(values.astype(float)), bins=bins)
        values_hist_ax.set_title(f"{id} Mean Val. Distribution")

    scatter_fig.tight_layout()
    hist_fig.tight_layout()
    return (
        bins,
        freq_hist_ax,
        freqs,
        hist_axes,
        hist_fig,
        id,
        scatter_ax,
        scatter_axes,
        scatter_fig,
        sub_df,
        values,
        values_hist_ax,
    )


@app.cell
def __(scatter_fig):
    scatter_fig
    return


@app.cell
def __(hist_fig):
    hist_fig
    return


@app.cell
def __(chart, df, pl):
    df.join(chart.value.select("id"), on="id", how="inner").sort(
        by="summary/eval/l0"
    ).select("id", pl.selectors.starts_with("config/"))
    return


@app.cell
def __(Float, beartype, jaxtyped, np):
    @jaxtyped(typechecker=beartype.beartype)
    def plot_dist(
        freqs: Float[np.ndarray, " d_sae"],
        freqs_log_range: tuple[float, float],
        values: Float[np.ndarray, " d_sae"],
        values_log_range: tuple[float, float],
        ax,
    ):
        log_sparsity = np.log10(freqs + 1e-9)
        log_values = np.log10(values + 1e-9)

        mask = np.ones(len(log_sparsity)).astype(bool)
        min_log_freq, max_log_freq = freqs_log_range
        mask[log_sparsity < min_log_freq] = False
        mask[log_sparsity > max_log_freq] = False
        min_log_value, max_log_value = values_log_range
        mask[log_values < min_log_value] = False
        mask[log_values > max_log_value] = False

        n_shown = mask.sum()
        ax.scatter(
            log_sparsity[mask],
            log_values[mask],
            marker=".",
            alpha=0.1,
            color="tab:blue",
            label=f"Shown ({n_shown})",
        )
        n_filtered = (~mask).sum()
        ax.scatter(
            log_sparsity[~mask],
            log_values[~mask],
            marker=".",
            alpha=0.1,
            color="tab:red",
            label=f"Filtered ({n_filtered})",
        )

        ax.axvline(min_log_freq, linewidth=0.5, color="tab:red")
        ax.axvline(max_log_freq, linewidth=0.5, color="tab:red")
        ax.axhline(min_log_value, linewidth=0.5, color="tab:red")
        ax.axhline(max_log_value, linewidth=0.5, color="tab:red")

        ax.set_xlabel("Feature Frequency (log10)")
        ax.set_ylabel("Mean Activation Value (log10)")

    return (plot_dist,)


@app.cell
def __(
    beartype,
    get_data_key,
    get_model_key,
    json,
    load_freqs,
    load_mean_values,
    mo,
    os,
    pl,
    tag_input,
    wandb,
):
    class MetadataAccessError(Exception):
        """Exception raised when metadata cannot be accessed or parsed."""

        pass

    @beartype.beartype
    def find_metadata(shard_root: str):
        if not os.path.exists(shard_root):
            msg = f"""
    ERROR: Shard root '{shard_root}' not found. You need to either:

    1. Run this notebook on the same machine where the shards are located.
    2. Copy the shards to this machine at path: {shard_root}
    3. Update the filtering criteria to only show checkpoints with available data""".strip()
            raise MetadataAccessError(msg)

        metadata_path = os.path.join(shard_root, "metadata.json")
        if not os.path.exists(metadata_path):
            raise MetadataAccessError("Missing metadata.json file")

        try:
            with open(metadata_path) as fd:
                return json.load(fd)
        except json.JSONDecodeError:
            raise MetadataAccessError("Malformed metadata.json file")

    @beartype.beartype
    def make_df(tag: str):
        filters = {}
        if tag:
            filters["config.tag"] = tag
        runs = wandb.Api().runs(path="samuelstevens/saev", filters=filters)

        rows = []
        for run in mo.status.progress_bar(
            runs,
            remove_on_exit=True,
            title="Loading",
            subtitle="Parsing runs from WandB",
        ):
            row = {}
            row["id"] = run.id

            row.update(**{
                f"summary/{key}": value for key, value in run.summary.items()
            })
            try:
                row["summary/eval/freqs"] = load_freqs(run)
            except ValueError:
                print(f"Run {run.id} did not log eval/freqs.")
                continue
            except RuntimeError:
                print(f"Wandb blew up on run {run.id}.")
                continue
            try:
                row["summary/eval/mean_values"] = load_mean_values(run)
            except ValueError:
                print(f"Run {run.id} did not log eval/mean_values.")
                continue
            except RuntimeError:
                print(f"Wandb blew up on run {run.id}.")
                continue

            # config
            row.update(**{
                f"config/data/{key}": value
                for key, value in run.config.pop("data").items()
            })
            row.update(**{
                f"config/sae/{key}": value
                for key, value in run.config.pop("sae").items()
            })

            if "objective" in run.config:
                row.update(**{
                    f"config/objective/{key}": value
                    for key, value in run.config.pop("objective").items()
                })

            row.update(**{f"config/{key}": value for key, value in run.config.items()})

            try:
                metadata = find_metadata(row["config/data/shard_root"])
            except MetadataAccessError as err:
                print(f"Bad run {run.id}: {err}")
                continue

            row["model_key"] = get_model_key(metadata)

            data_key = get_data_key(metadata)
            if data_key is None:
                print(f"Bad run {run.id}: unknown data.")
                continue
            row["data_key"] = data_key

            row["config/d_vit"] = metadata["d_vit"]
            rows.append(row)

        if not rows:
            raise ValueError("No runs found.")

        df = pl.DataFrame(rows).with_columns(
            (pl.col("config/sae/d_vit") * pl.col("config/sae/exp_factor")).alias(
                "config/sae/d_sae"
            )
        )
        return df

    df = make_df(tag_input.value)
    return MetadataAccessError, df, find_metadata, make_df


@app.cell
def __(beartype):
    @beartype.beartype
    def get_model_key(metadata: dict[str, object]) -> str:
        family = next(
            metadata[key] for key in ("vit_family", "model_family") if key in metadata
        )

        ckpt = next(
            metadata[key] for key in ("vit_ckpt", "model_ckpt") if key in metadata
        )

        if family == "dinov2" and ckpt == "dinov2_vitb14_reg":
            return "DINOv2 ViT-B/14 (reg)"
        if family == "clip" and ckpt == "ViT-B-16/openai":
            return "CLIP ViT-B/16"
        if family == "clip" and ckpt == "hf-hub:imageomics/bioclip":
            return "BioCLIP ViT-B/16"
        if family == "siglip" and ckpt == "hf-hub:timm/ViT-L-16-SigLIP2-256":
            return "SigLIP2 ViT-L/16"

        print(f"Unknown model: {(family, ckpt)}")
        return ckpt

    @beartype.beartype
    def get_data_key(metadata: dict[str, object]) -> str | None:
        if (
            "train_mini" in metadata["data"]
            and "ImageFolderDataset" in metadata["data"]
        ):
            return "iNat21"

        if "train" in metadata["data"] and "Imagenet" in metadata["data"]:
            return "ImageNet-1K"

        print(f"Unknown data: {metadata['data']}")
        return None

    return get_data_key, get_model_key


@app.cell
def __(Float, json, np, os):
    def load_freqs(run) -> Float[np.ndarray, " d_sae"]:
        try:
            for artifact in run.logged_artifacts():
                if "evalfreqs" not in artifact.name:
                    continue

                dpath = artifact.download()
                fpath = os.path.join(dpath, "eval", "freqs.table.json")
                print(fpath)
                with open(fpath) as fd:
                    raw = json.load(fd)
                return np.array(raw["data"]).reshape(-1)
        except Exception as err:
            raise RuntimeError("Wandb sucks.") from err

        raise ValueError(f"freqs not found in run '{run.id}'")

    def load_mean_values(run) -> Float[np.ndarray, " d_sae"]:
        try:
            for artifact in run.logged_artifacts():
                if "evalmean_values" not in artifact.name:
                    continue

                dpath = artifact.download()
                fpath = os.path.join(dpath, "eval", "mean_values.table.json")
                print(fpath)
                with open(fpath) as fd:
                    raw = json.load(fd)
                return np.array(raw["data"]).reshape(-1)
        except Exception as err:
            raise RuntimeError("Wandb sucks.") from err

        raise ValueError(f"mean_values not found in run '{run.id}'")

    return load_freqs, load_mean_values


@app.cell
def __(df):
    df.drop(
        "config/log_every",
        "config/slurm_acct",
        "config/device",
        "config/n_workers",
        "config/wandb_project",
        "config/track",
        "config/slurm",
        "config/log_to",
        "config/ckpt_path",
        "config/sae/ghost_grads",
    )
    return


if __name__ == "__main__":
    app.run()

>>>> nn/__init__.py
from .modeling import SparseAutoencoder, SparseAutoencoderConfig, dump, load
from .objectives import ObjectiveConfig, get_objective

__all__ = [
    "SparseAutoencoder",
    "SparseAutoencoderConfig",
    "ObjectiveConfig",
    "dump",
    "load",
    "get_objective",
]

>>>> nn/modeling.py
"""
Neural network architectures for sparse autoencoders.
"""

import dataclasses
import io
import json
import logging
import os
import typing

import beartype
import einops
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor

from .. import __version__, helpers


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class Relu:
    d_vit: int = 1024
    exp_factor: int = 16
    """Expansion factor for SAE."""
    n_reinit_samples: int = 1024 * 16 * 32
    """Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean."""
    remove_parallel_grads: bool = True
    """Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic."""
    normalize_w_dec: bool = True
    """Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation."""
    seed: int = 0
    """Random seed."""

    @property
    def d_sae(self) -> int:
        return self.d_vit * self.exp_factor


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class JumpRelu:
    """Implementation of the JumpReLU activation function for SAEs. Not implemented."""

    pass


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class TopK:
    d_vit: int = 1024
    exp_factor: int = 16
    """Expansion factor for SAE."""
    n_reinit_samples: int = 1024 * 16 * 32
    """Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean."""
    remove_parallel_grads: bool = True
    """Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic."""
    normalize_w_dec: bool = True
    """Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation."""
    seed: int = 0
    """Random seed."""
    top_k: int = 32

    @property
    def d_sae(self) -> int:
        return self.d_vit * self.exp_factor


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class BatchTopK:
    d_vit: int = 1024
    exp_factor: int = 16
    """Expansion factor for SAE."""
    n_reinit_samples: int = 1024 * 16 * 32
    """Number of samples to use for SAE re-init. Anthropic proposes initializing b_dec to the geometric median of the dataset here: https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias. We use the regular mean."""
    remove_parallel_grads: bool = True
    """Whether to remove gradients parallel to W_dec columns (which will be ignored because we force the columns to have unit norm). See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization for the original discussion from Anthropic."""
    normalize_w_dec: bool = True
    """Whether to make sure W_dec has unit norm columns. See https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder for original citation."""
    seed: int = 0
    """Random seed."""
    top_k: int = 32

    @property
    def d_sae(self) -> int:
        return self.d_vit * self.exp_factor


SparseAutoencoderConfig = Relu | JumpRelu | TopK | BatchTopK


@jaxtyped(typechecker=beartype.beartype)
class SparseAutoencoder(torch.nn.Module):
    """
    Sparse auto-encoder (SAE) using L1 sparsity penalty.
    """

    def __init__(self, cfg: SparseAutoencoderConfig):
        super().__init__()

        self.cfg = cfg
        self.logger = logging.getLogger(f"sae(seed={cfg.seed})")

        self.W_enc = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_vit, cfg.d_sae))
        )
        self.b_enc = torch.nn.Parameter(torch.zeros(cfg.d_sae))

        self.W_dec = torch.nn.Parameter(
            torch.nn.init.kaiming_uniform_(torch.empty(cfg.d_sae, cfg.d_vit))
        )
        self.b_dec = torch.nn.Parameter(torch.zeros(cfg.d_vit))

        self.normalize_w_dec()

        self.activation = get_activation(cfg)

    def forward(
        self, x: Float[Tensor, "batch d_model"]
    ) -> tuple[Float[Tensor, "batch d_model"], Float[Tensor, "batch d_sae"]]:
        """
        Given x, calculates the reconstructed x_hat and the intermediate activations f_x.

        Arguments:
            x: a batch of ViT activations.
        """

        h_pre = (
            einops.einsum(x, self.W_enc, "... d_vit, d_vit d_sae -> ... d_sae")
            + self.b_enc
        )
        f_x = self.activation(h_pre)
        x_hat = self.decode(f_x)

        return x_hat, f_x

    def decode(
        self, f_x: Float[Tensor, "batch d_sae"]
    ) -> Float[Tensor, "batch d_model"]:
        x_hat = (
            einops.einsum(f_x, self.W_dec, "... d_sae, d_sae d_vit -> ... d_vit")
            + self.b_dec
        )
        return x_hat

    @torch.no_grad()
    def normalize_w_dec(self):
        """
        Set W_dec to unit-norm columns.
        """
        if self.cfg.normalize_w_dec:
            self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)

    @torch.no_grad()
    def remove_parallel_grads(self):
        """
        Update grads so that they remove the parallel component
            (d_sae, d_vit) shape
        """
        if not self.cfg.remove_parallel_grads:
            return

        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            "d_sae d_vit, d_sae d_vit -> d_sae",
        )

        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            "d_sae, d_sae d_vit -> d_sae d_vit",
        )


@jaxtyped(typechecker=beartype.beartype)
class MatryoshkaSparseAutoencoder(SparseAutoencoder):
    """
    Subclass of SparseAutoencoder for use with the Matryoshka objective function.
    Needed since the matryoshka objective requires access to the weights of the decoder in order to calculate the
    reconstructions from prefixes of the sparse encoding.

    Still uses L1 for sparsity penalty, though when using BatchTopK as activation (recommended), this is not relevant.
    """

    def __init__(self, cfg: SparseAutoencoderConfig):
        super().__init__(cfg)

    def matryoshka_forward(
        self, x: Float[Tensor, "batch d_model"], n_prefixes: int
    ) -> tuple[Float[Tensor, "batch d_model"], Float[Tensor, "batch d_sae"]]:
        """
        Given x, calculates the reconstructed x_hat from the prefixes of encoded intermediate activations f_x.

        Arguments:
            x: a batch of ViT activations.
        """

        # Remove encoder bias as per Anthropic
        h_pre = (
            einops.einsum(
                x - self.b_dec, self.W_enc, "... d_vit, d_vit d_sae -> ... d_sae"
            )
            + self.b_enc
        )
        f_x = self.activation(h_pre)

        prefixes = self.sample_prefixes(len(f_x), n_prefixes).to(self.b_dec.device)

        block_indices = torch.torch.cat((
            torch.tensor([0]).to(self.b_dec.device),
            prefixes,
        ))
        block_bounds = list(zip(block_indices[:-1], block_indices[1:]))

        block_preds = [self.block_decode(f_x, block) for block in block_bounds]

        prefix_preds = torch.cumsum(torch.stack(block_preds), dim=0)

        return prefix_preds, f_x

    def block_decode(
        self, f_x: Float[Tensor, "batch d_sae"], block: tuple[int]
    ) -> Float[Tensor, "batch d_model"]:
        """Decodes sparse encoding using only the given interval of indices.

        Arguments:
            f_x: Sparse encoding"""

        # Can't use einsum here because the block lengths can change
        x_hat = (
            torch.matmul(f_x[:, block[0] : block[1]], self.W_dec[block[0] : block[1]])
            + self.b_dec
        )

        # x_hat = (
        #    einops.einsum(f_x[block[0]:block[1]], self.W_dec[block[0]:block[1]], "... block, block d_vit -> ... d_vit")
        #    + self.b_dec[block[0]:block[1]]
        # )

        return x_hat

    @torch.no_grad()
    def sample_prefixes(
        self,
        sae_dim: int,
        n_prefixes: int,
        min_prefix_length: int = 1,
        pareto_power: float = 0.5,
        replacement: bool = False,
    ) -> torch.Tensor:
        """
        Samples prefix lengths using a Pareto distribution. Derived from "Learning Multi-Level Features with
        Matryoshka Sparse Autoencoders" (https://doi.org/10.48550/arXiv.2503.17547)

        Args:
            sae_dim: Total number of latent dimensions
            n_prefixes: Number of prefixes to sample
            min_prefix_length: Minimum length of any prefix
            pareto_power: Power parameter for Pareto distribution (lower = more uniform)

        Returns:
            torch.Tensor: Sorted prefix lengths
        """
        if n_prefixes <= 1:
            return torch.tensor([sae_dim])

        # Calculate probability distribution favoring shorter prefixes
        lengths = torch.arange(1, sae_dim)
        pareto_cdf = 1 - ((min_prefix_length / lengths.float()) ** pareto_power)
        pareto_pdf = torch.cat([pareto_cdf[:1], pareto_cdf[1:] - pareto_cdf[:-1]])
        probability_dist = pareto_pdf / pareto_pdf.sum()

        # Sample and sort prefix lengths
        prefixes = torch.multinomial(
            probability_dist, num_samples=n_prefixes - 1, replacement=replacement
        )

        # Add n_latents as the final prefix
        prefixes = torch.cat((prefixes.detach().clone(), torch.tensor([sae_dim])))

        prefixes, _ = torch.sort(prefixes, descending=False)

        return prefixes


@jaxtyped(typechecker=beartype.beartype)
class TopKActivation(torch.nn.Module):
    """
    Top-K activation function. For use as activation function of sparse encoder.
    """

    def __init__(self, cfg: TopK = TopK()):
        super().__init__()
        self.cfg = cfg
        self.k = cfg.top_k

    def forward(self, x: Float[Tensor, "batch d_sae"]) -> Float[Tensor, "batch d_sae"]:
        """
        Apply top-k activation to the input tensor.
        """
        if self.k <= 0:
            raise ValueError("k must be a positive integer.")

        k_vals, k_inds = torch.topk(x, self.k, dim=-1, sorted=False)
        mask = torch.zeros_like(x).scatter_(
            dim=-1, index=k_inds, src=torch.ones_like(x)
        )

        return torch.mul(mask, x)


@jaxtyped(typechecker=beartype.beartype)
class BatchTopKActivation(torch.nn.Module):
    """
    Batch Top-K activation function. For use as activation function of sparse encoder.
    """

    def __init__(self, cfg: BatchTopK = BatchTopK()):
        super().__init__()
        self.cfg = cfg
        self.k = cfg.top_k

    def forward(self, x: Float[Tensor, "batch d_sae"]) -> Float[Tensor, "batch d_sae"]:
        """
        Apply top-k activation to the input tensor.
        """
        if self.k <= 0:
            raise ValueError("k must be a positive integer.")

        orig_shape = x.shape
        x = x.flatten()

        # Handle case where k exceeds number of elements
        k = min(self.k, x.numel())

        k_vals, k_inds = torch.topk(x, k, dim=-1, sorted=False)
        mask = torch.zeros_like(x).scatter_(
            dim=-1, index=k_inds, src=torch.ones_like(x)
        )

        return torch.mul(mask, x).reshape(orig_shape)


@beartype.beartype
def get_activation(cfg: SparseAutoencoderConfig) -> torch.nn.Module:
    if isinstance(cfg, Relu):
        return torch.nn.ReLU()
    elif isinstance(cfg, JumpRelu):
        raise NotImplementedError()
    elif isinstance(cfg, TopK):
        return TopKActivation(cfg)
    elif isinstance(cfg, BatchTopK):
        return BatchTopKActivation(cfg)
    else:
        typing.assert_never(cfg)


@beartype.beartype
def dump(fpath: str, sae: SparseAutoencoder):
    """
    Save an SAE checkpoint to disk along with configuration, using the [trick from equinox](https://docs.kidger.site/equinox/examples/serialisation).

    Arguments:
        fpath: filepath to save checkpoint to.
        sae: sparse autoencoder checkpoint to save.
    """
    header = {
        "schema": 1,
        "cfg": dataclasses.asdict(sae.cfg),
        "cls": sae.cfg.__class__.__name__,
        "commit": helpers.current_git_commit() or "unknown",
        "lib": __version__,
    }

    os.makedirs(os.path.dirname(fpath), exist_ok=True)
    with open(fpath, "wb") as fd:
        header_str = json.dumps(header)
        fd.write((header_str + "\n").encode("utf-8"))
        torch.save(sae.state_dict(), fd)


@beartype.beartype
def load(fpath: str, *, device="cpu") -> SparseAutoencoder:
    """
    Loads a sparse autoencoder from disk.
    """
    with open(fpath, "rb") as fd:
        header = json.loads(fd.readline())
        buffer = io.BytesIO(fd.read())

    if "schema" not in header:
        # Original, pre-schema stuff.
        for keyword in ("sparsity_coeff", "ghost_grads"):
            header.pop(keyword)
        cfg = Relu(**header)
    elif header["schema"] == 1:
        cls = globals()[header["cls"]]  # default for v0
        cfg = cls(**header["cfg"])
    else:
        raise ValueError(f"Unknown schema version: {header['schema']}")

    model = SparseAutoencoder(cfg)
    model.load_state_dict(torch.load(buffer, weights_only=True, map_location=device))
    return model

>>>> nn/objectives.py
import dataclasses
import typing

import beartype
import torch
from jaxtyping import Float, jaxtyped
from torch import Tensor


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class Vanilla:
    sparsity_coeff: float = 4e-4
    """How much to weight sparsity loss term."""


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class Matryoshka:
    """
    Config for the Matryoshka loss for another arbitrary SAE class.

    Reference code is here: https://github.com/noanabeshima/matryoshka-saes and the original reading is https://sparselatents.com/matryoshka.html and https://arxiv.org/pdf/2503.17547
    """

    sparsity_coeff: float = 4e-4
    """How much to weight sparsity loss term (if not using TopK/BatchTopK)."""
    n_prefixes: int = 10
    """Number of random length prefixes to use for loss calculation."""


ObjectiveConfig = Vanilla | Matryoshka


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True, slots=True)
class Loss:
    """The loss term for an autoencoder training batch."""

    @property
    def loss(self) -> Float[Tensor, ""]:
        """Total loss."""
        raise NotImplementedError()

    def metrics(self) -> dict[str, object]:
        raise NotImplementedError()


@jaxtyped(typechecker=beartype.beartype)
class Objective(torch.nn.Module):
    def forward(
        self,
        x: Float[Tensor, "batch d_model"],
        f_x: Float[Tensor, "batch d_sae"],
        x_hat: Float[Tensor, "batch d_model"],
    ) -> Loss:
        raise NotImplementedError()


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True, slots=True)
class VanillaLoss(Loss):
    """The vanilla loss terms for an training batch."""

    mse: Float[Tensor, ""]
    """Reconstruction loss (mean squared error)."""
    sparsity: Float[Tensor, ""]
    """Sparsity loss, typically lambda * L1."""
    l0: Float[Tensor, ""]
    """L0 magnitude of hidden activations."""
    l1: Float[Tensor, ""]
    """L1 magnitude of hidden activations."""

    @property
    def loss(self) -> Float[Tensor, ""]:
        """Total loss."""
        return self.mse + self.sparsity

    def metrics(self) -> dict[str, object]:
        return {
            "loss": self.loss.item(),
            "mse": self.mse.item(),
            "l0": self.l0.item(),
            "l1": self.l1.item(),
            "sparsity": self.sparsity.item(),
        }


@jaxtyped(typechecker=beartype.beartype)
class VanillaObjective(Objective):
    def __init__(self, cfg: Vanilla):
        super().__init__()
        self.cfg = cfg

    def forward(
        self,
        x: Float[Tensor, "batch d_model"],
        f_x: Float[Tensor, "batch d_sae"],
        x_hat: Float[Tensor, "batch d_model"],
    ) -> VanillaLoss:
        # Some values of x and x_hat can be very large. We can calculate a safe MSE
        mse_loss = mean_squared_err(x_hat, x)

        mse_loss = mse_loss.mean()
        l0 = (f_x > 0).float().sum(axis=1).mean(axis=0)
        l1 = f_x.sum(axis=1).mean(axis=0)
        sparsity_loss = self.cfg.sparsity_coeff * l1

        return VanillaLoss(mse_loss, sparsity_loss, l0, l1)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True, slots=True)
class MatryoshkaLoss(Loss):
    """The composite loss terms for an training batch."""

    mse: Float[Tensor, ""]
    """Average of reconstruction loss (mean squared error) for all prefix lengths."""
    sparsity: Float[Tensor, ""]
    """Sparsity loss, typically lambda * L1."""
    l0: Float[Tensor, ""]
    """Sum of L0 magnitudes of hidden activations for all prefix lengths."""
    l1: Float[Tensor, ""]
    """Sum of L1 magnitudes of hidden activations for all prefix lengths."""

    @property
    def loss(self) -> Float[Tensor, ""]:
        """Total loss."""
        return self.mse + self.sparsity

    def metrics(self) -> dict[str, object]:
        return {
            "loss": self.loss.item(),
            "mse": self.mse.item(),
            "l0": self.l0.item(),
            "l1": self.l1.item(),
            "sparsity": self.sparsity.item(),
        }


@jaxtyped(typechecker=beartype.beartype)
class MatryoshkaObjective(Objective):
    """Torch module for calculating the matryoshka loss for an SAE."""

    def __init__(self, cfg: Matryoshka):
        super().__init__()
        self.cfg = cfg

    def forward(
        self,
        x: Float[Tensor, "batch d_model"],
        f_x: Float[Tensor, "batch n_prefixes d_sae"],
        prefix_preds: Float[Tensor, "batch n_prefixes d_model"],
    ) -> "MatryoshkaLoss.Loss":
        # Some values of x and x_hat can be very large. We can calculate a safe MSE
        # mse_losses = torch.flatten([mean_squared_err(p, x) for p in prefix_preds])
        mse_losses = torch.stack([mean_squared_err(p, x) for p in prefix_preds], dim=0)
        mse_loss = mse_losses.sum(dim=-1).mean()
        l0 = (f_x > 0).float().sum(axis=1).mean(axis=0)
        l1 = f_x.sum(axis=1).mean(axis=0)
        sparsity_loss = self.cfg.sparsity_coeff * l1

        return MatryoshkaLoss(mse_loss, sparsity_loss, l0, l1)


@beartype.beartype
def get_objective(cfg: ObjectiveConfig) -> Objective:
    if isinstance(cfg, Vanilla):
        return VanillaObjective(cfg)
    elif isinstance(cfg, Matryoshka):
        return MatryoshkaObjective(cfg)
    else:
        typing.assert_never(cfg)


@jaxtyped(typechecker=beartype.beartype)
def ref_mean_squared_err(
    x_hat: Float[Tensor, "*d"], x: Float[Tensor, "*d"], norm: bool = False
) -> Float[Tensor, "*d"]:
    mse_loss = torch.pow((x_hat - x.float()), 2)

    if norm:
        mse_loss /= (x**2).sum(dim=-1, keepdim=True).sqrt()
    return mse_loss


@jaxtyped(typechecker=beartype.beartype)
def mean_squared_err(
    x_hat: Float[Tensor, "*batch d"], x: Float[Tensor, "*batch d"], norm: bool = False
) -> Float[Tensor, "*batch d"]:
    upper = x.abs().max()
    x = x / upper
    x_hat = x_hat / upper

    mse = (x_hat - x) ** 2
    # (sam): I am now realizing that we normalize by the L2 norm of x.
    if norm:
        mse /= torch.linalg.norm(x, axis=-1, keepdim=True) + 1e-12
        return mse * upper

    return mse * upper * upper

>>>> ops.py
import beartype
import torch
from jaxtyping import Float, Int, jaxtyped
from torch import Tensor


@jaxtyped(typechecker=beartype.beartype)
def gather_batched(
    value: Float[Tensor, "batch n dim"], i: Int[Tensor, "batch k"]
) -> Float[Tensor, "batch k dim"]:
    batch_size, n, dim = value.shape
    _, k = i.shape

    batch_i = torch.arange(batch_size, device=value.device)[:, None].expand(-1, k)
    return value[batch_i, i]

>>>> preprint.md
# Notes for Preprint

I'm writing a submission to ICML. The premise is that we apply sparse autoencoders to vision models like DINOv2 and CLIP to interpret and control their internal representations. 

## Outline

We're trying to (informally) explain our position with the following metaphor:

Scientific method: observation -> hypothesis -> experiment
Interpretability methods: model behavior -> proposed explanation → ?
SAEs complete the cycle: model behavior -> proposed explanation -> feature intervention

1. Introduction
    1.1. Understanding requires intervention - we must test hypotheses through controlled experiments (scientific method)
    1.2. Current methods provide only understanding or only control, never both
    1.3 Understanding and controlling vision models requires three key capabilities: (1) the ability to identify human-interpretable features (like 'fur' or 'wheels'), (2) reliable ways to manipulate these features, and (3) compatibility with existing models. 
    1.4 Current methods fail to meet these requirements; they either discover features that can't be manipulated, enable manipulations that aren't interpretable, or require expensive model retraining.
    1.5. SAEs from NLP provide unified solution: interpretable features that can be precisely controlled to validate hypotheses.
    1.6. Contributions: SAE for vision model, new understanding of differences in vision models, multiple control examples across tasks

2. Background & Related Work
    2.1. Vision model interpretability
    2.2. Model editing
    2.4. SAEs in language models

3. Method
    3.1. SAE architecture and training
    3.2. Feature intervention framework
        3.2.1. Train a (or use an existing pre-trained) task-specific head: make predictions based on [CLS] activations, use an LLM to generate captions based on an image and a prompt, etc.
        3.2.2. Manipulate the vision transformer activations using the SAE-proposed features and compare outputs before/after intervention.
    3.4. Evaluation metrics

4. Understanding Results
    4.1. Pre-Training Modality Affects Learned Features - DINOv2 vs CLIP
    4.2. Pre-Training Distritbuion Affects Learned Features - CLIP vs BioCLIP

5. Control Results - Task & Model Agnostic
    5.1. Semantic Segmentation Control
        * Intro explaining?
        * Technical description of training linear semseg head on DINOv2 features.
        * Qualitative results (cherry picked examples, full-width figure)
        * [MAYBE] Description of how we automatically find ADE20K class features in SAE latent space
        * [MAYBE] Quantitative results (single-column table)
    5.2. Image Classification Control
        * Birds with interpretable traits
    5.3. Vision-Language Control
        * Counting + removing objects
        * Colors + changing colors
        * Captioning (classification

6. Discussion
    6.1. Limitations
    6.2. Future work


## List of Figures

1. Hook figure: Full width explanatory figure that shows an overview of how we can use SAEs to interpret vision models and then intervene on that explanation and see how model predictions change. Status: visual outline
2. CLIP vs DINOv2: Full width figure demonstrating that CLIP learns semntically abstract visual features like "human teeth" across different visual styles, while DINOv2 does not. Status: visual outline
3. CLIP vs BioCLIP: Full width figure demonstrating some difference in CLIP and BioCLIP's learned features. Status: untouched.
4. Semantic segmentation: Full width figure demonstrating that we can validate patch-level hypotheses. Status: drafted
5. Image-classification: Full width figure demonstrating how you can manipulate fine-grained classification with SAEs. Status: untouched
6. Image captioning: Full width figure. Status: untouched

I also want to build some interactive dashboards and tools to demonstrate that this stuff works.

1. I want my current PCA dashboard with UMAP instead
2. Given a linear classifier of semantic segmentation features, I want to manipulate the features in a given patch, and apply the suppression to all patches to see the live changes on the segmentation mask.
3. After training an SAE on a CLS token, I can then train a linear classifier on the CLS token with ImageNet-1K, and manipulate the features directly.
4. Given a small vision-language model like Phi-3.5 or Moondream, I want to manipulate the vision embeddings (suppressing or adding one or more features) and then see how the top 5 responses change in response to the user input (non-zero temperature).
5. Given a zero-shot CLIP or SigLIP classifier, you can add subtract features from all patches, then see how the classification changes

---

With respect to writing, we want to frame everything as Goal->Problem->Solution. In general, I want you to be skeptical and challenging of arguments that are not supported by evidence.

Some questions that come up that are not in the outline yet:

Q: Am you using standard SAEs or have you adopted the architecture?

A: I am using ReLU SAEs with an L1 sparsity term and I have constrained the columns of W_dec to be unit norm to prevent shrinkage. We are not using sigmoid or tanh activations because of prior work from Anthropic exploring the use of these activation functions, finding them to produce worse features than ReLU.

Q: What datasets are you using?

A: I am using ImageNet-1K for training and testing. I am extending it to iNat2021 (train-mini, 500K images) to demonstrate that results hold beyond ImageNet. 

We're going to work together on writing this paper, so I want to give you an opportunity to ask any questions you might have.

It can be helpful to think about this project from the perspective of a top machine learning researcher, like Lucas Beyer, Yann LeCun, or Francois Chollet. What would they think about this project? What criticisms would they have? What parts would be novel or exciting to them?




>>>> py.typed

>>>> scripts/visuals.py
"""
There is some important notation used only in this file to dramatically shorten variable names.

Variables suffixed with `_im` refer to entire images, and variables suffixed with `_p` refer to patches.
"""

import collections.abc
import dataclasses
import json
import logging
import math
import os
import random
import typing

import beartype
import einops
import torch
from jaxtyping import Float, Int, jaxtyped
from PIL import Image
from torch import Tensor

import saev.data
import saev.data.images
from saev import helpers, imaging, nn

log_format = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
logging.basicConfig(level=logging.INFO, format=log_format)
logger = logging.getLogger("visuals")


@beartype.beartype
@dataclasses.dataclass(frozen=True, slots=True)
class Config:
    """Configuration for generating visuals from trained SAEs."""

    # Disk
    ckpt: str = os.path.join(".", "checkpoints", "sae.pt")
    """Path to the sae.pt file."""
    data: saev.data.OrderedConfig = dataclasses.field(
        default_factory=saev.data.OrderedConfig
    )
    """Data configuration"""
    images: saev.data.images.Config = dataclasses.field(
        default_factory=saev.data.images.Imagenet
    )
    """Which images to use."""
    dump_to: str = os.path.join(".", "data")
    """Where to save data."""

    # Algorithm
    top_k: int = 128
    """How many images per SAE feature to store."""
    epsilon: float = 1e-9
    """Value to add to avoid log(0)."""
    sort_by: typing.Literal["cls", "img", "patch"] = "patch"
    """How to find the top k images. 'cls' picks images where the SAE latents of the ViT's [CLS] token are maximized without any patch highligting. 'img' picks images that maximize the sum of an SAE latent over all patches in the image, highlighting the patches. 'patch' pickes images that maximize an SAE latent over all patches (not summed), highlighting the patches and only showing unique images."""
    log_freq_range: tuple[float, float] = (-6.0, -2.0)
    """Log10 frequency range for which to save images."""
    log_value_range: tuple[float, float] = (-1.0, 1.0)
    """Log10 frequency range for which to save images."""
    include_latents: list[int] = dataclasses.field(default_factory=list)
    """Latents to always include, no matter what."""
    n_distributions: int = 25
    """Number of features to save distributions for."""
    percentile: int = 99
    """Percentile to estimate for outlier detection."""
    n_latents: int = 400
    """Maximum number of latents to save images for."""
    sae_batch_size: int = 1024 * 16
    """Batch size for SAE inference."""
    topk_batch_size: int = 1024 * 16

    # Hardware
    device: str = "cuda"
    """Which accelerator to use."""
    seed: int = 42
    """Random seed."""
    slurm_acct: str = ""
    """Slurm account string. Empty means to not use Slurm."""
    slurm_partition: str = ""
    """Slurm partition."""
    n_hours: float = 4.0
    """Slurm job length in hours."""
    log_to: str = os.path.join(".", "logs")
    """Where to log Slurm job stdout/stderr."""

    @property
    def root(self) -> str:
        return os.path.join(self.dump_to, f"sort_by_{self.sort_by}")

    @property
    def top_values_fpath(self) -> str:
        return os.path.join(self.root, "top_values.pt")

    @property
    def top_img_i_fpath(self) -> str:
        return os.path.join(self.root, "top_img_i.pt")

    @property
    def top_patch_i_fpath(self) -> str:
        return os.path.join(self.root, "top_patch_i.pt")

    @property
    def mean_values_fpath(self) -> str:
        return os.path.join(self.root, "mean_values.pt")

    @property
    def sparsity_fpath(self) -> str:
        return os.path.join(self.root, "sparsity.pt")

    @property
    def distributions_fpath(self) -> str:
        return os.path.join(self.root, "distributions.pt")

    @property
    def percentiles_fpath(self) -> str:
        return os.path.join(self.root, f"percentiles_p{self.percentile}.pt")


@beartype.beartype
def safe_load(path: str) -> object:
    return torch.load(path, map_location="cpu", weights_only=True)


@jaxtyped(typechecker=beartype.beartype)
def gather_batched(
    value: Float[Tensor, "batch n dim"], i: Int[Tensor, "batch k"]
) -> Float[Tensor, "batch k dim"]:
    batch_size, n, dim = value.shape  # noqa: F841
    _, k = i.shape

    batch_i = torch.arange(batch_size, device=value.device)[:, None].expand(-1, k)
    return value[batch_i, i]


# def test_gather_batched_small():
#     values = torch.arange(0, 64, dtype=torch.float).view(4, 2, 8)
#     i = torch.tensor([[0], [0], [1], [1]])
#     actual = gather_batched(values, i)
#     expected = torch.tensor([
#         [[0, 1, 2, 3, 4, 5, 6, 7]],
#         [[16, 17, 18, 19, 20, 21, 22, 23]],
#         [[40, 41, 42, 43, 44, 45, 46, 47]],
#         [[56, 57, 58, 59, 60, 61, 62, 63]],
#     ]).float()
#     torch.testing.assert_close(actual, expected)


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class GridElement:
    img: Image.Image
    label: str
    patches: Float[Tensor, " n_patches"]


@beartype.beartype
def make_img(elem: GridElement, *, upper: float | None = None) -> Image.Image:
    # Resize to 256x256 and crop to 224x224
    resize_size_px = (512, 512)
    resize_w_px, resize_h_px = resize_size_px
    crop_size_px = (448, 448)
    crop_w_px, crop_h_px = crop_size_px
    crop_coords_px = (
        (resize_w_px - crop_w_px) // 2,
        (resize_h_px - crop_h_px) // 2,
        (resize_w_px + crop_w_px) // 2,
        (resize_h_px + crop_h_px) // 2,
    )

    img = elem.img.resize(resize_size_px).crop(crop_coords_px)
    img = imaging.add_highlights(img, elem.patches.numpy(), upper=upper)
    return img


@jaxtyped(typechecker=beartype.beartype)
def get_new_topk(
    val1: Float[Tensor, "d_sae k"],
    i1: Int[Tensor, "d_sae k"],
    val2: Float[Tensor, "d_sae k"],
    i2: Int[Tensor, "d_sae k"],
    k: int,
) -> tuple[Float[Tensor, "d_sae k"], Int[Tensor, "d_sae k"]]:
    """
    Picks out the new top k values among val1 and val2. Also keeps track of i1 and i2, then indices of the values in the original dataset.

    Args:
        val1: top k original SAE values.
        i1: the patch indices of those original top k values.
        val2: top k incoming SAE values.
        i2: the patch indices of those incoming top k values.
        k: k.

    Returns:
        The new top k values and their patch indices.
    """
    all_val = torch.cat([val1, val2], dim=1)
    new_values, top_i = torch.topk(all_val, k=k, dim=1)

    all_i = torch.cat([i1, i2], dim=1)
    new_indices = torch.gather(all_i, 1, top_i)
    return new_values, new_indices


@beartype.beartype
def batched_idx(
    total_size: int, batch_size: int
) -> collections.abc.Iterator[tuple[int, int]]:
    """
    Iterate over (start, end) indices for total_size examples, where end - start is at most batch_size.

    Args:
        total_size: total number of examples
        batch_size: maximum distance between the generated indices.

    Returns:
        A generator of (int, int) tuples that can slice up a list or a tensor.
    """
    for start in range(0, total_size, batch_size):
        stop = min(start + batch_size, total_size)
        yield start, stop


@jaxtyped(typechecker=beartype.beartype)
def get_sae_acts(
    vit_acts: Float[Tensor, "n d_vit"], sae: nn.SparseAutoencoder, cfg: Config
) -> Float[Tensor, "n d_sae"]:
    """
    Get SAE hidden layer activations for a batch of ViT activations.

    Args:
        vit_acts: Batch of ViT activations
        sae: Sparse autoencder.
        cfg: Experimental config.
    """
    sae_acts = []
    for start, end in batched_idx(len(vit_acts), cfg.sae_batch_size):
        _, f_x, *_ = sae(vit_acts[start:end].to(cfg.device))
        sae_acts.append(f_x)

    sae_acts = torch.cat(sae_acts, dim=0)
    sae_acts = sae_acts.to(cfg.device)
    return sae_acts


@jaxtyped(typechecker=beartype.beartype)
@dataclasses.dataclass(frozen=True)
class TopKPatch:
    ".. todo:: Document this class."

    top_values: Float[Tensor, "d_sae k n_patches_per_img"]
    top_i: Int[Tensor, "d_sae k"]
    mean_values: Float[Tensor, " d_sae"]
    sparsity: Float[Tensor, " d_sae"]
    distributions: Float[Tensor, "m n"]
    percentiles: Float[Tensor, " d_sae"]


@beartype.beartype
@torch.inference_mode()
def get_topk_patch(cfg: Config) -> TopKPatch:
    """
    Gets the top k images for each latent in the SAE.
    The top k images are for latent i are sorted by

        max over all patches: f_x(patch)[i]

    Thus, we could end up with duplicate images in the top k, if an image has more than one patch that maximally activates an SAE latent.

    Args:
        cfg: Config.

    Returns:
        A tuple of TopKPatch and m randomly sampled activation distributions.
    """
    assert cfg.sort_by == "patch"
    assert cfg.data.patches == "image"

    sae = nn.load(cfg.ckpt).to(cfg.device)
    metadata = saev.data.Metadata.load(cfg.data.shard_root)

    top_values_p = torch.full(
        (sae.cfg.d_sae, cfg.top_k, metadata.n_patches_per_img), -1.0, device=cfg.device
    )
    top_i_im = torch.zeros(
        (sae.cfg.d_sae, cfg.top_k), dtype=torch.int, device=cfg.device
    )

    sparsity_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)
    mean_values_S = torch.zeros((sae.cfg.d_sae,), device=cfg.device)

    batch_size = (
        cfg.topk_batch_size // metadata.n_patches_per_img * metadata.n_patches_per_img
    )
    n_imgs_per_batch = batch_size // metadata.n_patches_per_img
    dataloader = saev.data.OrderedDataLoader(
        dataclasses.replace(cfg.data, batch_size=batch_size),
    )

    distributions_MN = torch.zeros(
        (cfg.n_distributions, dataloader.n_samples), device="cpu"
    )
    estimator = PercentileEstimator(
        cfg.percentile, dataloader.n_samples, shape=(sae.cfg.d_sae,)
    )

    logger.info("Loaded SAE and data.")

    for batch in helpers.progress(dataloader, desc="picking top-k", every=50):
        vit_acts_BD = batch["act"]
        sae_acts_BS = get_sae_acts(vit_acts_BD, sae, cfg)

        for sae_act_S in sae_acts_BS:
            estimator.update(sae_act_S)

        sae_acts_SB = einops.rearrange(sae_acts_BS, "batch d_sae -> d_sae batch")
        distributions_MN[:, batch["image_i"]] = sae_acts_SB[: cfg.n_distributions].to(
            "cpu"
        )

        mean_values_S += einops.reduce(sae_acts_SB, "d_sae batch -> d_sae", "sum")
        sparsity_S += einops.reduce((sae_acts_SB > 0), "d_sae batch -> d_sae", "sum")

        i_im = torch.sort(torch.unique(batch["image_i"])).values
        values_p = sae_acts_SB.view(
            sae.cfg.d_sae, len(i_im), metadata.n_patches_per_img
        )

        # Checks that I did my reshaping correctly.
        assert values_p.shape[1] == i_im.shape[0]
        if not len(i_im) == n_imgs_per_batch:
            logger.warning(
                "Got %d images; expected %d images per batch.",
                len(i_im),
                n_imgs_per_batch,
            )

        _, k = torch.topk(sae_acts_SB, k=cfg.top_k, dim=1)
        k_im = k // metadata.n_patches_per_img

        values_p = gather_batched(values_p, k_im)
        i_im = i_im.to(cfg.device)[k_im]

        all_values_p = torch.cat((top_values_p, values_p), axis=1)
        _, k = torch.topk(all_values_p.max(axis=-1).values, k=cfg.top_k, axis=1)

        top_values_p = gather_batched(all_values_p, k)
        top_i_im = torch.gather(torch.cat((top_i_im, i_im), axis=1), 1, k)

    mean_values_S /= sparsity_S
    sparsity_S /= dataloader.n_samples

    return TopKPatch(
        top_values_p,
        top_i_im,
        mean_values_S,
        sparsity_S,
        distributions_MN,
        estimator.estimate.cpu(),
    )


@beartype.beartype
@torch.inference_mode()
def dump_activations(cfg: Config):
    """Dump ViT activation statistics for later use.

    The dataset described by ``cfg`` is processed to find the images or patches that maximally activate each SAE latent.  Various tensors summarising these activations are then written to ``cfg.root`` so they can be loaded by other tools.

    Args:
        cfg: options controlling which activations are processed and where the resulting files are saved.

    Returns:
        None. All data is saved to disk.
    """
    if cfg.sort_by == "img":
        raise helpers.RemovedFeatureError("Feature removed.")
    elif cfg.sort_by == "patch":
        topk = get_topk_patch(cfg)
    else:
        typing.assert_never(cfg.sort_by)

    os.makedirs(cfg.root, exist_ok=True)

    torch.save(topk.top_values, cfg.top_values_fpath)
    torch.save(topk.top_i, cfg.top_img_i_fpath)
    torch.save(topk.mean_values, cfg.mean_values_fpath)
    torch.save(topk.sparsity, cfg.sparsity_fpath)
    torch.save(topk.distributions, cfg.distributions_fpath)
    torch.save(topk.percentiles, cfg.percentiles_fpath)


@jaxtyped(typechecker=beartype.beartype)
def plot_activation_distributions(cfg: Config, distributions: Float[Tensor, "m n"]):
    import matplotlib.pyplot as plt
    import numpy as np

    m, _ = distributions.shape

    n_rows = int(math.sqrt(m))
    n_cols = n_rows
    fig, axes = plt.subplots(
        figsize=(4 * n_cols, 4 * n_rows),
        nrows=n_rows,
        ncols=n_cols,
        sharex=True,
        sharey=True,
    )

    _, bins = np.histogram(np.log10(distributions[distributions > 0].numpy()), bins=100)

    percentiles = [90, 95, 99, 100]
    colors = ("red", "darkorange", "gold", "lime")

    for dist, ax in zip(distributions, axes.reshape(-1)):
        vals = np.log10(dist[dist > 0].numpy())

        ax.hist(vals, bins=bins)

        if vals.size == 0:
            continue

        for i, (percentile, color) in enumerate(
            zip(np.percentile(vals, percentiles), colors)
        ):
            ax.axvline(percentile, color=color, label=f"{percentiles[i]}th %-ile")

        for i, (percentile, color) in enumerate(zip(percentiles, colors)):
            estimator = PercentileEstimator(percentile, len(vals))
            for v in vals:
                estimator.update(v)
            ax.axvline(
                estimator.estimate,
                color=color,
                linestyle="--",
                label=f"Est. {percentiles[i]}th %-ile",
            )

    ax.legend()

    fig.tight_layout()
    return fig


@beartype.beartype
@torch.inference_mode()
def dump_imgs(cfg: Config):
    """
    .. todo:: document this function.

    Dump top-k images to a directory.

    Args:
        cfg: Configuration object.
    """

    try:
        top_values = safe_load(cfg.top_values_fpath)
        sparsity = safe_load(cfg.sparsity_fpath)
        mean_values = safe_load(cfg.mean_values_fpath)
        top_i = safe_load(cfg.top_img_i_fpath)
        distributions = safe_load(cfg.distributions_fpath)
        _ = safe_load(cfg.percentiles_fpath)
    except FileNotFoundError as err:
        logger.warning("Need to dump files: %s", err)
        dump_activations(cfg)
        return dump_imgs(cfg)

    d_sae, cached_topk, *rest = top_values.shape
    # Check that the data is at least shaped correctly.
    assert cfg.top_k == cached_topk
    if cfg.sort_by == "img":
        assert len(rest) == 0
    elif cfg.sort_by == "patch":
        assert len(rest) == 1
        n_patches = rest[0]
        assert n_patches > 0
    else:
        typing.assert_never(cfg.sort_by)

    logger.info("Loaded sorted data.")

    os.makedirs(cfg.root, exist_ok=True)
    fig_fpath = os.path.join(
        cfg.root, f"{cfg.n_distributions}_activation_distributions.png"
    )
    plot_activation_distributions(cfg, distributions).savefig(fig_fpath, dpi=300)
    logger.info(
        "Saved %d activation distributions to '%s'.", cfg.n_distributions, fig_fpath
    )

    dataset = saev.data.images.get_dataset(cfg.images, img_transform=None)

    min_log_freq, max_log_freq = cfg.log_freq_range
    min_log_value, max_log_value = cfg.log_value_range

    mask = (
        (min_log_freq < torch.log10(sparsity))
        & (torch.log10(sparsity) < max_log_freq)
        & (min_log_value < torch.log10(mean_values))
        & (torch.log10(mean_values) < max_log_value)
    )

    neurons = cfg.include_latents
    random_neurons = torch.arange(d_sae)[mask.cpu()].tolist()
    random.seed(cfg.seed)
    random.shuffle(random_neurons)
    neurons += random_neurons[: cfg.n_latents]

    for i in helpers.progress(neurons, desc="saving visuals"):
        neuron_dir = os.path.join(cfg.root, "neurons", str(i))
        os.makedirs(neuron_dir, exist_ok=True)

        # Image grid
        elems = []
        seen_i_im = set()
        for i_im, values_p in zip(top_i[i].tolist(), top_values[i]):
            if i_im in seen_i_im:
                continue

            example = dataset[i_im]
            if cfg.sort_by == "img":
                elem = GridElement(example["image"], example["label"], torch.tensor([]))
            elif cfg.sort_by == "patch":
                elem = GridElement(example["image"], example["label"], values_p)
            else:
                typing.assert_never(cfg.sort_by)
            elems.append(elem)

            seen_i_im.add(i_im)

        # How to scale values.
        upper = None
        if top_values[i].numel() > 0:
            upper = top_values[i].max().item()

        for j, elem in enumerate(elems):
            img = make_img(elem, upper=upper)
            img.save(os.path.join(neuron_dir, f"{j}.png"))
            with open(os.path.join(neuron_dir, f"{j}.txt"), "w") as fd:
                fd.write(elem.label + "\n")

        # Metadata
        metadata = {
            "neuron": i,
            "log10_freq": torch.log10(sparsity[i]).item(),
            "log10_value": torch.log10(mean_values[i]).item(),
        }
        with open(os.path.join(neuron_dir, "metadata.json"), "w") as fd:
            json.dump(metadata, fd)


@beartype.beartype
class PercentileEstimator:
    def __init__(
        self,
        percentile: float | int,
        total: int,
        lr: float = 1e-3,
        shape: tuple[int, ...] = (),
    ):
        self.percentile = percentile
        self.total = total
        self.lr = lr

        self._estimate = torch.zeros(shape)
        self._step = 0

    def update(self, x):
        """
        Update the estimator with a new value.

        This method maintains the marker positions using the P2 algorithm rules.
        When a new value arrives, it's placed in the appropriate position relative to existing markers, and marker positions are adjusted to maintain their desired percentile positions.

        Arguments:
            x: The new value to incorporate into the estimation
        """
        self._step += 1

        step_size = self.lr * (self.total - self._step) / self.total

        # Is a no-op if it's already on the same device.
        if isinstance(x, Tensor):
            self._estimate = self._estimate.to(x.device)

        self._estimate += step_size * (
            torch.sign(x - self._estimate) + 2 * self.percentile / 100 - 1.0
        )

    @property
    def estimate(self):
        return self._estimate

>>>> utils/scheduling.py
import math
from typing import Any, Iterator, Protocol, runtime_checkable

import beartype


@beartype.beartype
class Scheduler:
    def step(self) -> float:
        err_msg = f"{self.__class__.__name__} must implement step()."
        raise NotImplementedError(err_msg)

    def __repr__(self) -> str:
        err_msg = f"{self.__class__.__name__} must implement __repr__()."
        raise NotImplementedError(err_msg)


@beartype.beartype
class Warmup(Scheduler):
    """
    Linearly increases from `init` to `final` over `n_warmup_steps` steps.
    """

    def __init__(self, init: float, final: float, n_steps: int):
        self.final = final
        self.init = init
        self.n_steps = n_steps
        self._step = 0

    def step(self) -> float:
        self._step += 1
        if self._step < self.n_steps:
            return self.init + (self.final - self.init) * (self._step / self.n_steps)

        return self.final

    def __repr__(self) -> str:
        return f"Warmup(init={self.init}, final={self.final}, n_steps={self.n_steps})"


@beartype.beartype
class WarmupCosine(Scheduler):
    """
    Linearly increases from `init` to `peak` over `n_warmup` steps, then decrease down to final using cosine decay over n_steps - n_warmup.
    """

    def __init__(
        self, init: float, n_warmup: int, peak: float, n_steps: int, final: float
    ):
        self.init = init
        self.peak = peak
        self.final = final
        self.n_warmup = n_warmup
        self.n_steps = n_steps
        self._step = 0

    def step(self) -> float:
        self._step += 1
        if self._step < self.n_warmup:
            return self.init + (self.peak - self.init) * (self._step / self.n_warmup)
        elif self._step < self.n_steps:
            # Cosine decay from self.peak to self.final over (n_steps - n_warmup)
            progress = (self._step - self.n_warmup) / (self.n_steps - self.n_warmup)
            cosine_factor = (1 + math.cos(math.pi * progress)) / 2
            return self.final + (self.peak - self.final) * cosine_factor

        return self.final

    def __repr__(self) -> str:
        return f"WarmupCosine(init={self.init}, peak={self.peak}, final={self.final}, n_warmup={self.n_warmup}, n_steps={self.n_steps})"


@runtime_checkable
class DataLoaderLike(Protocol):
    drop_last: bool
    batch_size: int  # This is also needed since BatchLimiter uses it

    def __iter__(self) -> Iterator[Any]: ...


@beartype.beartype
class BatchLimiter:
    """
    Limits the number of batches to only return `n_samples` total samples.
    """

    def __init__(self, dataloader: DataLoaderLike, n_samples: int):
        self.dataloader = dataloader
        self.n_samples = n_samples
        self.batch_size = dataloader.batch_size

    def __len__(self) -> int:
        return self.n_samples // self.batch_size

    def __getattr__(self, name: str) -> Any:
        """Pass through attribute access to the wrapped dataloader."""
        # __getattr__ is only called when the attribute wasn't found on self
        # So we delegate to the wrapped dataloader
        try:
            return getattr(self.dataloader, name)
        except AttributeError:
            # Re-raise with more context about where the attribute was not found
            raise AttributeError(
                f"'{self.__class__.__name__}' object and its wrapped dataloader have no attribute '{name}'"
            )

    def __iter__(self):
        self.n_seen = 0
        while True:
            for batch in self.dataloader:
                yield batch

                # Sometimes we underestimate because the final batch in the dataloader might not be a full batch.
                self.n_seen += self.batch_size
                if self.n_seen > self.n_samples:
                    return

            # We try to mitigate the above issue by ignoring the last batch if we don't have drop_last.
            if not self.dataloader.drop_last:
                self.n_seen -= self.batch_size


def _plot_example_schedules():
    import matplotlib.pyplot as plt
    import numpy as np

    fig, ax = plt.subplots()

    n_steps = 1000
    xs = np.arange(n_steps)

    schedule = WarmupCosine(0.1, 100, 0.9, 1000, 0.0)
    ys = [schedule.step() for _ in xs]

    ax.plot(xs, ys, label=str(schedule))

    fig.tight_layout()
    fig.savefig("schedules.png")


if __name__ == "__main__":
    _plot_example_schedules()

>>>> utils/wandb.py
import beartype
import wandb

MetricQueue = list[tuple[int, dict[str, object]]]


@beartype.beartype
class ParallelWandbRun:
    """
    Inspired by https://community.wandb.ai/t/is-it-possible-to-log-to-multiple-runs-simultaneously/4387
    """

    def __init__(
        self,
        project: str,
        cfgs: list[dict[str, object]],
        mode: str,
        tags: list[str],
        dir: str = ".wandb",
    ):
        cfg, *cfgs = cfgs
        self.project = project
        self.cfgs = cfgs
        self.mode = mode
        self.tags = tags
        self.dir = dir

        self.live_run = wandb.init(
            project=project, config=cfg, mode=mode, tags=tags, dir=dir
        )

        self.metric_queues: list[MetricQueue] = [[] for _ in self.cfgs]

    def log(self, metrics: list[dict[str, object]], *, step: int):
        metric, *metrics = metrics
        self.live_run.log(metric, step=step)
        for queue, metric in zip(self.metric_queues, metrics):
            queue.append((step, metric))

    def finish(self) -> list[str]:
        ids = [self.live_run.id]
        # Log the rest of the runs.
        self.live_run.finish()

        for queue, cfg in zip(self.metric_queues, self.cfgs):
            run = wandb.init(
                project=self.project,
                config=cfg,
                mode=self.mode,
                tags=self.tags + ["queued"],
                dir=self.dir,
            )
            for step, metric in queue:
                run.log(metric, step=step)
            ids.append(run.id)
            run.finish()

        return ids

